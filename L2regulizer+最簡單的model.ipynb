{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b08202011/Mlproject/blob/main/L2regulizer%2B%E6%9C%80%E7%B0%A1%E5%96%AE%E7%9A%84model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klC1qgz2Oi2",
        "outputId": "39d29e60-be20-4c2b-80df-8b4c717be797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
            "To: /content/train.csv\n",
            "100% 23.3M/23.3M [00:00<00:00, 123MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
            "To: /content/test.csv\n",
            "100% 8.88M/8.88M [00:00<00:00, 55.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=187gijxd4T5yuZe_g2K9UNepx7MT1CnAu\n",
            "To: /content/sample_submission.csv\n",
            "100% 50.5k/50.5k [00:00<00:00, 64.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
        "!gdown 15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
        "!gdown 187gijxd4T5yuZe_g2K9UNepx7MT1CnAu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "10JsWM_brgQw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "with open('train.csv',newline='') as csvfile:\n",
        "  train = csv.reader(csvfile)\n",
        "  train = list(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DscDJdCVr0kH",
        "outputId": "fb7e319b-45a3-4322-f5bc-ef4c59a2bc55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n"
          ]
        }
      ],
      "source": [
        "pick = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23]\n",
        "#可以選要用那些特徵，0是我們要predict的\n",
        "#只選數值或true fale 但且沒有選id\n",
        "feature = []\n",
        "for i in range(1,len(train)):\n",
        "  feature.append([])\n",
        "  for j in pick:\n",
        "    feature[i-1].append(train[i][j])\n",
        "\n",
        "for i in range(len(feature)):\n",
        "  for j in range(len(feature[i])):\n",
        "    if(feature[i][j]!='' and feature[i][j]!='False' and feature[i][j]!= 'True'):\n",
        "      feature[i][j] = float(feature[i][j])\n",
        "    elif(feature[i][j] == 'False'):\n",
        "      feature[i][j] = 0\n",
        "    elif(feature[i][j] == 'True'):\n",
        "      feature[i][j] = 1\n",
        "    else:\n",
        "      feature[i][j] = np.nan\n",
        "print(len(feature))\n",
        "\n",
        "\n",
        "with open('test.csv',newline='') as csvfile:         ###load testdata\n",
        "  testcsv = csv.reader(csvfile)\n",
        "  testcsv = list(testcsv)\n",
        "  \n",
        "test = []\n",
        "picktest = [0,1,2,3,4,5,6,7,8,9,10,11,12,22]\n",
        "\n",
        "for i in range(1,len(testcsv)):\n",
        "  test.append([])\n",
        "  for j in picktest:\n",
        "    test[i-1].append(testcsv[i][j])\n",
        "for i in range(len(test)):\n",
        "  for j in range(len(test[i])):\n",
        "    if(test[i][j]!='' and test[i][j]!='False' and test[i][j]!= 'True'):\n",
        "      test[i][j] = float(test[i][j])\n",
        "    elif(test[i][j] == 'False'):\n",
        "      test[i][j] = 0\n",
        "    elif(test[i][j] == 'True'):\n",
        "      test[i][j] = 1\n",
        "    else:\n",
        "      test[i][j] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFwj7Vtjr7C3",
        "outputId": "567ede2e-4d29-4de8-8bea-45bc4b57df71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ycimpute\n",
            "  Downloading ycimpute-0.2-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (2.0.1+cu118)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->ycimpute) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->ycimpute) (1.3.0)\n",
            "Installing collected packages: ycimpute\n",
            "Successfully installed ycimpute-0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ycimpute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vnSka536aDrP"
      },
      "outputs": [],
      "source": [
        "# Numerical Operations\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Reading/Writing Data\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "# For Progress Bar\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import f_regression\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
        "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "# For plotting learning curve\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.functional import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpQkUiqtr-VA",
        "outputId": "f3270caa-94ca-4af8-9020-916867b6b3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n",
            "6315\n",
            "(23485, 14)\n",
            "[[ 2.73359449e-04  3.00000000e+00 -2.51760000e+01 ...  1.37622000e+05\n",
            "   1.99813700e+07  3.07000000e+03]\n",
            " [ 1.84220009e-01  7.00000000e+00             nan ...  5.19000000e+03\n",
            "              nan  1.22000000e+02]\n",
            " [            nan  6.00000000e+00 -1.55960000e+01 ...  5.19000000e+03\n",
            "   1.93208100e+07  1.22000000e+02]\n",
            " ...\n",
            " [ 4.26222894e-01  5.00000000e+00 -5.80754484e+00 ...  2.30516298e+04\n",
            "   1.84909433e+07             nan]\n",
            " [ 2.04440254e-01             nan -1.15639636e+01 ...             nan\n",
            "   1.19959340e+08  1.51390000e+04]\n",
            " [ 1.46792398e-02             nan             nan ...  9.23548143e+04\n",
            "   3.43928191e+06  1.10000000e+01]]\n"
          ]
        }
      ],
      "source": [
        "feature = np.array(feature)\n",
        "test = np.array(test)\n",
        "from ycimpute.imputer import knnimput\n",
        "xtrain, ytrain = feature[:,1:], feature[:,0]\n",
        "print(xtrain.shape[0])\n",
        "print(test.shape[0])\n",
        "data = np.concatenate((xtrain,test))\n",
        "print(data.shape)\n",
        "\n",
        "data_np = torch.from_numpy(data)\n",
        "mean, std, var = torch.mean(data_np), torch.std(data_np), torch.var(data_np)\n",
        "data_np = (data_np-mean)/std\n",
        "data_np = np.array(data.tolist())\n",
        "print(data_np)\n",
        "#imp_mean.fit(data_np)\n",
        "#data_np = imp_mean.transform(data_np)\n",
        "\n",
        "\n",
        "#imp_median.fit(feature)\n",
        "#feature = imp_median.transform(feature)\n",
        "##可以修改這裡 使用不同的data填補方式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pae3xPxJdB_e",
        "outputId": "375f7f86-1910-442b-d15a-762ec1ffab2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing row 1/23485 with 1 missing, elapsed time: 126.257\n",
            "Imputing row 101/23485 with 1 missing, elapsed time: 126.280\n",
            "Imputing row 201/23485 with 0 missing, elapsed time: 126.303\n",
            "Imputing row 301/23485 with 2 missing, elapsed time: 126.322\n",
            "Imputing row 401/23485 with 1 missing, elapsed time: 126.342\n",
            "Imputing row 501/23485 with 1 missing, elapsed time: 126.362\n",
            "Imputing row 601/23485 with 4 missing, elapsed time: 126.383\n",
            "Imputing row 701/23485 with 1 missing, elapsed time: 126.404\n",
            "Imputing row 801/23485 with 2 missing, elapsed time: 126.428\n",
            "Imputing row 901/23485 with 3 missing, elapsed time: 126.450\n",
            "Imputing row 1001/23485 with 0 missing, elapsed time: 126.476\n",
            "Imputing row 1101/23485 with 0 missing, elapsed time: 126.497\n",
            "Imputing row 1201/23485 with 2 missing, elapsed time: 126.520\n",
            "Imputing row 1301/23485 with 2 missing, elapsed time: 126.543\n",
            "Imputing row 1401/23485 with 2 missing, elapsed time: 126.562\n",
            "Imputing row 1501/23485 with 3 missing, elapsed time: 126.584\n",
            "Imputing row 1601/23485 with 2 missing, elapsed time: 126.605\n",
            "Imputing row 1701/23485 with 1 missing, elapsed time: 126.633\n",
            "Imputing row 1801/23485 with 2 missing, elapsed time: 126.662\n",
            "Imputing row 1901/23485 with 0 missing, elapsed time: 126.686\n",
            "Imputing row 2001/23485 with 1 missing, elapsed time: 126.707\n",
            "Imputing row 2101/23485 with 5 missing, elapsed time: 126.734\n",
            "Imputing row 2201/23485 with 3 missing, elapsed time: 126.756\n",
            "Imputing row 2301/23485 with 2 missing, elapsed time: 126.776\n",
            "Imputing row 2401/23485 with 3 missing, elapsed time: 126.799\n",
            "Imputing row 2501/23485 with 0 missing, elapsed time: 126.820\n",
            "Imputing row 2601/23485 with 1 missing, elapsed time: 126.844\n",
            "Imputing row 2701/23485 with 1 missing, elapsed time: 126.869\n",
            "Imputing row 2801/23485 with 2 missing, elapsed time: 126.897\n",
            "Imputing row 2901/23485 with 0 missing, elapsed time: 126.919\n",
            "Imputing row 3001/23485 with 2 missing, elapsed time: 126.946\n",
            "Imputing row 3101/23485 with 1 missing, elapsed time: 126.967\n",
            "Imputing row 3201/23485 with 3 missing, elapsed time: 126.989\n",
            "Imputing row 3301/23485 with 3 missing, elapsed time: 127.014\n",
            "Imputing row 3401/23485 with 4 missing, elapsed time: 127.034\n",
            "Imputing row 3501/23485 with 1 missing, elapsed time: 127.056\n",
            "Imputing row 3601/23485 with 5 missing, elapsed time: 127.077\n",
            "Imputing row 3701/23485 with 2 missing, elapsed time: 127.100\n",
            "Imputing row 3801/23485 with 2 missing, elapsed time: 127.121\n",
            "Imputing row 3901/23485 with 3 missing, elapsed time: 127.140\n",
            "Imputing row 4001/23485 with 1 missing, elapsed time: 127.160\n",
            "Imputing row 4101/23485 with 2 missing, elapsed time: 127.186\n",
            "Imputing row 4201/23485 with 2 missing, elapsed time: 127.205\n",
            "Imputing row 4301/23485 with 2 missing, elapsed time: 127.226\n",
            "Imputing row 4401/23485 with 1 missing, elapsed time: 127.249\n",
            "Imputing row 4501/23485 with 1 missing, elapsed time: 127.274\n",
            "Imputing row 4601/23485 with 3 missing, elapsed time: 127.295\n",
            "Imputing row 4701/23485 with 2 missing, elapsed time: 127.318\n",
            "Imputing row 4801/23485 with 2 missing, elapsed time: 127.339\n",
            "Imputing row 4901/23485 with 3 missing, elapsed time: 127.360\n",
            "Imputing row 5001/23485 with 3 missing, elapsed time: 127.380\n",
            "Imputing row 5101/23485 with 3 missing, elapsed time: 127.404\n",
            "Imputing row 5201/23485 with 2 missing, elapsed time: 127.424\n",
            "Imputing row 5301/23485 with 1 missing, elapsed time: 127.448\n",
            "Imputing row 5401/23485 with 4 missing, elapsed time: 127.471\n",
            "Imputing row 5501/23485 with 4 missing, elapsed time: 127.495\n",
            "Imputing row 5601/23485 with 2 missing, elapsed time: 127.515\n",
            "Imputing row 5701/23485 with 2 missing, elapsed time: 127.540\n",
            "Imputing row 5801/23485 with 3 missing, elapsed time: 127.562\n",
            "Imputing row 5901/23485 with 2 missing, elapsed time: 127.582\n",
            "Imputing row 6001/23485 with 2 missing, elapsed time: 127.603\n",
            "Imputing row 6101/23485 with 2 missing, elapsed time: 127.623\n",
            "Imputing row 6201/23485 with 3 missing, elapsed time: 127.645\n",
            "Imputing row 6301/23485 with 1 missing, elapsed time: 127.665\n",
            "Imputing row 6401/23485 with 3 missing, elapsed time: 127.694\n",
            "Imputing row 6501/23485 with 1 missing, elapsed time: 127.714\n",
            "Imputing row 6601/23485 with 2 missing, elapsed time: 127.736\n",
            "Imputing row 6701/23485 with 1 missing, elapsed time: 127.760\n",
            "Imputing row 6801/23485 with 0 missing, elapsed time: 127.780\n",
            "Imputing row 6901/23485 with 3 missing, elapsed time: 127.801\n",
            "Imputing row 7001/23485 with 1 missing, elapsed time: 127.822\n",
            "Imputing row 7101/23485 with 2 missing, elapsed time: 127.844\n",
            "Imputing row 7201/23485 with 0 missing, elapsed time: 127.872\n",
            "Imputing row 7301/23485 with 2 missing, elapsed time: 127.892\n",
            "Imputing row 7401/23485 with 2 missing, elapsed time: 127.913\n",
            "Imputing row 7501/23485 with 1 missing, elapsed time: 127.936\n",
            "Imputing row 7601/23485 with 4 missing, elapsed time: 127.959\n",
            "Imputing row 7701/23485 with 2 missing, elapsed time: 127.983\n",
            "Imputing row 7801/23485 with 1 missing, elapsed time: 128.005\n",
            "Imputing row 7901/23485 with 1 missing, elapsed time: 128.028\n",
            "Imputing row 8001/23485 with 1 missing, elapsed time: 128.049\n",
            "Imputing row 8101/23485 with 4 missing, elapsed time: 128.070\n",
            "Imputing row 8201/23485 with 2 missing, elapsed time: 128.094\n",
            "Imputing row 8301/23485 with 3 missing, elapsed time: 128.116\n",
            "Imputing row 8401/23485 with 1 missing, elapsed time: 128.137\n",
            "Imputing row 8501/23485 with 2 missing, elapsed time: 128.157\n",
            "Imputing row 8601/23485 with 2 missing, elapsed time: 128.175\n",
            "Imputing row 8701/23485 with 3 missing, elapsed time: 128.199\n",
            "Imputing row 8801/23485 with 2 missing, elapsed time: 128.218\n",
            "Imputing row 8901/23485 with 3 missing, elapsed time: 128.239\n",
            "Imputing row 9001/23485 with 1 missing, elapsed time: 128.260\n",
            "Imputing row 9101/23485 with 1 missing, elapsed time: 128.284\n",
            "Imputing row 9201/23485 with 1 missing, elapsed time: 128.306\n",
            "Imputing row 9301/23485 with 5 missing, elapsed time: 128.326\n",
            "Imputing row 9401/23485 with 6 missing, elapsed time: 128.346\n",
            "Imputing row 9501/23485 with 0 missing, elapsed time: 128.368\n",
            "Imputing row 9601/23485 with 3 missing, elapsed time: 128.398\n",
            "Imputing row 9701/23485 with 3 missing, elapsed time: 128.423\n",
            "Imputing row 9801/23485 with 2 missing, elapsed time: 128.443\n",
            "Imputing row 9901/23485 with 1 missing, elapsed time: 128.462\n",
            "Imputing row 10001/23485 with 3 missing, elapsed time: 128.483\n",
            "Imputing row 10101/23485 with 3 missing, elapsed time: 128.502\n",
            "Imputing row 10201/23485 with 3 missing, elapsed time: 128.522\n",
            "Imputing row 10301/23485 with 1 missing, elapsed time: 128.544\n",
            "Imputing row 10401/23485 with 1 missing, elapsed time: 128.563\n",
            "Imputing row 10501/23485 with 3 missing, elapsed time: 128.584\n",
            "Imputing row 10601/23485 with 2 missing, elapsed time: 128.610\n",
            "Imputing row 10701/23485 with 1 missing, elapsed time: 128.638\n",
            "Imputing row 10801/23485 with 3 missing, elapsed time: 128.664\n",
            "Imputing row 10901/23485 with 1 missing, elapsed time: 128.683\n",
            "Imputing row 11001/23485 with 2 missing, elapsed time: 128.708\n",
            "Imputing row 11101/23485 with 3 missing, elapsed time: 128.738\n",
            "Imputing row 11201/23485 with 1 missing, elapsed time: 128.762\n",
            "Imputing row 11301/23485 with 1 missing, elapsed time: 128.785\n",
            "Imputing row 11401/23485 with 2 missing, elapsed time: 128.806\n",
            "Imputing row 11501/23485 with 2 missing, elapsed time: 128.825\n",
            "Imputing row 11601/23485 with 2 missing, elapsed time: 128.843\n",
            "Imputing row 11701/23485 with 1 missing, elapsed time: 128.866\n",
            "Imputing row 11801/23485 with 3 missing, elapsed time: 128.897\n",
            "Imputing row 11901/23485 with 2 missing, elapsed time: 128.916\n",
            "Imputing row 12001/23485 with 2 missing, elapsed time: 128.939\n",
            "Imputing row 12101/23485 with 1 missing, elapsed time: 128.962\n",
            "Imputing row 12201/23485 with 3 missing, elapsed time: 128.982\n",
            "Imputing row 12301/23485 with 2 missing, elapsed time: 129.003\n",
            "Imputing row 12401/23485 with 5 missing, elapsed time: 129.025\n",
            "Imputing row 12501/23485 with 1 missing, elapsed time: 129.046\n",
            "Imputing row 12601/23485 with 4 missing, elapsed time: 129.071\n",
            "Imputing row 12701/23485 with 1 missing, elapsed time: 129.093\n",
            "Imputing row 12801/23485 with 3 missing, elapsed time: 129.114\n",
            "Imputing row 12901/23485 with 2 missing, elapsed time: 129.132\n",
            "Imputing row 13001/23485 with 2 missing, elapsed time: 129.157\n",
            "Imputing row 13101/23485 with 1 missing, elapsed time: 129.180\n",
            "Imputing row 13201/23485 with 2 missing, elapsed time: 129.200\n",
            "Imputing row 13301/23485 with 1 missing, elapsed time: 129.329\n",
            "Imputing row 13401/23485 with 2 missing, elapsed time: 129.390\n",
            "Imputing row 13501/23485 with 3 missing, elapsed time: 129.456\n",
            "Imputing row 13601/23485 with 2 missing, elapsed time: 129.593\n",
            "Imputing row 13701/23485 with 3 missing, elapsed time: 129.719\n",
            "Imputing row 13801/23485 with 3 missing, elapsed time: 129.872\n",
            "Imputing row 13901/23485 with 1 missing, elapsed time: 129.902\n",
            "Imputing row 14001/23485 with 5 missing, elapsed time: 129.949\n",
            "Imputing row 14101/23485 with 1 missing, elapsed time: 129.973\n",
            "Imputing row 14201/23485 with 0 missing, elapsed time: 129.999\n",
            "Imputing row 14301/23485 with 2 missing, elapsed time: 130.021\n",
            "Imputing row 14401/23485 with 2 missing, elapsed time: 130.047\n",
            "Imputing row 14501/23485 with 1 missing, elapsed time: 130.068\n",
            "Imputing row 14601/23485 with 0 missing, elapsed time: 130.101\n",
            "Imputing row 14701/23485 with 1 missing, elapsed time: 130.141\n",
            "Imputing row 14801/23485 with 2 missing, elapsed time: 130.161\n",
            "Imputing row 14901/23485 with 2 missing, elapsed time: 130.182\n",
            "Imputing row 15001/23485 with 2 missing, elapsed time: 130.203\n",
            "Imputing row 15101/23485 with 1 missing, elapsed time: 130.224\n",
            "Imputing row 15201/23485 with 1 missing, elapsed time: 130.247\n",
            "Imputing row 15301/23485 with 3 missing, elapsed time: 130.269\n",
            "Imputing row 15401/23485 with 1 missing, elapsed time: 130.296\n",
            "Imputing row 15501/23485 with 2 missing, elapsed time: 130.323\n",
            "Imputing row 15601/23485 with 1 missing, elapsed time: 130.343\n",
            "Imputing row 15701/23485 with 2 missing, elapsed time: 130.368\n",
            "Imputing row 15801/23485 with 2 missing, elapsed time: 130.387\n",
            "Imputing row 15901/23485 with 2 missing, elapsed time: 130.408\n",
            "Imputing row 16001/23485 with 1 missing, elapsed time: 130.429\n",
            "Imputing row 16101/23485 with 2 missing, elapsed time: 130.448\n",
            "Imputing row 16201/23485 with 0 missing, elapsed time: 130.471\n",
            "Imputing row 16301/23485 with 1 missing, elapsed time: 130.493\n",
            "Imputing row 16401/23485 with 3 missing, elapsed time: 130.512\n",
            "Imputing row 16501/23485 with 1 missing, elapsed time: 130.535\n",
            "Imputing row 16601/23485 with 3 missing, elapsed time: 130.555\n",
            "Imputing row 16701/23485 with 2 missing, elapsed time: 130.579\n",
            "Imputing row 16801/23485 with 2 missing, elapsed time: 130.602\n",
            "Imputing row 16901/23485 with 3 missing, elapsed time: 130.624\n",
            "Imputing row 17001/23485 with 2 missing, elapsed time: 130.644\n",
            "Imputing row 17101/23485 with 5 missing, elapsed time: 130.666\n",
            "Imputing row 17201/23485 with 3 missing, elapsed time: 130.687\n",
            "Imputing row 17301/23485 with 1 missing, elapsed time: 130.708\n",
            "Imputing row 17401/23485 with 4 missing, elapsed time: 130.729\n",
            "Imputing row 17501/23485 with 3 missing, elapsed time: 130.757\n",
            "Imputing row 17601/23485 with 0 missing, elapsed time: 130.777\n",
            "Imputing row 17701/23485 with 0 missing, elapsed time: 130.799\n",
            "Imputing row 17801/23485 with 4 missing, elapsed time: 130.820\n",
            "Imputing row 17901/23485 with 1 missing, elapsed time: 130.842\n",
            "Imputing row 18001/23485 with 1 missing, elapsed time: 130.863\n",
            "Imputing row 18101/23485 with 5 missing, elapsed time: 130.891\n",
            "Imputing row 18201/23485 with 2 missing, elapsed time: 130.919\n",
            "Imputing row 18301/23485 with 6 missing, elapsed time: 130.943\n",
            "Imputing row 18401/23485 with 4 missing, elapsed time: 130.965\n",
            "Imputing row 18501/23485 with 1 missing, elapsed time: 130.988\n",
            "Imputing row 18601/23485 with 0 missing, elapsed time: 131.011\n",
            "Imputing row 18701/23485 with 4 missing, elapsed time: 131.033\n",
            "Imputing row 18801/23485 with 3 missing, elapsed time: 131.056\n",
            "Imputing row 18901/23485 with 1 missing, elapsed time: 131.079\n",
            "Imputing row 19001/23485 with 1 missing, elapsed time: 131.100\n",
            "Imputing row 19101/23485 with 1 missing, elapsed time: 131.122\n",
            "Imputing row 19201/23485 with 2 missing, elapsed time: 131.145\n",
            "Imputing row 19301/23485 with 1 missing, elapsed time: 131.169\n",
            "Imputing row 19401/23485 with 5 missing, elapsed time: 131.193\n",
            "Imputing row 19501/23485 with 2 missing, elapsed time: 131.215\n",
            "Imputing row 19601/23485 with 5 missing, elapsed time: 131.238\n",
            "Imputing row 19701/23485 with 3 missing, elapsed time: 131.260\n",
            "Imputing row 19801/23485 with 3 missing, elapsed time: 131.283\n",
            "Imputing row 19901/23485 with 2 missing, elapsed time: 131.307\n",
            "Imputing row 20001/23485 with 3 missing, elapsed time: 131.327\n",
            "Imputing row 20101/23485 with 2 missing, elapsed time: 131.349\n",
            "Imputing row 20201/23485 with 0 missing, elapsed time: 131.372\n",
            "Imputing row 20301/23485 with 1 missing, elapsed time: 131.396\n",
            "Imputing row 20401/23485 with 3 missing, elapsed time: 131.418\n",
            "Imputing row 20501/23485 with 1 missing, elapsed time: 131.441\n",
            "Imputing row 20601/23485 with 2 missing, elapsed time: 131.465\n",
            "Imputing row 20701/23485 with 0 missing, elapsed time: 131.487\n",
            "Imputing row 20801/23485 with 0 missing, elapsed time: 131.508\n",
            "Imputing row 20901/23485 with 2 missing, elapsed time: 131.530\n",
            "Imputing row 21001/23485 with 2 missing, elapsed time: 131.549\n",
            "Imputing row 21101/23485 with 4 missing, elapsed time: 131.571\n",
            "Imputing row 21201/23485 with 2 missing, elapsed time: 131.597\n",
            "Imputing row 21301/23485 with 2 missing, elapsed time: 131.616\n",
            "Imputing row 21401/23485 with 3 missing, elapsed time: 131.635\n",
            "Imputing row 21501/23485 with 2 missing, elapsed time: 131.657\n",
            "Imputing row 21601/23485 with 0 missing, elapsed time: 131.677\n",
            "Imputing row 21701/23485 with 2 missing, elapsed time: 131.697\n",
            "Imputing row 21801/23485 with 4 missing, elapsed time: 131.718\n",
            "Imputing row 21901/23485 with 2 missing, elapsed time: 131.740\n",
            "Imputing row 22001/23485 with 0 missing, elapsed time: 131.759\n",
            "Imputing row 22101/23485 with 3 missing, elapsed time: 131.779\n",
            "Imputing row 22201/23485 with 2 missing, elapsed time: 131.797\n",
            "Imputing row 22301/23485 with 1 missing, elapsed time: 131.820\n",
            "Imputing row 22401/23485 with 2 missing, elapsed time: 131.842\n",
            "Imputing row 22501/23485 with 1 missing, elapsed time: 131.864\n",
            "Imputing row 22601/23485 with 1 missing, elapsed time: 131.885\n",
            "Imputing row 22701/23485 with 1 missing, elapsed time: 131.907\n",
            "Imputing row 22801/23485 with 2 missing, elapsed time: 131.941\n",
            "Imputing row 22901/23485 with 3 missing, elapsed time: 131.964\n",
            "Imputing row 23001/23485 with 3 missing, elapsed time: 131.985\n",
            "Imputing row 23101/23485 with 2 missing, elapsed time: 132.015\n",
            "Imputing row 23201/23485 with 0 missing, elapsed time: 132.037\n",
            "Imputing row 23301/23485 with 2 missing, elapsed time: 132.057\n",
            "Imputing row 23401/23485 with 3 missing, elapsed time: 132.078\n"
          ]
        }
      ],
      "source": [
        "data_np = knnimput.KNN(k=4).complete(data_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo7Ka39Qbzvz",
        "outputId": "3d4fdfaa-ad5c-42c7-a987-0e7ea848f5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.09584584e-01  7.23135868e+00 -6.25100000e+00  2.77000000e-02\n",
            "  3.79641600e-03  9.33999086e-02  1.00000000e-03  4.31448300e-01\n",
            "  1.50220000e+02  2.65000000e+05  2.02457327e+08  9.97035000e+05\n",
            "  3.99661898e+08  2.43500000e+04]\n",
            "(17170, 14)\n",
            "17170\n"
          ]
        }
      ],
      "source": [
        "print(data_np[3])\n",
        "feature, test_data = data_np[:xtrain.shape[0],:], data_np[xtrain.shape[0]:,:]\n",
        "print(feature.shape)\n",
        "ytrain = np.reshape(ytrain,(ytrain.shape[0],1))\n",
        "feature = np.concatenate((ytrain,feature),axis=1)\n",
        "print(feature.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IyLGsIqeWbL",
        "outputId": "1471ab70-96e0-4e16-a3fa-297bee9d56e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00000000e+00 2.73359449e-04 3.00000000e+00 ... 1.37622000e+05\n",
            "  1.99813700e+07 3.07000000e+03]\n",
            " [0.00000000e+00 1.84220009e-01 7.00000000e+00 ... 5.19000000e+03\n",
            "  2.43793864e+08 1.22000000e+02]\n",
            " [0.00000000e+00 4.84078414e-01 6.00000000e+00 ... 5.19000000e+03\n",
            "  1.93208100e+07 1.22000000e+02]\n",
            " ...\n",
            " [1.00000000e+00 5.71787000e-01 4.00000000e+00 ... 3.29000000e+02\n",
            "  2.33188740e+07 0.00000000e+00]\n",
            " [1.00000000e+00 4.51217663e-01 9.00000000e+00 ... 2.40307098e+05\n",
            "  6.87396100e+06 0.00000000e+00]\n",
            " [2.00000000e+00 8.25293672e-01 6.00000000e+00 ... 2.48400000e+03\n",
            "  5.69558400e+06 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "print(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EuIQ-7tKuocP"
      },
      "outputs": [],
      "source": [
        "def same_seed(seed): \n",
        "    '''Fixes random number generator seeds for reproducibility.'''\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_valid_split(data_set, valid_ratio, seed):\n",
        "    '''Split provided training data into training set and validation set'''\n",
        "    valid_set_size = int(valid_ratio * len(data_set)) \n",
        "    train_set_size = len(data_set) - valid_set_size\n",
        "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
        "    return np.array(train_set), np.array(valid_set)\n",
        "\n",
        "def predict(test_loader, model, device):\n",
        "    model.eval() # Set your model to evaluation mode.\n",
        "    preds = []\n",
        "    for x in tqdm(test_loader):\n",
        "        x = x.to(device)                        \n",
        "        with torch.no_grad():                   \n",
        "            pred = torch.clamp(model(x),0,9)\n",
        "            #pred = np.array(pred.tolist())\n",
        "            #predict = np.zeros((pred.shape[0]))\n",
        "            #for i in range(pred.shape[0]):\n",
        "            #  rate = 0\n",
        "            #  for j in range(pred.shape[1]):\n",
        "            #    rate += pred[i][j]\n",
        "            #    if rate >0.5:\n",
        "            #      predict[i] = j \n",
        "            #      break\n",
        "            #pred = torch.from_numpy(predict).to(device)              \n",
        "            preds.append(pred.detach().cpu())   \n",
        "    preds = torch.cat(preds, dim=0).numpy()  \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D320CZu7nkU"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ypSQGM5i7q_g"
      },
      "outputs": [],
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    '''\n",
        "    x: Features.\n",
        "    y: Targets, if none, do prediction.\n",
        "    '''\n",
        "    def __init__(self, x, y=None):\n",
        "        if y is None:\n",
        "            self.y = y\n",
        "        else:\n",
        "            self.y = torch.FloatTensor(y)\n",
        "        self.x = torch.FloatTensor(x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.x[idx]\n",
        "        else:\n",
        "            return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dFunF8QtoP_"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zIkJ9iRauAy8"
      },
      "outputs": [],
      "source": [
        "class My_Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(My_Model, self).__init__()\n",
        "        # TODO: modify model's structure, be aware of dimensions. \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "                                     \n",
        "        )\n",
        "        self.maxpoollayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(16, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "        )\n",
        "        self.finallayer2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Tanh(),                  ## model layer 可以隨便調，不知道哪裡有現成的可以找\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(16, 10),\n",
        "        )\n",
        "        \n",
        "        self.finallayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 640),\n",
        "            nn.BatchNorm1d(640),\n",
        "            nn.Tanh(),          \n",
        "            nn.Linear(640, 1000),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1000, 10),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "        self.rnn = nn.RNN(input_dim, 256, 3)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #RNN\n",
        "        #h0 = torch.randn(3, 256).to(device)            \n",
        "        #outRnn, _ = self.rnn(x, h0)\n",
        "        \n",
        "        #outRnn = self.fc2(outRnn)\n",
        "        #outRnn = self.softmax(outRnn)\n",
        "        z = self.layers(x)\n",
        "        #z = self.layers2(z)\n",
        "\n",
        " \n",
        "        #y = self.layers(x)\n",
        "        #x = self.layers(x)\n",
        "        #x = self.maxpoollayer(x)\n",
        "        #x = self.maxpoollayer2(x)\n",
        "        #x = self.maxpoollayer3(x)\n",
        "        #x = self.finallayer(x)\n",
        "        #x = self.softmax(x)\n",
        "        #y = self.finallayer(y)\n",
        "        # = self.softmax(y)\n",
        "        \n",
        "\n",
        "\n",
        "        #x = torch.add(torch.mul(x,0.7),torch.mul(y,0.3))\n",
        "        #x = torch.add(torch.mul(x,0.9),torch.mul(outRnn,0.1))   #把不同model混在一起的\n",
        "\n",
        "        #out = torch.linspace(0,9,10).to(device)\n",
        "        #x = torch.mul(x,out)\n",
        "        #x = torch.sum(x, dim=1)                  \n",
        "                               \n",
        "#        y = np.array(y.tolist())                  ###算中位數的\n",
        "#        temp = np.zeros((y.shape[0]))\n",
        "#        for i in range(y.shape[0]):\n",
        "#          rate = 0\n",
        "#          for j in range(y.shape[1]):\n",
        "#           rate += y[i][j]\n",
        "#            if rate >0.5:\n",
        "#              temp[i] = j \n",
        "#              break\n",
        "#        y = torch.from_numpy(temp).to(device)\n",
        "\n",
        "        z = self.finallayer2(z)\n",
        "        z = self.sigmoid(z)                    ##拿掉可能比較好一點點?但沒差太多\n",
        "        z = self.fc(z)                      ###z \n",
        "        #output = torch.add(torch.mul(x,0.0001),torch.mul(z,0.9999))\n",
        "        #\n",
        "        #z = torch.mul(z,9)\n",
        "        #x = self.fc(x)\n",
        "        output = z\n",
        "        #\n",
        "        \n",
        "        output = output.squeeze(-1) # (B, 1) -> (B)\n",
        "        return output\n",
        "class wideL1loss(nn.Module):                     #####區間的loss\n",
        "  def __init__(self):\n",
        "    super(wideL1loss, self).__init__()\n",
        "  def forward(self, x,y):\n",
        "    loss = torch.mean(torch.subtract(torch.clamp(torch.abs(x-y),2.5,10),2.5))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EXYkYmuQzB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "khMhvW1duO9l"
      },
      "outputs": [],
      "source": [
        "def trainer(train_data, model, config, device):\n",
        "\n",
        "    criterion = nn.SmoothL1Loss(beta=0.5) # Define your loss function, do not modify this.\n",
        "    #criterion = nn.CrossEntropyLoss() \n",
        "    loss2 = wideL1loss()                  ###我用來算區間的loss\n",
        "    criterion2 = nn.L1Loss() \n",
        "\n",
        "    # Define your optimization algorithm. \n",
        "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
        "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr= config['learning_rate'] , weight_decay=0.01, amsgrad=False) \n",
        "    writer = SummaryWriter() # Writer of tensoboard.\n",
        "\n",
        "    if not os.path.isdir('./models'):\n",
        "        os.mkdir('./models') # Create directory of saving models.\n",
        "\n",
        "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
        "\n",
        "\n",
        "    np.random.seed(config['seed'])\n",
        "    splits = np.array_split(train_data, 5)\n",
        "    train =  np.concatenate(splits[:0]+splits[0+1:])\n",
        "    valid= splits[0]\n",
        "    x_train = train[...,1:]                          ###現在沒用cross validation\n",
        "    x_valid = valid[...,1:]\n",
        "    y_train = train[...,0]\n",
        "    y_valid = valid[...,0]\n",
        "    train_dataset  = COVID19Dataset(x_train, y_train)\n",
        "    valid_dataset = COVID19Dataset(x_valid, y_valid)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      lossvalid_record = []\n",
        "      loss_record = []\n",
        "      realloss = []\n",
        "      for i in range(1):\n",
        "        model.train() # Set your model to train mode.\n",
        "         \n",
        "        \n",
        "        # Pytorch data loader loads pytorch dataset into batches.\n",
        "        \n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()               # Set gradient to zero.\n",
        "            x, y = x.to(device), y.to(device)   # Move your data to device. \n",
        "            pred = model(x)\n",
        "            \n",
        "            #loss = criterion(pred, y)\n",
        "            l2_norm = sum((p.pow(2)/(1+p.pow(2))).sum()for p in model.parameters())        ##上課的l2 norm resize 過的不知道加個log會不會更好\n",
        "            loss = criterion(pred, y)+config['regulizer']*l2_norm+config['loss2']*loss2(pred, y)\n",
        "            #print(loss)\n",
        "            loss.backward()                     # Compute gradient(backpropagation).\n",
        "            optimizer.step()                    # Update parameters.\n",
        "            step += 1\n",
        "            \n",
        "            loss_record.append(loss.detach().item())\n",
        " \n",
        "        model.eval() # Set your model to evaluation mode.\n",
        "        \n",
        "        for x, y in valid_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = torch.clamp(model(x),0,9)\n",
        "                #loss = criterion(pred, y)\n",
        "                l2_norm = sum((p.pow(2)/(1+p.pow(2))).sum()for p in model.parameters())\n",
        "                loss = criterion(pred, y)+config['regulizer']*l2_norm+config['loss2']*loss2(pred, y)\n",
        "                realloss2 = criterion2(pred, y)\n",
        "            lossvalid_record.append(loss.item())\n",
        "            realloss.append(realloss2.item())\n",
        "\n",
        "        \n",
        "      mean_train_loss = sum(loss_record)/len(loss_record)         ###compute CV loss\n",
        "      writer.add_scalar('Loss/train', mean_train_loss, step)\n",
        "      reloss = sum(realloss)/len(realloss)\n",
        "      mean_valid_loss = sum(lossvalid_record)/len(lossvalid_record)\n",
        "      print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, L1 loss: {reloss:.4f}')\n",
        "      # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
        "\n",
        "      if mean_valid_loss < best_loss:\n",
        "          best_loss = mean_valid_loss\n",
        "          torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
        "          print('Saving model with loss {:.3f}...'.format(best_loss))\n",
        "          early_stop_count = 0\n",
        "      else: \n",
        "          early_stop_count += 1\n",
        "\n",
        "      if early_stop_count >= config['early_stop']:\n",
        "          print('\\nModel is not improving, so we halt the training session.')\n",
        "          return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Hg8aMlzaqk"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dB14dhABzVxO"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "config = {\n",
        "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
        "    'select_all': True,   # Whether to use all features.\n",
        "    'n_fold': 5,   # validation_size = train_size * valid_ratio\n",
        "    'n_epochs': 30000,     # Number of epochs.            \n",
        "    'batch_size': 32, \n",
        "    'learning_rate': 1e-3,\n",
        "    'regulizer': 1e-4,  \n",
        "    'loss2'  :0,          \n",
        "    'early_stop': 1500,    # If model has not improved for this many consecutive epochs, stop training.     \n",
        "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DqtxFou74eQr"
      },
      "outputs": [],
      "source": [
        "same_seed(config['seed'])\n",
        "\n",
        "#imp_median.fit(test)\n",
        "#test_data = imp_median.transform(test)\n",
        "train_data= feature\n",
        "#train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
        "\n",
        "x_test =  test_data\n",
        "test_dataset = COVID19Dataset(x_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zpP7hF1y18"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn82Vj0R06Yt",
        "outputId": "28af3e22-92d8-42fb-e813-1270b328822e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30000]: Train loss: 3.2312, Valid loss: 1.9385, L1 loss: 2.1316\n",
            "Saving model with loss 1.938...\n",
            "Epoch [2/30000]: Train loss: 1.9352, Valid loss: 1.6952, L1 loss: 1.9223\n",
            "Saving model with loss 1.695...\n",
            "Epoch [3/30000]: Train loss: 1.7668, Valid loss: 1.6431, L1 loss: 1.8777\n",
            "Saving model with loss 1.643...\n",
            "Epoch [4/30000]: Train loss: 1.7161, Valid loss: 1.6107, L1 loss: 1.8448\n",
            "Saving model with loss 1.611...\n",
            "Epoch [5/30000]: Train loss: 1.7039, Valid loss: 1.6127, L1 loss: 1.8449\n",
            "Epoch [6/30000]: Train loss: 1.6868, Valid loss: 1.5785, L1 loss: 1.8097\n",
            "Saving model with loss 1.578...\n",
            "Epoch [7/30000]: Train loss: 1.6780, Valid loss: 1.5436, L1 loss: 1.7738\n",
            "Saving model with loss 1.544...\n",
            "Epoch [8/30000]: Train loss: 1.6753, Valid loss: 1.5358, L1 loss: 1.7668\n",
            "Saving model with loss 1.536...\n",
            "Epoch [9/30000]: Train loss: 1.6650, Valid loss: 1.5575, L1 loss: 1.7901\n",
            "Epoch [10/30000]: Train loss: 1.6559, Valid loss: 1.5537, L1 loss: 1.7867\n",
            "Epoch [11/30000]: Train loss: 1.6635, Valid loss: 1.5428, L1 loss: 1.7743\n",
            "Epoch [12/30000]: Train loss: 1.6505, Valid loss: 1.5766, L1 loss: 1.8097\n",
            "Epoch [13/30000]: Train loss: 1.6603, Valid loss: 1.5488, L1 loss: 1.7811\n",
            "Epoch [14/30000]: Train loss: 1.6596, Valid loss: 1.6015, L1 loss: 1.8352\n",
            "Epoch [15/30000]: Train loss: 1.6423, Valid loss: 1.5911, L1 loss: 1.8243\n",
            "Epoch [16/30000]: Train loss: 1.6557, Valid loss: 1.5118, L1 loss: 1.7433\n",
            "Saving model with loss 1.512...\n",
            "Epoch [17/30000]: Train loss: 1.6525, Valid loss: 1.5274, L1 loss: 1.7596\n",
            "Epoch [18/30000]: Train loss: 1.6540, Valid loss: 1.5659, L1 loss: 1.7984\n",
            "Epoch [19/30000]: Train loss: 1.6462, Valid loss: 1.5558, L1 loss: 1.7879\n",
            "Epoch [20/30000]: Train loss: 1.6469, Valid loss: 1.5223, L1 loss: 1.7536\n",
            "Epoch [21/30000]: Train loss: 1.6459, Valid loss: 1.5191, L1 loss: 1.7512\n",
            "Epoch [22/30000]: Train loss: 1.6365, Valid loss: 1.5457, L1 loss: 1.7771\n",
            "Epoch [23/30000]: Train loss: 1.6345, Valid loss: 1.5183, L1 loss: 1.7490\n",
            "Epoch [24/30000]: Train loss: 1.6338, Valid loss: 1.5334, L1 loss: 1.7647\n",
            "Epoch [25/30000]: Train loss: 1.6359, Valid loss: 1.5185, L1 loss: 1.7493\n",
            "Epoch [26/30000]: Train loss: 1.6477, Valid loss: 1.5093, L1 loss: 1.7399\n",
            "Saving model with loss 1.509...\n",
            "Epoch [27/30000]: Train loss: 1.6274, Valid loss: 1.5362, L1 loss: 1.7679\n",
            "Epoch [28/30000]: Train loss: 1.6320, Valid loss: 1.4996, L1 loss: 1.7290\n",
            "Saving model with loss 1.500...\n",
            "Epoch [29/30000]: Train loss: 1.6258, Valid loss: 1.5321, L1 loss: 1.7631\n",
            "Epoch [30/30000]: Train loss: 1.6302, Valid loss: 1.5716, L1 loss: 1.8043\n",
            "Epoch [31/30000]: Train loss: 1.6184, Valid loss: 1.5483, L1 loss: 1.7801\n",
            "Epoch [32/30000]: Train loss: 1.6457, Valid loss: 1.5317, L1 loss: 1.7625\n",
            "Epoch [33/30000]: Train loss: 1.6244, Valid loss: 1.5047, L1 loss: 1.7356\n",
            "Epoch [34/30000]: Train loss: 1.6205, Valid loss: 1.5391, L1 loss: 1.7702\n",
            "Epoch [35/30000]: Train loss: 1.6298, Valid loss: 1.4992, L1 loss: 1.7285\n",
            "Saving model with loss 1.499...\n",
            "Epoch [36/30000]: Train loss: 1.6327, Valid loss: 1.5274, L1 loss: 1.7590\n",
            "Epoch [37/30000]: Train loss: 1.6252, Valid loss: 1.5090, L1 loss: 1.7398\n",
            "Epoch [38/30000]: Train loss: 1.6285, Valid loss: 1.5494, L1 loss: 1.7818\n",
            "Epoch [39/30000]: Train loss: 1.6359, Valid loss: 1.5031, L1 loss: 1.7330\n",
            "Epoch [40/30000]: Train loss: 1.6233, Valid loss: 1.5176, L1 loss: 1.7483\n",
            "Epoch [41/30000]: Train loss: 1.6223, Valid loss: 1.5278, L1 loss: 1.7585\n",
            "Epoch [42/30000]: Train loss: 1.6407, Valid loss: 1.5110, L1 loss: 1.7405\n",
            "Epoch [43/30000]: Train loss: 1.6313, Valid loss: 1.5186, L1 loss: 1.7497\n",
            "Epoch [44/30000]: Train loss: 1.6176, Valid loss: 1.5360, L1 loss: 1.7663\n",
            "Epoch [45/30000]: Train loss: 1.6140, Valid loss: 1.5041, L1 loss: 1.7320\n",
            "Epoch [46/30000]: Train loss: 1.6333, Valid loss: 1.5413, L1 loss: 1.7725\n",
            "Epoch [47/30000]: Train loss: 1.6353, Valid loss: 1.5099, L1 loss: 1.7394\n",
            "Epoch [48/30000]: Train loss: 1.6241, Valid loss: 1.4877, L1 loss: 1.7171\n",
            "Saving model with loss 1.488...\n",
            "Epoch [49/30000]: Train loss: 1.6295, Valid loss: 1.5076, L1 loss: 1.7366\n",
            "Epoch [50/30000]: Train loss: 1.6300, Valid loss: 1.4971, L1 loss: 1.7264\n",
            "Epoch [51/30000]: Train loss: 1.6297, Valid loss: 1.5562, L1 loss: 1.7882\n",
            "Epoch [52/30000]: Train loss: 1.6183, Valid loss: 1.5143, L1 loss: 1.7438\n",
            "Epoch [53/30000]: Train loss: 1.6198, Valid loss: 1.5117, L1 loss: 1.7418\n",
            "Epoch [54/30000]: Train loss: 1.6279, Valid loss: 1.5027, L1 loss: 1.7321\n",
            "Epoch [55/30000]: Train loss: 1.6336, Valid loss: 1.5359, L1 loss: 1.7674\n",
            "Epoch [56/30000]: Train loss: 1.6125, Valid loss: 1.5091, L1 loss: 1.7377\n",
            "Epoch [57/30000]: Train loss: 1.6271, Valid loss: 1.5346, L1 loss: 1.7640\n",
            "Epoch [58/30000]: Train loss: 1.6174, Valid loss: 1.5702, L1 loss: 1.8015\n",
            "Epoch [59/30000]: Train loss: 1.6291, Valid loss: 1.5611, L1 loss: 1.7930\n",
            "Epoch [60/30000]: Train loss: 1.6209, Valid loss: 1.5050, L1 loss: 1.7347\n",
            "Epoch [61/30000]: Train loss: 1.6219, Valid loss: 1.6133, L1 loss: 1.8446\n",
            "Epoch [62/30000]: Train loss: 1.6291, Valid loss: 1.4925, L1 loss: 1.7193\n",
            "Epoch [63/30000]: Train loss: 1.6148, Valid loss: 1.5122, L1 loss: 1.7410\n",
            "Epoch [64/30000]: Train loss: 1.6212, Valid loss: 1.5099, L1 loss: 1.7387\n",
            "Epoch [65/30000]: Train loss: 1.6116, Valid loss: 1.5286, L1 loss: 1.7584\n",
            "Epoch [66/30000]: Train loss: 1.6291, Valid loss: 1.4996, L1 loss: 1.7283\n",
            "Epoch [67/30000]: Train loss: 1.6308, Valid loss: 1.5303, L1 loss: 1.7604\n",
            "Epoch [68/30000]: Train loss: 1.6160, Valid loss: 1.4977, L1 loss: 1.7251\n",
            "Epoch [69/30000]: Train loss: 1.6284, Valid loss: 1.4901, L1 loss: 1.7196\n",
            "Epoch [70/30000]: Train loss: 1.6275, Valid loss: 1.5130, L1 loss: 1.7430\n",
            "Epoch [71/30000]: Train loss: 1.6153, Valid loss: 1.5000, L1 loss: 1.7281\n",
            "Epoch [72/30000]: Train loss: 1.6281, Valid loss: 1.5350, L1 loss: 1.7654\n",
            "Epoch [73/30000]: Train loss: 1.6231, Valid loss: 1.5039, L1 loss: 1.7343\n",
            "Epoch [74/30000]: Train loss: 1.6228, Valid loss: 1.5203, L1 loss: 1.7501\n",
            "Epoch [75/30000]: Train loss: 1.6052, Valid loss: 1.5216, L1 loss: 1.7502\n",
            "Epoch [76/30000]: Train loss: 1.6143, Valid loss: 1.5297, L1 loss: 1.7598\n",
            "Epoch [77/30000]: Train loss: 1.6223, Valid loss: 1.5216, L1 loss: 1.7498\n",
            "Epoch [78/30000]: Train loss: 1.6184, Valid loss: 1.5154, L1 loss: 1.7452\n",
            "Epoch [79/30000]: Train loss: 1.6190, Valid loss: 1.5445, L1 loss: 1.7747\n",
            "Epoch [80/30000]: Train loss: 1.6233, Valid loss: 1.5237, L1 loss: 1.7541\n",
            "Epoch [81/30000]: Train loss: 1.6254, Valid loss: 1.4847, L1 loss: 1.7134\n",
            "Saving model with loss 1.485...\n",
            "Epoch [82/30000]: Train loss: 1.6137, Valid loss: 1.4993, L1 loss: 1.7265\n",
            "Epoch [83/30000]: Train loss: 1.6212, Valid loss: 1.5147, L1 loss: 1.7437\n",
            "Epoch [84/30000]: Train loss: 1.6255, Valid loss: 1.5325, L1 loss: 1.7606\n",
            "Epoch [85/30000]: Train loss: 1.6235, Valid loss: 1.5519, L1 loss: 1.7823\n",
            "Epoch [86/30000]: Train loss: 1.6229, Valid loss: 1.5072, L1 loss: 1.7356\n",
            "Epoch [87/30000]: Train loss: 1.6179, Valid loss: 1.5112, L1 loss: 1.7399\n",
            "Epoch [88/30000]: Train loss: 1.6195, Valid loss: 1.5215, L1 loss: 1.7498\n",
            "Epoch [89/30000]: Train loss: 1.6233, Valid loss: 1.4886, L1 loss: 1.7176\n",
            "Epoch [90/30000]: Train loss: 1.6243, Valid loss: 1.4854, L1 loss: 1.7135\n",
            "Epoch [91/30000]: Train loss: 1.6244, Valid loss: 1.4926, L1 loss: 1.7212\n",
            "Epoch [92/30000]: Train loss: 1.6196, Valid loss: 1.4988, L1 loss: 1.7269\n",
            "Epoch [93/30000]: Train loss: 1.6077, Valid loss: 1.4890, L1 loss: 1.7159\n",
            "Epoch [94/30000]: Train loss: 1.6136, Valid loss: 1.4978, L1 loss: 1.7260\n",
            "Epoch [95/30000]: Train loss: 1.6119, Valid loss: 1.5174, L1 loss: 1.7470\n",
            "Epoch [96/30000]: Train loss: 1.6156, Valid loss: 1.5161, L1 loss: 1.7436\n",
            "Epoch [97/30000]: Train loss: 1.6277, Valid loss: 1.4962, L1 loss: 1.7245\n",
            "Epoch [98/30000]: Train loss: 1.6171, Valid loss: 1.4925, L1 loss: 1.7207\n",
            "Epoch [99/30000]: Train loss: 1.6136, Valid loss: 1.5236, L1 loss: 1.7534\n",
            "Epoch [100/30000]: Train loss: 1.6289, Valid loss: 1.5206, L1 loss: 1.7509\n",
            "Epoch [101/30000]: Train loss: 1.6155, Valid loss: 1.4979, L1 loss: 1.7261\n",
            "Epoch [102/30000]: Train loss: 1.6121, Valid loss: 1.5048, L1 loss: 1.7337\n",
            "Epoch [103/30000]: Train loss: 1.6183, Valid loss: 1.4974, L1 loss: 1.7250\n",
            "Epoch [104/30000]: Train loss: 1.6131, Valid loss: 1.5005, L1 loss: 1.7293\n",
            "Epoch [105/30000]: Train loss: 1.6052, Valid loss: 1.5338, L1 loss: 1.7623\n",
            "Epoch [106/30000]: Train loss: 1.6216, Valid loss: 1.5000, L1 loss: 1.7289\n",
            "Epoch [107/30000]: Train loss: 1.6212, Valid loss: 1.4753, L1 loss: 1.7043\n",
            "Saving model with loss 1.475...\n",
            "Epoch [108/30000]: Train loss: 1.6247, Valid loss: 1.5187, L1 loss: 1.7491\n",
            "Epoch [109/30000]: Train loss: 1.6143, Valid loss: 1.5244, L1 loss: 1.7549\n",
            "Epoch [110/30000]: Train loss: 1.6034, Valid loss: 1.5192, L1 loss: 1.7490\n",
            "Epoch [111/30000]: Train loss: 1.6202, Valid loss: 1.4986, L1 loss: 1.7271\n",
            "Epoch [112/30000]: Train loss: 1.6129, Valid loss: 1.5102, L1 loss: 1.7383\n",
            "Epoch [113/30000]: Train loss: 1.6180, Valid loss: 1.5179, L1 loss: 1.7471\n",
            "Epoch [114/30000]: Train loss: 1.6177, Valid loss: 1.5103, L1 loss: 1.7392\n",
            "Epoch [115/30000]: Train loss: 1.6010, Valid loss: 1.4969, L1 loss: 1.7252\n",
            "Epoch [116/30000]: Train loss: 1.6160, Valid loss: 1.5009, L1 loss: 1.7276\n",
            "Epoch [117/30000]: Train loss: 1.6099, Valid loss: 1.4974, L1 loss: 1.7265\n",
            "Epoch [118/30000]: Train loss: 1.6148, Valid loss: 1.4802, L1 loss: 1.7074\n",
            "Epoch [119/30000]: Train loss: 1.6127, Valid loss: 1.4943, L1 loss: 1.7228\n",
            "Epoch [120/30000]: Train loss: 1.6088, Valid loss: 1.4801, L1 loss: 1.7073\n",
            "Epoch [121/30000]: Train loss: 1.6148, Valid loss: 1.4934, L1 loss: 1.7203\n",
            "Epoch [122/30000]: Train loss: 1.6193, Valid loss: 1.5367, L1 loss: 1.7667\n",
            "Epoch [123/30000]: Train loss: 1.6133, Valid loss: 1.5054, L1 loss: 1.7331\n",
            "Epoch [124/30000]: Train loss: 1.6104, Valid loss: 1.4789, L1 loss: 1.7066\n",
            "Epoch [125/30000]: Train loss: 1.6208, Valid loss: 1.5082, L1 loss: 1.7381\n",
            "Epoch [126/30000]: Train loss: 1.6164, Valid loss: 1.5171, L1 loss: 1.7457\n",
            "Epoch [127/30000]: Train loss: 1.6046, Valid loss: 1.4769, L1 loss: 1.7050\n",
            "Epoch [128/30000]: Train loss: 1.6138, Valid loss: 1.4817, L1 loss: 1.7085\n",
            "Epoch [129/30000]: Train loss: 1.6120, Valid loss: 1.5247, L1 loss: 1.7549\n",
            "Epoch [130/30000]: Train loss: 1.6139, Valid loss: 1.4783, L1 loss: 1.7068\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = My_Model(input_dim=train_data.shape[1]-1).to(device) # put your model and data on the same computation device.\n",
        "trainer(train_data, model, config, device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNnGeuu72qdu"
      },
      "outputs": [],
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'Danceability'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i+17170, p])\n",
        "model.load_state_dict(torch.load(config['save_path']))\n",
        "preds = predict(test_loader, model, device)\n",
        "print(preds)\n",
        "save_pred(preds, 'sample_submission.csv')\n",
        "from google.colab import files\n",
        "files.download('sample_submission.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}