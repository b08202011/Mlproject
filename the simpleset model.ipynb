{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b08202011/Mlproject/blob/main/the%20simpleset%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klC1qgz2Oi2",
        "outputId": "40f5cea1-d89a-42ab-bbd7-c0e3f1a223ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
            "To: /content/train.csv\n",
            "100% 23.3M/23.3M [00:00<00:00, 138MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
            "To: /content/test.csv\n",
            "100% 8.88M/8.88M [00:00<00:00, 122MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=187gijxd4T5yuZe_g2K9UNepx7MT1CnAu\n",
            "To: /content/sample_submission.csv\n",
            "100% 50.5k/50.5k [00:00<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
        "!gdown 15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
        "!gdown 187gijxd4T5yuZe_g2K9UNepx7MT1CnAu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "10JsWM_brgQw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "with open('train.csv',newline='') as csvfile:\n",
        "  train = csv.reader(csvfile)\n",
        "  train = list(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DscDJdCVr0kH",
        "outputId": "1a36a67c-3339-480a-8504-1c7b6950eaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n"
          ]
        }
      ],
      "source": [
        "pick = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23]\n",
        "#可以選要用那些特徵，0是我們要predict的\n",
        "#只選數值或true fale 但且沒有選id\n",
        "feature = []\n",
        "for i in range(1,len(train)):\n",
        "  feature.append([])\n",
        "  for j in pick:\n",
        "    feature[i-1].append(train[i][j])\n",
        "\n",
        "for i in range(len(feature)):\n",
        "  for j in range(len(feature[i])):\n",
        "    if(feature[i][j]!='' and feature[i][j]!='False' and feature[i][j]!= 'True'):\n",
        "      feature[i][j] = float(feature[i][j])\n",
        "    elif(feature[i][j] == 'False'):\n",
        "      feature[i][j] = 0\n",
        "    elif(feature[i][j] == 'True'):\n",
        "      feature[i][j] = 1\n",
        "    else:\n",
        "      feature[i][j] = np.nan\n",
        "print(len(feature))\n",
        "\n",
        "\n",
        "with open('test.csv',newline='') as csvfile:         ###load testdata\n",
        "  testcsv = csv.reader(csvfile)\n",
        "  testcsv = list(testcsv)\n",
        "  \n",
        "test = []\n",
        "picktest = [0,1,2,3,4,5,6,7,8,9,10,11,12,22]\n",
        "\n",
        "for i in range(1,len(testcsv)):\n",
        "  test.append([])\n",
        "  for j in picktest:\n",
        "    test[i-1].append(testcsv[i][j])\n",
        "for i in range(len(test)):\n",
        "  for j in range(len(test[i])):\n",
        "    if(test[i][j]!='' and test[i][j]!='False' and test[i][j]!= 'True'):\n",
        "      test[i][j] = float(test[i][j])\n",
        "    elif(test[i][j] == 'False'):\n",
        "      test[i][j] = 0\n",
        "    elif(test[i][j] == 'True'):\n",
        "      test[i][j] = 1\n",
        "    else:\n",
        "      test[i][j] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFwj7Vtjr7C3",
        "outputId": "9e9e4d5f-f3c2-4964-bcc3-f0c0307713dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ycimpute\n",
            "  Downloading ycimpute-0.2-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (2.0.1+cu118)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->ycimpute) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->ycimpute) (1.3.0)\n",
            "Installing collected packages: ycimpute\n",
            "Successfully installed ycimpute-0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ycimpute"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Operations\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Reading/Writing Data\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "# For Progress Bar\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import f_regression\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
        "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "# For plotting learning curve\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.functional import normalize"
      ],
      "metadata": {
        "id": "vnSka536aDrP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpQkUiqtr-VA",
        "outputId": "d64eabd7-99f3-4bd8-9149-1bcad15f1ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n",
            "6315\n",
            "(23485, 14)\n",
            "[[ 2.73359449e-04  3.00000000e+00 -2.51760000e+01 ...  1.37622000e+05\n",
            "   1.99813700e+07  3.07000000e+03]\n",
            " [ 1.84220009e-01  7.00000000e+00             nan ...  5.19000000e+03\n",
            "              nan  1.22000000e+02]\n",
            " [            nan  6.00000000e+00 -1.55960000e+01 ...  5.19000000e+03\n",
            "   1.93208100e+07  1.22000000e+02]\n",
            " ...\n",
            " [ 4.26222894e-01  5.00000000e+00 -5.80754484e+00 ...  2.30516298e+04\n",
            "   1.84909433e+07             nan]\n",
            " [ 2.04440254e-01             nan -1.15639636e+01 ...             nan\n",
            "   1.19959340e+08  1.51390000e+04]\n",
            " [ 1.46792398e-02             nan             nan ...  9.23548143e+04\n",
            "   3.43928191e+06  1.10000000e+01]]\n"
          ]
        }
      ],
      "source": [
        "feature = np.array(feature)\n",
        "test = np.array(test)\n",
        "from ycimpute.imputer import knnimput\n",
        "xtrain, ytrain = feature[:,1:], feature[:,0]\n",
        "print(xtrain.shape[0])\n",
        "print(test.shape[0])\n",
        "data = np.concatenate((xtrain,test))\n",
        "print(data.shape)\n",
        "\n",
        "data_np = torch.from_numpy(data)\n",
        "mean, std, var = torch.mean(data_np), torch.std(data_np), torch.var(data_np)\n",
        "data_np = (data_np-mean)/std\n",
        "data_np = np.array(data.tolist())\n",
        "print(data_np)\n",
        "#imp_mean.fit(data_np)\n",
        "#data_np = imp_mean.transform(data_np)\n",
        "\n",
        "\n",
        "#imp_median.fit(feature)\n",
        "#feature = imp_median.transform(feature)\n",
        "##可以修改這裡 使用不同的data填補方式"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = knnimput.KNN(k=4).complete(data_np)"
      ],
      "metadata": {
        "id": "Pae3xPxJdB_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89dc5f7-5325-4ad2-b037-8637afa134ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing row 1/23485 with 1 missing, elapsed time: 127.416\n",
            "Imputing row 101/23485 with 1 missing, elapsed time: 127.436\n",
            "Imputing row 201/23485 with 0 missing, elapsed time: 127.459\n",
            "Imputing row 301/23485 with 2 missing, elapsed time: 127.478\n",
            "Imputing row 401/23485 with 1 missing, elapsed time: 127.498\n",
            "Imputing row 501/23485 with 1 missing, elapsed time: 127.517\n",
            "Imputing row 601/23485 with 4 missing, elapsed time: 127.539\n",
            "Imputing row 701/23485 with 1 missing, elapsed time: 127.565\n",
            "Imputing row 801/23485 with 2 missing, elapsed time: 127.588\n",
            "Imputing row 901/23485 with 3 missing, elapsed time: 127.610\n",
            "Imputing row 1001/23485 with 0 missing, elapsed time: 127.636\n",
            "Imputing row 1101/23485 with 0 missing, elapsed time: 127.656\n",
            "Imputing row 1201/23485 with 2 missing, elapsed time: 127.677\n",
            "Imputing row 1301/23485 with 2 missing, elapsed time: 127.701\n",
            "Imputing row 1401/23485 with 2 missing, elapsed time: 127.723\n",
            "Imputing row 1501/23485 with 3 missing, elapsed time: 127.746\n",
            "Imputing row 1601/23485 with 2 missing, elapsed time: 127.768\n",
            "Imputing row 1701/23485 with 1 missing, elapsed time: 127.791\n",
            "Imputing row 1801/23485 with 2 missing, elapsed time: 127.811\n",
            "Imputing row 1901/23485 with 0 missing, elapsed time: 127.842\n",
            "Imputing row 2001/23485 with 1 missing, elapsed time: 127.878\n",
            "Imputing row 2101/23485 with 5 missing, elapsed time: 127.911\n",
            "Imputing row 2201/23485 with 3 missing, elapsed time: 127.947\n",
            "Imputing row 2301/23485 with 2 missing, elapsed time: 127.980\n",
            "Imputing row 2401/23485 with 3 missing, elapsed time: 128.016\n",
            "Imputing row 2501/23485 with 0 missing, elapsed time: 128.053\n",
            "Imputing row 2601/23485 with 1 missing, elapsed time: 128.089\n",
            "Imputing row 2701/23485 with 1 missing, elapsed time: 128.125\n",
            "Imputing row 2801/23485 with 2 missing, elapsed time: 128.165\n",
            "Imputing row 2901/23485 with 0 missing, elapsed time: 128.205\n",
            "Imputing row 3001/23485 with 2 missing, elapsed time: 128.247\n",
            "Imputing row 3101/23485 with 1 missing, elapsed time: 128.282\n",
            "Imputing row 3201/23485 with 3 missing, elapsed time: 128.317\n",
            "Imputing row 3301/23485 with 3 missing, elapsed time: 128.348\n",
            "Imputing row 3401/23485 with 4 missing, elapsed time: 128.379\n",
            "Imputing row 3501/23485 with 1 missing, elapsed time: 128.412\n",
            "Imputing row 3601/23485 with 5 missing, elapsed time: 128.444\n",
            "Imputing row 3701/23485 with 2 missing, elapsed time: 128.476\n",
            "Imputing row 3801/23485 with 2 missing, elapsed time: 128.507\n",
            "Imputing row 3901/23485 with 3 missing, elapsed time: 128.535\n",
            "Imputing row 4001/23485 with 1 missing, elapsed time: 128.568\n",
            "Imputing row 4101/23485 with 2 missing, elapsed time: 128.599\n",
            "Imputing row 4201/23485 with 2 missing, elapsed time: 128.630\n",
            "Imputing row 4301/23485 with 2 missing, elapsed time: 128.662\n",
            "Imputing row 4401/23485 with 1 missing, elapsed time: 128.695\n",
            "Imputing row 4501/23485 with 1 missing, elapsed time: 128.728\n",
            "Imputing row 4601/23485 with 3 missing, elapsed time: 128.756\n",
            "Imputing row 4701/23485 with 2 missing, elapsed time: 128.787\n",
            "Imputing row 4801/23485 with 2 missing, elapsed time: 128.821\n",
            "Imputing row 4901/23485 with 3 missing, elapsed time: 128.854\n",
            "Imputing row 5001/23485 with 3 missing, elapsed time: 128.885\n",
            "Imputing row 5101/23485 with 3 missing, elapsed time: 128.922\n",
            "Imputing row 5201/23485 with 2 missing, elapsed time: 128.957\n",
            "Imputing row 5301/23485 with 1 missing, elapsed time: 128.997\n",
            "Imputing row 5401/23485 with 4 missing, elapsed time: 129.029\n",
            "Imputing row 5501/23485 with 4 missing, elapsed time: 129.063\n",
            "Imputing row 5601/23485 with 2 missing, elapsed time: 129.095\n",
            "Imputing row 5701/23485 with 2 missing, elapsed time: 129.128\n",
            "Imputing row 5801/23485 with 3 missing, elapsed time: 129.163\n",
            "Imputing row 5901/23485 with 2 missing, elapsed time: 129.200\n",
            "Imputing row 6001/23485 with 2 missing, elapsed time: 129.233\n",
            "Imputing row 6101/23485 with 2 missing, elapsed time: 129.264\n",
            "Imputing row 6201/23485 with 3 missing, elapsed time: 129.296\n",
            "Imputing row 6301/23485 with 1 missing, elapsed time: 129.325\n",
            "Imputing row 6401/23485 with 3 missing, elapsed time: 129.359\n",
            "Imputing row 6501/23485 with 1 missing, elapsed time: 129.397\n",
            "Imputing row 6601/23485 with 2 missing, elapsed time: 129.436\n",
            "Imputing row 6701/23485 with 1 missing, elapsed time: 129.469\n",
            "Imputing row 6801/23485 with 0 missing, elapsed time: 129.497\n",
            "Imputing row 6901/23485 with 3 missing, elapsed time: 129.538\n",
            "Imputing row 7001/23485 with 1 missing, elapsed time: 129.575\n",
            "Imputing row 7101/23485 with 2 missing, elapsed time: 129.616\n",
            "Imputing row 7201/23485 with 0 missing, elapsed time: 129.651\n",
            "Imputing row 7301/23485 with 2 missing, elapsed time: 129.686\n",
            "Imputing row 7401/23485 with 2 missing, elapsed time: 129.721\n",
            "Imputing row 7501/23485 with 1 missing, elapsed time: 129.763\n",
            "Imputing row 7601/23485 with 4 missing, elapsed time: 129.802\n",
            "Imputing row 7701/23485 with 2 missing, elapsed time: 129.828\n",
            "Imputing row 7801/23485 with 1 missing, elapsed time: 129.850\n",
            "Imputing row 7901/23485 with 1 missing, elapsed time: 129.873\n",
            "Imputing row 8001/23485 with 1 missing, elapsed time: 129.894\n",
            "Imputing row 8101/23485 with 4 missing, elapsed time: 129.917\n",
            "Imputing row 8201/23485 with 2 missing, elapsed time: 129.941\n",
            "Imputing row 8301/23485 with 3 missing, elapsed time: 129.962\n",
            "Imputing row 8401/23485 with 1 missing, elapsed time: 129.982\n",
            "Imputing row 8501/23485 with 2 missing, elapsed time: 130.002\n",
            "Imputing row 8601/23485 with 2 missing, elapsed time: 130.021\n",
            "Imputing row 8701/23485 with 3 missing, elapsed time: 130.051\n",
            "Imputing row 8801/23485 with 2 missing, elapsed time: 130.072\n",
            "Imputing row 8901/23485 with 3 missing, elapsed time: 130.095\n",
            "Imputing row 9001/23485 with 1 missing, elapsed time: 130.115\n",
            "Imputing row 9101/23485 with 1 missing, elapsed time: 130.137\n",
            "Imputing row 9201/23485 with 1 missing, elapsed time: 130.158\n",
            "Imputing row 9301/23485 with 5 missing, elapsed time: 130.179\n",
            "Imputing row 9401/23485 with 6 missing, elapsed time: 130.201\n",
            "Imputing row 9501/23485 with 0 missing, elapsed time: 130.220\n",
            "Imputing row 9601/23485 with 3 missing, elapsed time: 130.240\n",
            "Imputing row 9701/23485 with 3 missing, elapsed time: 130.262\n",
            "Imputing row 9801/23485 with 2 missing, elapsed time: 130.281\n",
            "Imputing row 9901/23485 with 1 missing, elapsed time: 130.300\n",
            "Imputing row 10001/23485 with 3 missing, elapsed time: 130.320\n",
            "Imputing row 10101/23485 with 3 missing, elapsed time: 130.338\n",
            "Imputing row 10201/23485 with 3 missing, elapsed time: 130.357\n",
            "Imputing row 10301/23485 with 1 missing, elapsed time: 130.380\n",
            "Imputing row 10401/23485 with 1 missing, elapsed time: 130.399\n",
            "Imputing row 10501/23485 with 3 missing, elapsed time: 130.420\n",
            "Imputing row 10601/23485 with 2 missing, elapsed time: 130.443\n",
            "Imputing row 10701/23485 with 1 missing, elapsed time: 130.468\n",
            "Imputing row 10801/23485 with 3 missing, elapsed time: 130.488\n",
            "Imputing row 10901/23485 with 1 missing, elapsed time: 130.507\n",
            "Imputing row 11001/23485 with 2 missing, elapsed time: 130.528\n",
            "Imputing row 11101/23485 with 3 missing, elapsed time: 130.550\n",
            "Imputing row 11201/23485 with 1 missing, elapsed time: 130.574\n",
            "Imputing row 11301/23485 with 1 missing, elapsed time: 130.597\n",
            "Imputing row 11401/23485 with 2 missing, elapsed time: 130.622\n",
            "Imputing row 11501/23485 with 2 missing, elapsed time: 130.641\n",
            "Imputing row 11601/23485 with 2 missing, elapsed time: 130.658\n",
            "Imputing row 11701/23485 with 1 missing, elapsed time: 130.682\n",
            "Imputing row 11801/23485 with 3 missing, elapsed time: 130.704\n",
            "Imputing row 11901/23485 with 2 missing, elapsed time: 130.723\n",
            "Imputing row 12001/23485 with 2 missing, elapsed time: 130.745\n",
            "Imputing row 12101/23485 with 1 missing, elapsed time: 130.767\n",
            "Imputing row 12201/23485 with 3 missing, elapsed time: 130.788\n",
            "Imputing row 12301/23485 with 2 missing, elapsed time: 130.809\n",
            "Imputing row 12401/23485 with 5 missing, elapsed time: 130.834\n",
            "Imputing row 12501/23485 with 1 missing, elapsed time: 130.854\n",
            "Imputing row 12601/23485 with 4 missing, elapsed time: 130.876\n",
            "Imputing row 12701/23485 with 1 missing, elapsed time: 130.899\n",
            "Imputing row 12801/23485 with 3 missing, elapsed time: 130.918\n",
            "Imputing row 12901/23485 with 2 missing, elapsed time: 130.937\n",
            "Imputing row 13001/23485 with 2 missing, elapsed time: 130.957\n",
            "Imputing row 13101/23485 with 1 missing, elapsed time: 130.979\n",
            "Imputing row 13201/23485 with 2 missing, elapsed time: 130.998\n",
            "Imputing row 13301/23485 with 1 missing, elapsed time: 131.033\n",
            "Imputing row 13401/23485 with 2 missing, elapsed time: 131.055\n",
            "Imputing row 13501/23485 with 3 missing, elapsed time: 131.073\n",
            "Imputing row 13601/23485 with 2 missing, elapsed time: 131.092\n",
            "Imputing row 13701/23485 with 3 missing, elapsed time: 131.115\n",
            "Imputing row 13801/23485 with 3 missing, elapsed time: 131.138\n",
            "Imputing row 13901/23485 with 1 missing, elapsed time: 131.158\n",
            "Imputing row 14001/23485 with 5 missing, elapsed time: 131.179\n",
            "Imputing row 14101/23485 with 1 missing, elapsed time: 131.204\n",
            "Imputing row 14201/23485 with 0 missing, elapsed time: 131.225\n",
            "Imputing row 14301/23485 with 2 missing, elapsed time: 131.245\n",
            "Imputing row 14401/23485 with 2 missing, elapsed time: 131.267\n",
            "Imputing row 14501/23485 with 1 missing, elapsed time: 131.287\n",
            "Imputing row 14601/23485 with 0 missing, elapsed time: 131.309\n",
            "Imputing row 14701/23485 with 1 missing, elapsed time: 131.332\n",
            "Imputing row 14801/23485 with 2 missing, elapsed time: 131.353\n",
            "Imputing row 14901/23485 with 2 missing, elapsed time: 131.372\n",
            "Imputing row 15001/23485 with 2 missing, elapsed time: 131.393\n",
            "Imputing row 15101/23485 with 1 missing, elapsed time: 131.413\n",
            "Imputing row 15201/23485 with 1 missing, elapsed time: 131.435\n",
            "Imputing row 15301/23485 with 3 missing, elapsed time: 131.454\n",
            "Imputing row 15401/23485 with 1 missing, elapsed time: 131.475\n",
            "Imputing row 15501/23485 with 2 missing, elapsed time: 131.496\n",
            "Imputing row 15601/23485 with 1 missing, elapsed time: 131.517\n",
            "Imputing row 15701/23485 with 2 missing, elapsed time: 131.541\n",
            "Imputing row 15801/23485 with 2 missing, elapsed time: 131.561\n",
            "Imputing row 15901/23485 with 2 missing, elapsed time: 131.581\n",
            "Imputing row 16001/23485 with 1 missing, elapsed time: 131.601\n",
            "Imputing row 16101/23485 with 2 missing, elapsed time: 131.620\n",
            "Imputing row 16201/23485 with 0 missing, elapsed time: 131.647\n",
            "Imputing row 16301/23485 with 1 missing, elapsed time: 131.668\n",
            "Imputing row 16401/23485 with 3 missing, elapsed time: 131.687\n",
            "Imputing row 16501/23485 with 1 missing, elapsed time: 131.705\n",
            "Imputing row 16601/23485 with 3 missing, elapsed time: 131.724\n",
            "Imputing row 16701/23485 with 2 missing, elapsed time: 131.748\n",
            "Imputing row 16801/23485 with 2 missing, elapsed time: 131.769\n",
            "Imputing row 16901/23485 with 3 missing, elapsed time: 131.788\n",
            "Imputing row 17001/23485 with 2 missing, elapsed time: 131.818\n",
            "Imputing row 17101/23485 with 5 missing, elapsed time: 131.962\n",
            "Imputing row 17201/23485 with 3 missing, elapsed time: 132.081\n",
            "Imputing row 17301/23485 with 1 missing, elapsed time: 132.189\n",
            "Imputing row 17401/23485 with 4 missing, elapsed time: 132.300\n",
            "Imputing row 17501/23485 with 3 missing, elapsed time: 132.398\n",
            "Imputing row 17601/23485 with 0 missing, elapsed time: 132.485\n",
            "Imputing row 17701/23485 with 0 missing, elapsed time: 132.577\n",
            "Imputing row 17801/23485 with 4 missing, elapsed time: 132.702\n",
            "Imputing row 17901/23485 with 1 missing, elapsed time: 132.819\n",
            "Imputing row 18001/23485 with 1 missing, elapsed time: 132.937\n",
            "Imputing row 18101/23485 with 5 missing, elapsed time: 133.055\n",
            "Imputing row 18201/23485 with 2 missing, elapsed time: 133.166\n",
            "Imputing row 18301/23485 with 6 missing, elapsed time: 133.272\n",
            "Imputing row 18401/23485 with 4 missing, elapsed time: 133.377\n",
            "Imputing row 18501/23485 with 1 missing, elapsed time: 133.482\n",
            "Imputing row 18601/23485 with 0 missing, elapsed time: 133.603\n",
            "Imputing row 18701/23485 with 4 missing, elapsed time: 133.735\n",
            "Imputing row 18801/23485 with 3 missing, elapsed time: 133.860\n",
            "Imputing row 18901/23485 with 1 missing, elapsed time: 133.967\n",
            "Imputing row 19001/23485 with 1 missing, elapsed time: 134.091\n",
            "Imputing row 19101/23485 with 1 missing, elapsed time: 134.190\n",
            "Imputing row 19201/23485 with 2 missing, elapsed time: 134.307\n",
            "Imputing row 19301/23485 with 1 missing, elapsed time: 134.423\n",
            "Imputing row 19401/23485 with 5 missing, elapsed time: 134.528\n",
            "Imputing row 19501/23485 with 2 missing, elapsed time: 134.646\n",
            "Imputing row 19601/23485 with 5 missing, elapsed time: 134.709\n",
            "Imputing row 19701/23485 with 3 missing, elapsed time: 134.729\n",
            "Imputing row 19801/23485 with 3 missing, elapsed time: 134.752\n",
            "Imputing row 19901/23485 with 2 missing, elapsed time: 134.781\n",
            "Imputing row 20001/23485 with 3 missing, elapsed time: 134.813\n",
            "Imputing row 20101/23485 with 2 missing, elapsed time: 134.835\n",
            "Imputing row 20201/23485 with 0 missing, elapsed time: 134.857\n",
            "Imputing row 20301/23485 with 1 missing, elapsed time: 134.878\n",
            "Imputing row 20401/23485 with 3 missing, elapsed time: 134.900\n",
            "Imputing row 20501/23485 with 1 missing, elapsed time: 134.921\n",
            "Imputing row 20601/23485 with 2 missing, elapsed time: 134.944\n",
            "Imputing row 20701/23485 with 0 missing, elapsed time: 134.967\n",
            "Imputing row 20801/23485 with 0 missing, elapsed time: 134.989\n",
            "Imputing row 20901/23485 with 2 missing, elapsed time: 135.010\n",
            "Imputing row 21001/23485 with 2 missing, elapsed time: 135.029\n",
            "Imputing row 21101/23485 with 4 missing, elapsed time: 135.051\n",
            "Imputing row 21201/23485 with 2 missing, elapsed time: 135.071\n",
            "Imputing row 21301/23485 with 2 missing, elapsed time: 135.089\n",
            "Imputing row 21401/23485 with 3 missing, elapsed time: 135.109\n",
            "Imputing row 21501/23485 with 2 missing, elapsed time: 135.131\n",
            "Imputing row 21601/23485 with 0 missing, elapsed time: 135.150\n",
            "Imputing row 21701/23485 with 2 missing, elapsed time: 135.173\n",
            "Imputing row 21801/23485 with 4 missing, elapsed time: 135.194\n",
            "Imputing row 21901/23485 with 2 missing, elapsed time: 135.216\n",
            "Imputing row 22001/23485 with 0 missing, elapsed time: 135.235\n",
            "Imputing row 22101/23485 with 3 missing, elapsed time: 135.254\n",
            "Imputing row 22201/23485 with 2 missing, elapsed time: 135.273\n",
            "Imputing row 22301/23485 with 1 missing, elapsed time: 135.294\n",
            "Imputing row 22401/23485 with 2 missing, elapsed time: 135.315\n",
            "Imputing row 22501/23485 with 1 missing, elapsed time: 135.336\n",
            "Imputing row 22601/23485 with 1 missing, elapsed time: 135.357\n",
            "Imputing row 22701/23485 with 1 missing, elapsed time: 135.381\n",
            "Imputing row 22801/23485 with 2 missing, elapsed time: 135.404\n",
            "Imputing row 22901/23485 with 3 missing, elapsed time: 135.428\n",
            "Imputing row 23001/23485 with 3 missing, elapsed time: 135.449\n",
            "Imputing row 23101/23485 with 2 missing, elapsed time: 135.471\n",
            "Imputing row 23201/23485 with 0 missing, elapsed time: 135.492\n",
            "Imputing row 23301/23485 with 2 missing, elapsed time: 135.513\n",
            "Imputing row 23401/23485 with 3 missing, elapsed time: 135.534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_np[3])\n",
        "feature, test_data = data_np[:xtrain.shape[0],:], data_np[xtrain.shape[0]:,:]\n",
        "print(feature.shape)\n",
        "ytrain = np.reshape(ytrain,(ytrain.shape[0],1))\n",
        "feature = np.concatenate((ytrain,feature),axis=1)\n",
        "print(feature.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo7Ka39Qbzvz",
        "outputId": "7602c1b8-022f-4d38-9404-14e05f19ef7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.09584584e-01  7.23135868e+00 -6.25100000e+00  2.77000000e-02\n",
            "  3.79641600e-03  9.33999086e-02  1.00000000e-03  4.31448300e-01\n",
            "  1.50220000e+02  2.65000000e+05  2.02457327e+08  9.97035000e+05\n",
            "  3.99661898e+08  2.43500000e+04]\n",
            "(17170, 14)\n",
            "17170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IyLGsIqeWbL",
        "outputId": "75da98b4-0210-4f5a-8415-0b74b546244b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00000000e+00 2.73359449e-04 3.00000000e+00 ... 1.37622000e+05\n",
            "  1.99813700e+07 3.07000000e+03]\n",
            " [0.00000000e+00 1.84220009e-01 7.00000000e+00 ... 5.19000000e+03\n",
            "  2.43793864e+08 1.22000000e+02]\n",
            " [0.00000000e+00 4.84078414e-01 6.00000000e+00 ... 5.19000000e+03\n",
            "  1.93208100e+07 1.22000000e+02]\n",
            " ...\n",
            " [1.00000000e+00 5.71787000e-01 4.00000000e+00 ... 3.29000000e+02\n",
            "  2.33188740e+07 0.00000000e+00]\n",
            " [1.00000000e+00 4.51217663e-01 9.00000000e+00 ... 2.40307098e+05\n",
            "  6.87396100e+06 0.00000000e+00]\n",
            " [2.00000000e+00 8.25293672e-01 6.00000000e+00 ... 2.48400000e+03\n",
            "  5.69558400e+06 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EuIQ-7tKuocP"
      },
      "outputs": [],
      "source": [
        "def same_seed(seed): \n",
        "    '''Fixes random number generator seeds for reproducibility.'''\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_valid_split(data_set, valid_ratio, seed):\n",
        "    '''Split provided training data into training set and validation set'''\n",
        "    valid_set_size = int(valid_ratio * len(data_set)) \n",
        "    train_set_size = len(data_set) - valid_set_size\n",
        "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
        "    return np.array(train_set), np.array(valid_set)\n",
        "\n",
        "def predict(test_loader, model, device):\n",
        "    model.eval() # Set your model to evaluation mode.\n",
        "    preds = []\n",
        "    for x in tqdm(test_loader):\n",
        "        x = x.to(device)                        \n",
        "        with torch.no_grad():                   \n",
        "            pred = torch.clamp(torch.round(model(x)),0,9)\n",
        "            #pred = np.array(pred.tolist())\n",
        "            #predict = np.zeros((pred.shape[0]))\n",
        "            #for i in range(pred.shape[0]):\n",
        "            #  rate = 0\n",
        "            #  for j in range(pred.shape[1]):\n",
        "            #    rate += pred[i][j]\n",
        "            #    if rate >0.5:\n",
        "            #      predict[i] = j \n",
        "            #      break\n",
        "            #pred = torch.from_numpy(predict).to(device)              \n",
        "            preds.append(pred.detach().cpu())   \n",
        "    preds = torch.cat(preds, dim=0).numpy()  \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D320CZu7nkU"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ypSQGM5i7q_g"
      },
      "outputs": [],
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    '''\n",
        "    x: Features.\n",
        "    y: Targets, if none, do prediction.\n",
        "    '''\n",
        "    def __init__(self, x, y=None):\n",
        "        if y is None:\n",
        "            self.y = y\n",
        "        else:\n",
        "            self.y = torch.FloatTensor(y)\n",
        "        self.x = torch.FloatTensor(x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.x[idx]\n",
        "        else:\n",
        "            return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dFunF8QtoP_"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zIkJ9iRauAy8"
      },
      "outputs": [],
      "source": [
        "class My_Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(My_Model, self).__init__()\n",
        "        # TODO: modify model's structure, be aware of dimensions. \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "                                     \n",
        "        )\n",
        "        self.maxpoollayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(16, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "        )\n",
        "        self.maxpoollayer2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(16, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "        )\n",
        "        self.maxpoollayer3 = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(16, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "        )\n",
        "        self.layers2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.BatchNorm1d(16),\n",
        "                                     \n",
        "        )\n",
        "        \n",
        "        self.finallayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 640),\n",
        "            nn.BatchNorm1d(640),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(640, 1000),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1000, 10),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "        self.rnn = nn.RNN(input_dim, 256, 3)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #RNN\n",
        "        #h0 = torch.randn(3, 256).to(device) \n",
        "        #outRnn, _ = self.rnn(x, h0)\n",
        "        \n",
        "        #outRnn = self.fc2(outRnn)\n",
        "        #outRnn = self.softmax(outRnn)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        z = self.layers(x)\n",
        "        #z = self.layers2(z)\n",
        "\n",
        "\n",
        "        #y = self.layers(x)\n",
        "        #x = self.layers(x)\n",
        "        #x = self.maxpoollayer(x)\n",
        "        #x = self.maxpoollayer2(x)\n",
        "        #x = self.maxpoollayer3(x)\n",
        "        #x = self.finallayer(x)\n",
        "        #x = self.softmax(x)\n",
        "        #y = self.finallayer(y)\n",
        "        # = self.softmax(y)\n",
        "        \n",
        "\n",
        "\n",
        "        #x = torch.add(torch.mul(x,0.7),torch.mul(y,0.3))\n",
        "        #x = torch.add(torch.mul(x,0.9),torch.mul(outRnn,0.1))\n",
        "\n",
        "        #out = torch.linspace(0,9,10).to(device)\n",
        "        #x = torch.mul(x,out)\n",
        "        #x = torch.sum(x, dim=1)                  ###x\n",
        "\n",
        "#        y = np.array(y.tolist())\n",
        "#        temp = np.zeros((y.shape[0]))\n",
        "#        for i in range(y.shape[0]):\n",
        "#          rate = 0\n",
        "#          for j in range(y.shape[1]):\n",
        "#           rate += y[i][j]\n",
        "#            if rate >0.5:\n",
        "#              temp[i] = j \n",
        "#              break\n",
        "#        y = torch.from_numpy(temp).to(device)\n",
        "\n",
        "        z = self.finallayer(z)\n",
        "        z = self.fc(z)                      ###z \n",
        "        #output = torch.add(torch.mul(x,0.0001),torch.mul(z,0.9999))\n",
        "        #x = torch.sigmoid(x)\n",
        "        #x = self.fc(x)\n",
        "        output = z\n",
        "        #\n",
        "        \n",
        "        output = output.squeeze(-1) # (B, 1) -> (B)\n",
        "        return output\n",
        "class wideL1loss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(wideL1loss, self).__init__()\n",
        "  def forward(self, x,y):\n",
        "    loss = torch.mean(torch.subtract(torch.clamp(torch.abs(x-y),2.5,10),2.5))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EXYkYmuQzB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "khMhvW1duO9l"
      },
      "outputs": [],
      "source": [
        "def trainer(train_data, model, config, device):\n",
        "\n",
        "    criterion = nn.SmoothL1Loss(beta=0.5) # Define your loss function, do not modify this.\n",
        "    #criterion = nn.CrossEntropyLoss() \n",
        "    loss2 = wideL1loss()\n",
        "    criterion2 = nn.L1Loss()\n",
        "\n",
        "    # Define your optimization algorithm. \n",
        "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
        "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr= config['learning_rate'] , weight_decay=0.01, amsgrad=False) \n",
        "    writer = SummaryWriter() # Writer of tensoboard.\n",
        "\n",
        "    if not os.path.isdir('./models'):\n",
        "        os.mkdir('./models') # Create directory of saving models.\n",
        "\n",
        "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
        "\n",
        "\n",
        "    np.random.seed(config['seed'])\n",
        "    splits = np.array_split(train_data, 5)\n",
        "    train =  np.concatenate(splits[:0]+splits[0+1:])\n",
        "    valid= splits[0]\n",
        "    x_train = train[...,1:]\n",
        "    x_valid = valid[...,1:]\n",
        "    y_train = train[...,0]\n",
        "    y_valid = valid[...,0]\n",
        "    train_dataset  = COVID19Dataset(x_train, y_train)\n",
        "    valid_dataset = COVID19Dataset(x_valid, y_valid)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      lossvalid_record = []\n",
        "      loss_record = []\n",
        "      realloss = []\n",
        "      for i in range(1):\n",
        "        model.train() # Set your model to train mode.\n",
        "         \n",
        "        \n",
        "        # Pytorch data loader loads pytorch dataset into batches.\n",
        "        \n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()               # Set gradient to zero.\n",
        "            x, y = x.to(device), y.to(device)   # Move your data to device. \n",
        "            pred = model(x)\n",
        "            \n",
        "            #loss = criterion(pred, y)\n",
        "            l2_norm = sum((p.pow(2)/(1+p.pow(2))).sum()for p in model.parameters())\n",
        "            loss = criterion(pred, y)+config['regulizer']*l2_norm+config['loss2']*loss2(pred, y)\n",
        "            #print(loss)\n",
        "            loss.backward()                     # Compute gradient(backpropagation).\n",
        "            optimizer.step()                    # Update parameters.\n",
        "            step += 1\n",
        "            \n",
        "            loss_record.append(loss.detach().item())\n",
        " \n",
        "        model.eval() # Set your model to evaluation mode.\n",
        "        \n",
        "        for x, y in valid_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = torch.clamp(torch.round(model(x)),0,9)\n",
        "                #loss = criterion(pred, y)\n",
        "                l2_norm = sum((p.pow(2)/(1+p.pow(2))).sum()for p in model.parameters())\n",
        "                loss = criterion(pred, y)+config['regulizer']*l2_norm+config['loss2']*loss2(pred, y)\n",
        "                realloss2 = criterion2(pred, y)\n",
        "            lossvalid_record.append(loss.item())\n",
        "            realloss.append(realloss2.item())\n",
        "\n",
        "\n",
        "        \n",
        "      mean_train_loss = sum(loss_record)/len(loss_record)         ###compute CV loss\n",
        "      writer.add_scalar('Loss/train', mean_train_loss, step)\n",
        "      reloss = sum(realloss)/len(realloss)\n",
        "      mean_valid_loss = sum(lossvalid_record)/len(lossvalid_record)\n",
        "      print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, L1 loss: {reloss:.4f}')\n",
        "      # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
        "\n",
        "      if mean_valid_loss < best_loss:\n",
        "          best_loss = mean_valid_loss\n",
        "          torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
        "          print('Saving model with loss {:.3f}...'.format(best_loss))\n",
        "          early_stop_count = 0\n",
        "      else: \n",
        "          early_stop_count += 1\n",
        "\n",
        "      if early_stop_count >= config['early_stop']:\n",
        "          print('\\nModel is not improving, so we halt the training session.')\n",
        "          return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Hg8aMlzaqk"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dB14dhABzVxO"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "config = {\n",
        "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
        "    'select_all': True,   # Whether to use all features.\n",
        "    'n_fold': 5,   # validation_size = train_size * valid_ratio\n",
        "    'n_epochs': 30000,     # Number of epochs.            \n",
        "    'batch_size': 128, \n",
        "    'learning_rate': 1e-3,\n",
        "    'regulizer': 1e-4,  \n",
        "    'loss2'  :3,          \n",
        "    'early_stop': 1500,    # If model has not improved for this many consecutive epochs, stop training.     \n",
        "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DqtxFou74eQr"
      },
      "outputs": [],
      "source": [
        "same_seed(config['seed'])\n",
        "\n",
        "#imp_median.fit(test)\n",
        "#test_data = imp_median.transform(test)\n",
        "train_data= feature\n",
        "#train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
        "\n",
        "x_test =  test_data\n",
        "test_dataset = COVID19Dataset(x_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zpP7hF1y18"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hn82Vj0R06Yt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "461636bc-f72d-455c-ba55-32d50d47b4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30000]: Train loss: 4.4945, Valid loss: 2.7901, L1 loss: 1.9437\n",
            "Saving model with loss 2.790...\n",
            "Epoch [2/30000]: Train loss: 3.0238, Valid loss: 2.7481, L1 loss: 1.8037\n",
            "Saving model with loss 2.748...\n",
            "Epoch [3/30000]: Train loss: 2.8752, Valid loss: 2.5265, L1 loss: 1.8213\n",
            "Saving model with loss 2.526...\n",
            "Epoch [4/30000]: Train loss: 2.7526, Valid loss: 2.6081, L1 loss: 1.8467\n",
            "Epoch [5/30000]: Train loss: 2.6899, Valid loss: 2.6204, L1 loss: 1.8517\n",
            "Epoch [6/30000]: Train loss: 2.6448, Valid loss: 2.4610, L1 loss: 1.7896\n",
            "Saving model with loss 2.461...\n",
            "Epoch [7/30000]: Train loss: 2.6084, Valid loss: 2.5567, L1 loss: 1.8601\n",
            "Epoch [8/30000]: Train loss: 2.5501, Valid loss: 2.5876, L1 loss: 1.8364\n",
            "Epoch [9/30000]: Train loss: 2.5399, Valid loss: 2.5967, L1 loss: 1.8499\n",
            "Epoch [10/30000]: Train loss: 2.5167, Valid loss: 2.4674, L1 loss: 1.8334\n",
            "Epoch [11/30000]: Train loss: 2.5133, Valid loss: 2.8783, L1 loss: 1.9458\n",
            "Epoch [12/30000]: Train loss: 2.5223, Valid loss: 2.4074, L1 loss: 1.7680\n",
            "Saving model with loss 2.407...\n",
            "Epoch [13/30000]: Train loss: 2.4565, Valid loss: 2.4127, L1 loss: 1.7783\n",
            "Epoch [14/30000]: Train loss: 2.4546, Valid loss: 2.4092, L1 loss: 1.7360\n",
            "Epoch [15/30000]: Train loss: 2.4353, Valid loss: 2.4792, L1 loss: 1.8245\n",
            "Epoch [16/30000]: Train loss: 2.4143, Valid loss: 2.6340, L1 loss: 1.8649\n",
            "Epoch [17/30000]: Train loss: 2.4248, Valid loss: 2.4092, L1 loss: 1.7438\n",
            "Epoch [18/30000]: Train loss: 2.4408, Valid loss: 2.4086, L1 loss: 1.7441\n",
            "Epoch [19/30000]: Train loss: 2.4149, Valid loss: 2.4557, L1 loss: 1.7972\n",
            "Epoch [20/30000]: Train loss: 2.3858, Valid loss: 2.5184, L1 loss: 1.8381\n",
            "Epoch [21/30000]: Train loss: 2.3968, Valid loss: 2.5622, L1 loss: 1.7936\n",
            "Epoch [22/30000]: Train loss: 2.3886, Valid loss: 2.4549, L1 loss: 1.7694\n",
            "Epoch [23/30000]: Train loss: 2.3906, Valid loss: 2.4656, L1 loss: 1.7909\n",
            "Epoch [24/30000]: Train loss: 2.3715, Valid loss: 2.5802, L1 loss: 1.7790\n",
            "Epoch [25/30000]: Train loss: 2.3794, Valid loss: 2.5496, L1 loss: 1.8098\n",
            "Epoch [26/30000]: Train loss: 2.3704, Valid loss: 2.5605, L1 loss: 1.8136\n",
            "Epoch [27/30000]: Train loss: 2.3631, Valid loss: 2.3992, L1 loss: 1.7293\n",
            "Saving model with loss 2.399...\n",
            "Epoch [28/30000]: Train loss: 2.3553, Valid loss: 2.5999, L1 loss: 1.8642\n",
            "Epoch [29/30000]: Train loss: 2.3176, Valid loss: 2.3914, L1 loss: 1.7267\n",
            "Saving model with loss 2.391...\n",
            "Epoch [30/30000]: Train loss: 2.3164, Valid loss: 2.4466, L1 loss: 1.8036\n",
            "Epoch [31/30000]: Train loss: 2.3312, Valid loss: 2.4113, L1 loss: 1.7605\n",
            "Epoch [32/30000]: Train loss: 2.3398, Valid loss: 2.4928, L1 loss: 1.8044\n",
            "Epoch [33/30000]: Train loss: 2.3247, Valid loss: 2.3748, L1 loss: 1.7642\n",
            "Saving model with loss 2.375...\n",
            "Epoch [34/30000]: Train loss: 2.3284, Valid loss: 2.4461, L1 loss: 1.7879\n",
            "Epoch [35/30000]: Train loss: 2.3158, Valid loss: 2.5486, L1 loss: 1.8174\n",
            "Epoch [36/30000]: Train loss: 2.3090, Valid loss: 2.5019, L1 loss: 1.7910\n",
            "Epoch [37/30000]: Train loss: 2.3034, Valid loss: 2.3989, L1 loss: 1.7651\n",
            "Epoch [38/30000]: Train loss: 2.3177, Valid loss: 2.5304, L1 loss: 1.8212\n",
            "Epoch [39/30000]: Train loss: 2.3056, Valid loss: 2.3519, L1 loss: 1.7459\n",
            "Saving model with loss 2.352...\n",
            "Epoch [40/30000]: Train loss: 2.3121, Valid loss: 2.4503, L1 loss: 1.7828\n",
            "Epoch [41/30000]: Train loss: 2.2917, Valid loss: 2.4260, L1 loss: 1.7756\n",
            "Epoch [42/30000]: Train loss: 2.3086, Valid loss: 2.4031, L1 loss: 1.7760\n",
            "Epoch [43/30000]: Train loss: 2.2707, Valid loss: 2.3935, L1 loss: 1.7317\n",
            "Epoch [44/30000]: Train loss: 2.2805, Valid loss: 2.4189, L1 loss: 1.7723\n",
            "Epoch [45/30000]: Train loss: 2.2765, Valid loss: 2.4334, L1 loss: 1.7802\n",
            "Epoch [46/30000]: Train loss: 2.2886, Valid loss: 2.3550, L1 loss: 1.7487\n",
            "Epoch [47/30000]: Train loss: 2.2581, Valid loss: 2.4383, L1 loss: 1.8089\n",
            "Epoch [48/30000]: Train loss: 2.2622, Valid loss: 2.3428, L1 loss: 1.7396\n",
            "Saving model with loss 2.343...\n",
            "Epoch [49/30000]: Train loss: 2.2709, Valid loss: 2.4608, L1 loss: 1.8040\n",
            "Epoch [50/30000]: Train loss: 2.2703, Valid loss: 2.4591, L1 loss: 1.8146\n",
            "Epoch [51/30000]: Train loss: 2.2700, Valid loss: 2.3615, L1 loss: 1.7304\n",
            "Epoch [52/30000]: Train loss: 2.2592, Valid loss: 2.3560, L1 loss: 1.7676\n",
            "Epoch [53/30000]: Train loss: 2.2440, Valid loss: 2.4689, L1 loss: 1.8250\n",
            "Epoch [54/30000]: Train loss: 2.2504, Valid loss: 2.3511, L1 loss: 1.7231\n",
            "Epoch [55/30000]: Train loss: 2.2511, Valid loss: 2.4372, L1 loss: 1.7686\n",
            "Epoch [56/30000]: Train loss: 2.2119, Valid loss: 2.4521, L1 loss: 1.7873\n",
            "Epoch [57/30000]: Train loss: 2.2400, Valid loss: 2.4089, L1 loss: 1.7690\n",
            "Epoch [58/30000]: Train loss: 2.2409, Valid loss: 2.4892, L1 loss: 1.8080\n",
            "Epoch [59/30000]: Train loss: 2.2319, Valid loss: 2.4150, L1 loss: 1.7449\n",
            "Epoch [60/30000]: Train loss: 2.2322, Valid loss: 2.4229, L1 loss: 1.7711\n",
            "Epoch [61/30000]: Train loss: 2.2168, Valid loss: 2.4123, L1 loss: 1.7470\n",
            "Epoch [62/30000]: Train loss: 2.2379, Valid loss: 2.3734, L1 loss: 1.7564\n",
            "Epoch [63/30000]: Train loss: 2.2432, Valid loss: 2.4698, L1 loss: 1.7818\n",
            "Epoch [64/30000]: Train loss: 2.2053, Valid loss: 2.3532, L1 loss: 1.7694\n",
            "Epoch [65/30000]: Train loss: 2.2480, Valid loss: 2.5334, L1 loss: 1.8076\n",
            "Epoch [66/30000]: Train loss: 2.2038, Valid loss: 2.5377, L1 loss: 1.8372\n",
            "Epoch [67/30000]: Train loss: 2.1981, Valid loss: 2.4626, L1 loss: 1.7513\n",
            "Epoch [68/30000]: Train loss: 2.1927, Valid loss: 2.5101, L1 loss: 1.8115\n",
            "Epoch [69/30000]: Train loss: 2.2109, Valid loss: 2.4327, L1 loss: 1.7916\n",
            "Epoch [70/30000]: Train loss: 2.1935, Valid loss: 2.3270, L1 loss: 1.7028\n",
            "Saving model with loss 2.327...\n",
            "Epoch [71/30000]: Train loss: 2.1973, Valid loss: 2.3535, L1 loss: 1.7489\n",
            "Epoch [72/30000]: Train loss: 2.2110, Valid loss: 2.4141, L1 loss: 1.7973\n",
            "Epoch [73/30000]: Train loss: 2.1934, Valid loss: 2.4252, L1 loss: 1.7852\n",
            "Epoch [74/30000]: Train loss: 2.1751, Valid loss: 2.3884, L1 loss: 1.7584\n",
            "Epoch [75/30000]: Train loss: 2.1738, Valid loss: 2.3973, L1 loss: 1.7845\n",
            "Epoch [76/30000]: Train loss: 2.2015, Valid loss: 2.5119, L1 loss: 1.8051\n",
            "Epoch [77/30000]: Train loss: 2.1919, Valid loss: 2.3736, L1 loss: 1.7565\n",
            "Epoch [78/30000]: Train loss: 2.1472, Valid loss: 2.3753, L1 loss: 1.7250\n",
            "Epoch [79/30000]: Train loss: 2.1572, Valid loss: 2.4164, L1 loss: 1.7796\n",
            "Epoch [80/30000]: Train loss: 2.1844, Valid loss: 2.4598, L1 loss: 1.7467\n",
            "Epoch [81/30000]: Train loss: 2.1870, Valid loss: 2.4793, L1 loss: 1.8001\n",
            "Epoch [82/30000]: Train loss: 2.1810, Valid loss: 2.5385, L1 loss: 1.8236\n",
            "Epoch [83/30000]: Train loss: 2.1519, Valid loss: 2.4288, L1 loss: 1.7848\n",
            "Epoch [84/30000]: Train loss: 2.1650, Valid loss: 2.4824, L1 loss: 1.7604\n",
            "Epoch [85/30000]: Train loss: 2.1449, Valid loss: 2.4478, L1 loss: 1.7956\n",
            "Epoch [86/30000]: Train loss: 2.1658, Valid loss: 2.4259, L1 loss: 1.7835\n",
            "Epoch [87/30000]: Train loss: 2.1718, Valid loss: 2.3726, L1 loss: 1.7626\n",
            "Epoch [88/30000]: Train loss: 2.1353, Valid loss: 2.3954, L1 loss: 1.7357\n",
            "Epoch [89/30000]: Train loss: 2.1311, Valid loss: 2.4804, L1 loss: 1.7607\n",
            "Epoch [90/30000]: Train loss: 2.1295, Valid loss: 2.3810, L1 loss: 1.7556\n",
            "Epoch [91/30000]: Train loss: 2.1381, Valid loss: 2.5174, L1 loss: 1.8151\n",
            "Epoch [92/30000]: Train loss: 2.1401, Valid loss: 2.3975, L1 loss: 1.7583\n",
            "Epoch [93/30000]: Train loss: 2.1369, Valid loss: 2.4285, L1 loss: 1.7857\n",
            "Epoch [94/30000]: Train loss: 2.1330, Valid loss: 2.4070, L1 loss: 1.7846\n",
            "Epoch [95/30000]: Train loss: 2.1208, Valid loss: 2.4664, L1 loss: 1.7900\n",
            "Epoch [96/30000]: Train loss: 2.1172, Valid loss: 2.3781, L1 loss: 1.7399\n",
            "Epoch [97/30000]: Train loss: 2.1240, Valid loss: 2.4060, L1 loss: 1.7713\n",
            "Epoch [98/30000]: Train loss: 2.1178, Valid loss: 2.3418, L1 loss: 1.7305\n",
            "Epoch [99/30000]: Train loss: 2.1162, Valid loss: 2.4671, L1 loss: 1.7962\n",
            "Epoch [100/30000]: Train loss: 2.1101, Valid loss: 2.3906, L1 loss: 1.7495\n",
            "Epoch [101/30000]: Train loss: 2.1052, Valid loss: 2.4339, L1 loss: 1.7902\n",
            "Epoch [102/30000]: Train loss: 2.1009, Valid loss: 2.4361, L1 loss: 1.7446\n",
            "Epoch [103/30000]: Train loss: 2.1123, Valid loss: 2.4722, L1 loss: 1.7772\n",
            "Epoch [104/30000]: Train loss: 2.0918, Valid loss: 2.4139, L1 loss: 1.7743\n",
            "Epoch [105/30000]: Train loss: 2.0746, Valid loss: 2.4291, L1 loss: 1.7371\n",
            "Epoch [106/30000]: Train loss: 2.0842, Valid loss: 2.3856, L1 loss: 1.7478\n",
            "Epoch [107/30000]: Train loss: 2.1012, Valid loss: 2.3868, L1 loss: 1.7527\n",
            "Epoch [108/30000]: Train loss: 2.0946, Valid loss: 2.3973, L1 loss: 1.7573\n",
            "Epoch [109/30000]: Train loss: 2.0825, Valid loss: 2.4981, L1 loss: 1.8141\n",
            "Epoch [110/30000]: Train loss: 2.1109, Valid loss: 2.5363, L1 loss: 1.8076\n",
            "Epoch [111/30000]: Train loss: 2.0520, Valid loss: 2.3622, L1 loss: 1.7358\n",
            "Epoch [112/30000]: Train loss: 2.0796, Valid loss: 2.5362, L1 loss: 1.8479\n",
            "Epoch [113/30000]: Train loss: 2.0707, Valid loss: 2.4267, L1 loss: 1.7617\n",
            "Epoch [114/30000]: Train loss: 2.0622, Valid loss: 2.3859, L1 loss: 1.7767\n",
            "Epoch [115/30000]: Train loss: 2.0367, Valid loss: 2.3803, L1 loss: 1.7457\n",
            "Epoch [116/30000]: Train loss: 2.0541, Valid loss: 2.4510, L1 loss: 1.7416\n",
            "Epoch [117/30000]: Train loss: 2.0738, Valid loss: 2.4480, L1 loss: 1.7751\n",
            "Epoch [118/30000]: Train loss: 2.0650, Valid loss: 2.3743, L1 loss: 1.7487\n",
            "Epoch [119/30000]: Train loss: 2.0407, Valid loss: 2.3709, L1 loss: 1.7554\n",
            "Epoch [120/30000]: Train loss: 2.0557, Valid loss: 2.4489, L1 loss: 1.8207\n",
            "Epoch [121/30000]: Train loss: 2.0424, Valid loss: 2.5062, L1 loss: 1.7732\n",
            "Epoch [122/30000]: Train loss: 2.0495, Valid loss: 2.4609, L1 loss: 1.7593\n",
            "Epoch [123/30000]: Train loss: 2.0264, Valid loss: 2.4763, L1 loss: 1.7959\n",
            "Epoch [124/30000]: Train loss: 2.0077, Valid loss: 2.4681, L1 loss: 1.7825\n",
            "Epoch [125/30000]: Train loss: 2.0266, Valid loss: 2.4700, L1 loss: 1.7798\n",
            "Epoch [126/30000]: Train loss: 2.0228, Valid loss: 2.4986, L1 loss: 1.8190\n",
            "Epoch [127/30000]: Train loss: 2.0348, Valid loss: 2.4608, L1 loss: 1.7707\n",
            "Epoch [128/30000]: Train loss: 2.0238, Valid loss: 2.3966, L1 loss: 1.7329\n",
            "Epoch [129/30000]: Train loss: 1.9978, Valid loss: 2.5058, L1 loss: 1.8201\n",
            "Epoch [130/30000]: Train loss: 2.0322, Valid loss: 2.3932, L1 loss: 1.7467\n",
            "Epoch [131/30000]: Train loss: 2.0306, Valid loss: 2.4788, L1 loss: 1.7900\n",
            "Epoch [132/30000]: Train loss: 2.0043, Valid loss: 2.4744, L1 loss: 1.7896\n",
            "Epoch [133/30000]: Train loss: 2.0405, Valid loss: 2.4562, L1 loss: 1.7589\n",
            "Epoch [134/30000]: Train loss: 2.0114, Valid loss: 2.3664, L1 loss: 1.7508\n",
            "Epoch [135/30000]: Train loss: 1.9961, Valid loss: 2.4653, L1 loss: 1.7574\n",
            "Epoch [136/30000]: Train loss: 2.0180, Valid loss: 2.4098, L1 loss: 1.7219\n",
            "Epoch [137/30000]: Train loss: 1.9872, Valid loss: 2.4295, L1 loss: 1.7450\n",
            "Epoch [138/30000]: Train loss: 2.0070, Valid loss: 2.4284, L1 loss: 1.7579\n",
            "Epoch [139/30000]: Train loss: 1.9838, Valid loss: 2.4408, L1 loss: 1.7631\n",
            "Epoch [140/30000]: Train loss: 1.9885, Valid loss: 2.4355, L1 loss: 1.7371\n",
            "Epoch [141/30000]: Train loss: 1.9837, Valid loss: 2.5008, L1 loss: 1.7748\n",
            "Epoch [142/30000]: Train loss: 1.9848, Valid loss: 2.6219, L1 loss: 1.8222\n",
            "Epoch [143/30000]: Train loss: 1.9724, Valid loss: 2.6800, L1 loss: 1.8720\n",
            "Epoch [144/30000]: Train loss: 1.9471, Valid loss: 2.4814, L1 loss: 1.7925\n",
            "Epoch [145/30000]: Train loss: 1.9580, Valid loss: 2.5230, L1 loss: 1.7716\n",
            "Epoch [146/30000]: Train loss: 2.0062, Valid loss: 2.5707, L1 loss: 1.7899\n",
            "Epoch [147/30000]: Train loss: 1.9526, Valid loss: 2.6279, L1 loss: 1.8335\n",
            "Epoch [148/30000]: Train loss: 1.9517, Valid loss: 2.4372, L1 loss: 1.7605\n",
            "Epoch [149/30000]: Train loss: 1.9846, Valid loss: 2.5324, L1 loss: 1.8319\n",
            "Epoch [150/30000]: Train loss: 1.9748, Valid loss: 2.5204, L1 loss: 1.8045\n",
            "Epoch [151/30000]: Train loss: 1.9415, Valid loss: 2.5514, L1 loss: 1.8208\n",
            "Epoch [152/30000]: Train loss: 1.9468, Valid loss: 2.5228, L1 loss: 1.7979\n",
            "Epoch [153/30000]: Train loss: 1.9114, Valid loss: 2.5528, L1 loss: 1.7541\n",
            "Epoch [154/30000]: Train loss: 1.9222, Valid loss: 2.4588, L1 loss: 1.7751\n",
            "Epoch [155/30000]: Train loss: 1.9316, Valid loss: 2.5113, L1 loss: 1.8245\n",
            "Epoch [156/30000]: Train loss: 1.9411, Valid loss: 2.4878, L1 loss: 1.7807\n",
            "Epoch [157/30000]: Train loss: 1.9290, Valid loss: 2.3806, L1 loss: 1.7366\n",
            "Epoch [158/30000]: Train loss: 1.9191, Valid loss: 2.4945, L1 loss: 1.8095\n",
            "Epoch [159/30000]: Train loss: 1.9488, Valid loss: 2.5567, L1 loss: 1.8339\n",
            "Epoch [160/30000]: Train loss: 1.9128, Valid loss: 2.5821, L1 loss: 1.8302\n",
            "Epoch [161/30000]: Train loss: 1.9190, Valid loss: 2.4019, L1 loss: 1.7170\n",
            "Epoch [162/30000]: Train loss: 1.9033, Valid loss: 2.4722, L1 loss: 1.7569\n",
            "Epoch [163/30000]: Train loss: 1.9185, Valid loss: 2.5880, L1 loss: 1.8061\n",
            "Epoch [164/30000]: Train loss: 1.8880, Valid loss: 2.4785, L1 loss: 1.7598\n",
            "Epoch [165/30000]: Train loss: 1.9036, Valid loss: 2.5279, L1 loss: 1.7753\n",
            "Epoch [166/30000]: Train loss: 1.8930, Valid loss: 2.5117, L1 loss: 1.7522\n",
            "Epoch [167/30000]: Train loss: 1.8989, Valid loss: 2.4785, L1 loss: 1.7760\n",
            "Epoch [168/30000]: Train loss: 1.9106, Valid loss: 2.4368, L1 loss: 1.7515\n",
            "Epoch [169/30000]: Train loss: 1.8873, Valid loss: 2.5550, L1 loss: 1.8045\n",
            "Epoch [170/30000]: Train loss: 1.8837, Valid loss: 2.4799, L1 loss: 1.7865\n",
            "Epoch [171/30000]: Train loss: 1.8728, Valid loss: 2.5551, L1 loss: 1.8160\n",
            "Epoch [172/30000]: Train loss: 1.8841, Valid loss: 2.5392, L1 loss: 1.8236\n",
            "Epoch [173/30000]: Train loss: 1.8870, Valid loss: 2.3868, L1 loss: 1.7259\n",
            "Epoch [174/30000]: Train loss: 1.8906, Valid loss: 2.5362, L1 loss: 1.8037\n",
            "Epoch [175/30000]: Train loss: 1.8646, Valid loss: 2.5326, L1 loss: 1.7738\n",
            "Epoch [176/30000]: Train loss: 1.8960, Valid loss: 2.4972, L1 loss: 1.7866\n",
            "Epoch [177/30000]: Train loss: 1.8731, Valid loss: 2.5811, L1 loss: 1.8390\n",
            "Epoch [178/30000]: Train loss: 1.8481, Valid loss: 2.5287, L1 loss: 1.8142\n",
            "Epoch [179/30000]: Train loss: 1.8574, Valid loss: 2.5768, L1 loss: 1.7892\n",
            "Epoch [180/30000]: Train loss: 1.8219, Valid loss: 2.4744, L1 loss: 1.7744\n",
            "Epoch [181/30000]: Train loss: 1.8339, Valid loss: 2.6472, L1 loss: 1.8374\n",
            "Epoch [182/30000]: Train loss: 1.8403, Valid loss: 2.5977, L1 loss: 1.8047\n",
            "Epoch [183/30000]: Train loss: 1.8567, Valid loss: 2.5004, L1 loss: 1.7859\n",
            "Epoch [184/30000]: Train loss: 1.8293, Valid loss: 2.4344, L1 loss: 1.7731\n",
            "Epoch [185/30000]: Train loss: 1.8232, Valid loss: 2.5086, L1 loss: 1.7634\n",
            "Epoch [186/30000]: Train loss: 1.8123, Valid loss: 2.5575, L1 loss: 1.7934\n",
            "Epoch [187/30000]: Train loss: 1.8412, Valid loss: 2.4928, L1 loss: 1.7593\n",
            "Epoch [188/30000]: Train loss: 1.8350, Valid loss: 2.4820, L1 loss: 1.7823\n",
            "Epoch [189/30000]: Train loss: 1.8098, Valid loss: 2.4947, L1 loss: 1.7616\n",
            "Epoch [190/30000]: Train loss: 1.8216, Valid loss: 2.6269, L1 loss: 1.8370\n",
            "Epoch [191/30000]: Train loss: 1.8369, Valid loss: 2.5124, L1 loss: 1.7890\n",
            "Epoch [192/30000]: Train loss: 1.8329, Valid loss: 2.4900, L1 loss: 1.7764\n",
            "Epoch [193/30000]: Train loss: 1.8095, Valid loss: 2.5048, L1 loss: 1.7757\n",
            "Epoch [194/30000]: Train loss: 1.8164, Valid loss: 2.5178, L1 loss: 1.7928\n",
            "Epoch [195/30000]: Train loss: 1.7949, Valid loss: 2.4918, L1 loss: 1.7974\n",
            "Epoch [196/30000]: Train loss: 1.7646, Valid loss: 2.5371, L1 loss: 1.8011\n",
            "Epoch [197/30000]: Train loss: 1.8053, Valid loss: 2.5682, L1 loss: 1.7874\n",
            "Epoch [198/30000]: Train loss: 1.8353, Valid loss: 2.6175, L1 loss: 1.8335\n",
            "Epoch [199/30000]: Train loss: 1.7616, Valid loss: 2.5140, L1 loss: 1.7988\n",
            "Epoch [200/30000]: Train loss: 1.7999, Valid loss: 2.4663, L1 loss: 1.7899\n",
            "Epoch [201/30000]: Train loss: 1.7875, Valid loss: 2.6042, L1 loss: 1.8413\n",
            "Epoch [202/30000]: Train loss: 1.8144, Valid loss: 2.5110, L1 loss: 1.7838\n",
            "Epoch [203/30000]: Train loss: 1.7952, Valid loss: 2.6678, L1 loss: 1.8347\n",
            "Epoch [204/30000]: Train loss: 1.8121, Valid loss: 2.5008, L1 loss: 1.8174\n",
            "Epoch [205/30000]: Train loss: 1.7572, Valid loss: 2.5163, L1 loss: 1.7906\n",
            "Epoch [206/30000]: Train loss: 1.7489, Valid loss: 2.5105, L1 loss: 1.7881\n",
            "Epoch [207/30000]: Train loss: 1.7978, Valid loss: 2.4450, L1 loss: 1.7448\n",
            "Epoch [208/30000]: Train loss: 1.7944, Valid loss: 2.5835, L1 loss: 1.8423\n",
            "Epoch [209/30000]: Train loss: 1.7687, Valid loss: 2.4215, L1 loss: 1.7571\n",
            "Epoch [210/30000]: Train loss: 1.7658, Valid loss: 2.5649, L1 loss: 1.8080\n",
            "Epoch [211/30000]: Train loss: 1.7412, Valid loss: 2.5654, L1 loss: 1.7848\n",
            "Epoch [212/30000]: Train loss: 1.7365, Valid loss: 2.6426, L1 loss: 1.8620\n",
            "Epoch [213/30000]: Train loss: 1.7333, Valid loss: 2.5957, L1 loss: 1.8125\n",
            "Epoch [214/30000]: Train loss: 1.7485, Valid loss: 2.4937, L1 loss: 1.7885\n",
            "Epoch [215/30000]: Train loss: 1.7383, Valid loss: 2.4394, L1 loss: 1.7536\n",
            "Epoch [216/30000]: Train loss: 1.7404, Valid loss: 2.5306, L1 loss: 1.7784\n",
            "Epoch [217/30000]: Train loss: 1.7679, Valid loss: 2.4855, L1 loss: 1.7259\n",
            "Epoch [218/30000]: Train loss: 1.7317, Valid loss: 2.6256, L1 loss: 1.8210\n",
            "Epoch [219/30000]: Train loss: 1.7324, Valid loss: 2.5064, L1 loss: 1.7850\n",
            "Epoch [220/30000]: Train loss: 1.7254, Valid loss: 2.6170, L1 loss: 1.8073\n",
            "Epoch [221/30000]: Train loss: 1.7526, Valid loss: 2.5824, L1 loss: 1.8116\n",
            "Epoch [222/30000]: Train loss: 1.7281, Valid loss: 2.5210, L1 loss: 1.7768\n",
            "Epoch [223/30000]: Train loss: 1.6977, Valid loss: 2.6164, L1 loss: 1.8135\n",
            "Epoch [224/30000]: Train loss: 1.7345, Valid loss: 2.5273, L1 loss: 1.8076\n",
            "Epoch [225/30000]: Train loss: 1.7157, Valid loss: 2.5377, L1 loss: 1.7901\n",
            "Epoch [226/30000]: Train loss: 1.7172, Valid loss: 2.6092, L1 loss: 1.8442\n",
            "Epoch [227/30000]: Train loss: 1.7050, Valid loss: 2.4732, L1 loss: 1.7668\n",
            "Epoch [228/30000]: Train loss: 1.7371, Valid loss: 2.6283, L1 loss: 1.8417\n",
            "Epoch [229/30000]: Train loss: 1.7297, Valid loss: 2.5771, L1 loss: 1.7791\n",
            "Epoch [230/30000]: Train loss: 1.6985, Valid loss: 2.5955, L1 loss: 1.8207\n",
            "Epoch [231/30000]: Train loss: 1.6705, Valid loss: 2.5190, L1 loss: 1.7661\n",
            "Epoch [232/30000]: Train loss: 1.7056, Valid loss: 2.5519, L1 loss: 1.8091\n",
            "Epoch [233/30000]: Train loss: 1.6826, Valid loss: 2.8695, L1 loss: 1.9036\n",
            "Epoch [234/30000]: Train loss: 1.6652, Valid loss: 2.5354, L1 loss: 1.8045\n",
            "Epoch [235/30000]: Train loss: 1.6688, Valid loss: 2.5901, L1 loss: 1.8279\n",
            "Epoch [236/30000]: Train loss: 1.6941, Valid loss: 2.6391, L1 loss: 1.8339\n",
            "Epoch [237/30000]: Train loss: 1.6554, Valid loss: 2.5415, L1 loss: 1.7928\n",
            "Epoch [238/30000]: Train loss: 1.6965, Valid loss: 2.5380, L1 loss: 1.8002\n",
            "Epoch [239/30000]: Train loss: 1.6743, Valid loss: 2.6282, L1 loss: 1.8443\n",
            "Epoch [240/30000]: Train loss: 1.6573, Valid loss: 2.5412, L1 loss: 1.8239\n",
            "Epoch [241/30000]: Train loss: 1.6559, Valid loss: 2.5929, L1 loss: 1.8238\n",
            "Epoch [242/30000]: Train loss: 1.6513, Valid loss: 2.6302, L1 loss: 1.8229\n",
            "Epoch [243/30000]: Train loss: 1.6571, Valid loss: 2.4710, L1 loss: 1.7705\n",
            "Epoch [244/30000]: Train loss: 1.6363, Valid loss: 2.5128, L1 loss: 1.7574\n",
            "Epoch [245/30000]: Train loss: 1.6436, Valid loss: 2.5346, L1 loss: 1.7523\n",
            "Epoch [246/30000]: Train loss: 1.6609, Valid loss: 2.5209, L1 loss: 1.7487\n",
            "Epoch [247/30000]: Train loss: 1.6291, Valid loss: 2.6154, L1 loss: 1.8058\n",
            "Epoch [248/30000]: Train loss: 1.6454, Valid loss: 2.6390, L1 loss: 1.8418\n",
            "Epoch [249/30000]: Train loss: 1.6467, Valid loss: 2.6721, L1 loss: 1.8815\n",
            "Epoch [250/30000]: Train loss: 1.6366, Valid loss: 2.5200, L1 loss: 1.7902\n",
            "Epoch [251/30000]: Train loss: 1.6241, Valid loss: 2.5841, L1 loss: 1.8095\n",
            "Epoch [252/30000]: Train loss: 1.6019, Valid loss: 2.6656, L1 loss: 1.8033\n",
            "Epoch [253/30000]: Train loss: 1.6278, Valid loss: 2.6713, L1 loss: 1.8173\n",
            "Epoch [254/30000]: Train loss: 1.5923, Valid loss: 2.5573, L1 loss: 1.7928\n",
            "Epoch [255/30000]: Train loss: 1.6183, Valid loss: 2.5378, L1 loss: 1.7824\n",
            "Epoch [256/30000]: Train loss: 1.6165, Valid loss: 2.6530, L1 loss: 1.8142\n",
            "Epoch [257/30000]: Train loss: 1.6168, Valid loss: 2.5193, L1 loss: 1.7761\n",
            "Epoch [258/30000]: Train loss: 1.5905, Valid loss: 2.5266, L1 loss: 1.7610\n",
            "Epoch [259/30000]: Train loss: 1.6206, Valid loss: 2.5256, L1 loss: 1.7766\n",
            "Epoch [260/30000]: Train loss: 1.5805, Valid loss: 2.5737, L1 loss: 1.8111\n",
            "Epoch [261/30000]: Train loss: 1.5885, Valid loss: 2.5750, L1 loss: 1.7988\n",
            "Epoch [262/30000]: Train loss: 1.6085, Valid loss: 2.8115, L1 loss: 1.8758\n",
            "Epoch [263/30000]: Train loss: 1.5827, Valid loss: 2.5176, L1 loss: 1.7673\n",
            "Epoch [264/30000]: Train loss: 1.6179, Valid loss: 2.5954, L1 loss: 1.8375\n",
            "Epoch [265/30000]: Train loss: 1.5739, Valid loss: 2.5849, L1 loss: 1.8080\n",
            "Epoch [266/30000]: Train loss: 1.5451, Valid loss: 2.6237, L1 loss: 1.8049\n",
            "Epoch [267/30000]: Train loss: 1.5609, Valid loss: 2.5518, L1 loss: 1.8030\n",
            "Epoch [268/30000]: Train loss: 1.5678, Valid loss: 2.7163, L1 loss: 1.8716\n",
            "Epoch [269/30000]: Train loss: 1.5704, Valid loss: 2.5409, L1 loss: 1.7647\n",
            "Epoch [270/30000]: Train loss: 1.5871, Valid loss: 2.6144, L1 loss: 1.8239\n",
            "Epoch [271/30000]: Train loss: 1.5787, Valid loss: 2.5850, L1 loss: 1.8022\n",
            "Epoch [272/30000]: Train loss: 1.5579, Valid loss: 2.5226, L1 loss: 1.7594\n",
            "Epoch [273/30000]: Train loss: 1.5662, Valid loss: 2.5808, L1 loss: 1.8091\n",
            "Epoch [274/30000]: Train loss: 1.5616, Valid loss: 2.6017, L1 loss: 1.7899\n",
            "Epoch [275/30000]: Train loss: 1.5479, Valid loss: 2.6209, L1 loss: 1.8383\n",
            "Epoch [276/30000]: Train loss: 1.5823, Valid loss: 2.6121, L1 loss: 1.8265\n",
            "Epoch [277/30000]: Train loss: 1.5483, Valid loss: 2.5801, L1 loss: 1.7819\n",
            "Epoch [278/30000]: Train loss: 1.5404, Valid loss: 2.5801, L1 loss: 1.8015\n",
            "Epoch [279/30000]: Train loss: 1.5380, Valid loss: 2.5733, L1 loss: 1.8171\n",
            "Epoch [280/30000]: Train loss: 1.5056, Valid loss: 2.6218, L1 loss: 1.8115\n",
            "Epoch [281/30000]: Train loss: 1.5119, Valid loss: 2.5934, L1 loss: 1.7981\n",
            "Epoch [282/30000]: Train loss: 1.5198, Valid loss: 2.5319, L1 loss: 1.7910\n",
            "Epoch [283/30000]: Train loss: 1.5302, Valid loss: 2.6205, L1 loss: 1.8394\n",
            "Epoch [284/30000]: Train loss: 1.4852, Valid loss: 2.6151, L1 loss: 1.7835\n",
            "Epoch [285/30000]: Train loss: 1.5343, Valid loss: 2.5375, L1 loss: 1.7584\n",
            "Epoch [286/30000]: Train loss: 1.5364, Valid loss: 2.6048, L1 loss: 1.8145\n",
            "Epoch [287/30000]: Train loss: 1.5248, Valid loss: 2.6401, L1 loss: 1.8114\n",
            "Epoch [288/30000]: Train loss: 1.5201, Valid loss: 2.6903, L1 loss: 1.8482\n",
            "Epoch [289/30000]: Train loss: 1.5146, Valid loss: 2.5426, L1 loss: 1.7827\n",
            "Epoch [290/30000]: Train loss: 1.5078, Valid loss: 2.6558, L1 loss: 1.8486\n",
            "Epoch [291/30000]: Train loss: 1.5071, Valid loss: 2.6888, L1 loss: 1.8412\n",
            "Epoch [292/30000]: Train loss: 1.4986, Valid loss: 2.5659, L1 loss: 1.8112\n",
            "Epoch [293/30000]: Train loss: 1.5023, Valid loss: 2.6163, L1 loss: 1.8118\n",
            "Epoch [294/30000]: Train loss: 1.5076, Valid loss: 2.6195, L1 loss: 1.8334\n",
            "Epoch [295/30000]: Train loss: 1.4994, Valid loss: 2.5981, L1 loss: 1.7849\n",
            "Epoch [296/30000]: Train loss: 1.5107, Valid loss: 2.5925, L1 loss: 1.8155\n",
            "Epoch [297/30000]: Train loss: 1.4795, Valid loss: 2.6465, L1 loss: 1.8068\n",
            "Epoch [298/30000]: Train loss: 1.4770, Valid loss: 2.6578, L1 loss: 1.8383\n",
            "Epoch [299/30000]: Train loss: 1.5063, Valid loss: 2.6141, L1 loss: 1.8061\n",
            "Epoch [300/30000]: Train loss: 1.4871, Valid loss: 2.7249, L1 loss: 1.8545\n",
            "Epoch [301/30000]: Train loss: 1.4914, Valid loss: 2.6389, L1 loss: 1.8015\n",
            "Epoch [302/30000]: Train loss: 1.4632, Valid loss: 2.5461, L1 loss: 1.7534\n",
            "Epoch [303/30000]: Train loss: 1.4991, Valid loss: 2.6170, L1 loss: 1.8172\n",
            "Epoch [304/30000]: Train loss: 1.4777, Valid loss: 2.6887, L1 loss: 1.8191\n",
            "Epoch [305/30000]: Train loss: 1.4561, Valid loss: 2.6677, L1 loss: 1.8195\n",
            "Epoch [306/30000]: Train loss: 1.4277, Valid loss: 2.5997, L1 loss: 1.7851\n",
            "Epoch [307/30000]: Train loss: 1.4745, Valid loss: 2.6014, L1 loss: 1.8024\n",
            "Epoch [308/30000]: Train loss: 1.4595, Valid loss: 2.5682, L1 loss: 1.8048\n",
            "Epoch [309/30000]: Train loss: 1.4577, Valid loss: 2.6894, L1 loss: 1.8465\n",
            "Epoch [310/30000]: Train loss: 1.4534, Valid loss: 2.6615, L1 loss: 1.8517\n",
            "Epoch [311/30000]: Train loss: 1.4620, Valid loss: 2.5805, L1 loss: 1.7824\n",
            "Epoch [312/30000]: Train loss: 1.4280, Valid loss: 2.7344, L1 loss: 1.8518\n",
            "Epoch [313/30000]: Train loss: 1.4685, Valid loss: 2.6088, L1 loss: 1.8158\n",
            "Epoch [314/30000]: Train loss: 1.4585, Valid loss: 2.6436, L1 loss: 1.8245\n",
            "Epoch [315/30000]: Train loss: 1.4345, Valid loss: 2.6813, L1 loss: 1.8170\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-1f55fb7e93b6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMy_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put your model and data on the same computation device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-2edf5d36252f>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(train_data, model, config, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Compute gradient(backpropagation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# Update parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = My_Model(input_dim=train_data.shape[1]-1).to(device) # put your model and data on the same computation device.\n",
        "trainer(train_data, model, config, device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mNnGeuu72qdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "51c9b7b2-559f-4f5d-b049-9df386ba8c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 278.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7. 6. 5. ... 2. 4. 8.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d6419dc6-1ad1-4abd-ae61-aa507d29ed8c\", \"sample_submission.csv\", 69547)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'Danceability'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i+17170, p])\n",
        "model.load_state_dict(torch.load(config['save_path']))\n",
        "preds = predict(test_loader, model, device)\n",
        "print(preds)\n",
        "save_pred(preds, 'sample_submission.csv')\n",
        "from google.colab import files\n",
        "files.download('sample_submission.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}