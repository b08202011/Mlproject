{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b08202011/Mlproject/blob/main/%E5%85%88normalize%2Bsoftmax%2B%E7%AE%97%E6%9C%9F%E6%9C%9B%E5%80%BC%2BsoftL1loss%2Bconstraint%2Blr0_001%2Bmaxpoollayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klC1qgz2Oi2",
        "outputId": "bc89ccf2-40a9-4c6c-f622-502952c7d3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
            "To: /content/train.csv\n",
            "100% 23.3M/23.3M [00:00<00:00, 56.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
            "To: /content/test.csv\n",
            "100% 8.88M/8.88M [00:00<00:00, 49.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=187gijxd4T5yuZe_g2K9UNepx7MT1CnAu\n",
            "To: /content/sample_submission.csv\n",
            "100% 50.5k/50.5k [00:00<00:00, 107MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1lpwTva7SEtLRNwpBbYKZLoHDoeG7FTxp\n",
        "!gdown 15MfSdxpfkHQi_Z0qdJIdrdPzLrSfbX1E\n",
        "!gdown 187gijxd4T5yuZe_g2K9UNepx7MT1CnAu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "10JsWM_brgQw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "with open('train.csv',newline='') as csvfile:\n",
        "  train = csv.reader(csvfile)\n",
        "  train = list(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DscDJdCVr0kH",
        "outputId": "81e8cdb9-7f44-478f-bfef-9100354c9f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n"
          ]
        }
      ],
      "source": [
        "pick = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,23]\n",
        "#可以選要用那些特徵，0是我們要predict的\n",
        "#只選數值或true fale 但且沒有選id\n",
        "feature = []\n",
        "for i in range(1,len(train)):\n",
        "  feature.append([])\n",
        "  for j in pick:\n",
        "    feature[i-1].append(train[i][j])\n",
        "\n",
        "for i in range(len(feature)):\n",
        "  for j in range(len(feature[i])):\n",
        "    if(feature[i][j]!='' and feature[i][j]!='False' and feature[i][j]!= 'True'):\n",
        "      feature[i][j] = float(feature[i][j])\n",
        "    elif(feature[i][j] == 'False'):\n",
        "      feature[i][j] = 0\n",
        "    elif(feature[i][j] == 'True'):\n",
        "      feature[i][j] = 1\n",
        "    else:\n",
        "      feature[i][j] = np.nan\n",
        "print(len(feature))\n",
        "\n",
        "\n",
        "with open('test.csv',newline='') as csvfile:         ###load testdata\n",
        "  testcsv = csv.reader(csvfile)\n",
        "  testcsv = list(testcsv)\n",
        "  \n",
        "test = []\n",
        "picktest = [0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,22]\n",
        "\n",
        "for i in range(1,len(testcsv)):\n",
        "  test.append([])\n",
        "  for j in picktest:\n",
        "    test[i-1].append(testcsv[i][j])\n",
        "for i in range(len(test)):\n",
        "  for j in range(len(test[i])):\n",
        "    if(test[i][j]!='' and test[i][j]!='False' and test[i][j]!= 'True'):\n",
        "      test[i][j] = float(test[i][j])\n",
        "    elif(test[i][j] == 'False'):\n",
        "      test[i][j] = 0\n",
        "    elif(test[i][j] == 'True'):\n",
        "      test[i][j] = 1\n",
        "    else:\n",
        "      test[i][j] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFwj7Vtjr7C3",
        "outputId": "a24fdd8c-9490-46a3-c3e5-9721de55a2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ycimpute\n",
            "  Downloading ycimpute-0.2-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ycimpute) (2.0.1+cu118)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->ycimpute) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->ycimpute) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->ycimpute) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->ycimpute) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->ycimpute) (1.3.0)\n",
            "Installing collected packages: ycimpute\n",
            "Successfully installed ycimpute-0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ycimpute"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Operations\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Reading/Writing Data\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "# For Progress Bar\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import f_regression\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
        "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "# For plotting learning curve\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.functional import normalize"
      ],
      "metadata": {
        "id": "vnSka536aDrP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpQkUiqtr-VA",
        "outputId": "67be9879-7dde-47ce-920a-70a725721296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17170\n",
            "6315\n",
            "(23485, 16)\n",
            "[[ 2.73359449e-04  3.00000000e+00 -2.51760000e+01 ...  0.00000000e+00\n",
            "   0.00000000e+00  3.07000000e+03]\n",
            " [ 1.84220009e-01  7.00000000e+00             nan ...  0.00000000e+00\n",
            "              nan  1.22000000e+02]\n",
            " [            nan  6.00000000e+00 -1.55960000e+01 ...  0.00000000e+00\n",
            "   0.00000000e+00  1.22000000e+02]\n",
            " ...\n",
            " [ 4.26222894e-01  5.00000000e+00 -5.80754484e+00 ...  1.00000000e+00\n",
            "   0.00000000e+00             nan]\n",
            " [ 2.04440254e-01             nan -1.15639636e+01 ...  1.00000000e+00\n",
            "   1.00000000e+00  1.51390000e+04]\n",
            " [ 1.46792398e-02             nan             nan ...  1.00000000e+00\n",
            "   1.00000000e+00  1.10000000e+01]]\n"
          ]
        }
      ],
      "source": [
        "feature = np.array(feature)\n",
        "test = np.array(test)\n",
        "from ycimpute.imputer import knnimput\n",
        "xtrain, ytrain = feature[:,1:], feature[:,0]\n",
        "print(xtrain.shape[0])\n",
        "print(test.shape[0])\n",
        "data = np.concatenate((xtrain,test))\n",
        "print(data.shape)\n",
        "\n",
        "data_np = torch.from_numpy(data)\n",
        "mean, std, var = torch.mean(data_np), torch.std(data_np), torch.var(data_np)\n",
        "data_np = (data_np-mean)/std\n",
        "data_np = np.array(data.tolist())\n",
        "print(data_np)\n",
        "\n",
        "\n",
        "\n",
        "#imp_median.fit(feature)\n",
        "#feature = imp_median.transform(feature)\n",
        "##可以修改這裡 使用不同的data填補方式"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = knnimput.KNN(k=4).complete(data_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pae3xPxJdB_e",
        "outputId": "2b12a77a-f17a-4e5a-9e42-dcdd857afb72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing row 1/23485 with 1 missing, elapsed time: 155.771\n",
            "Imputing row 101/23485 with 1 missing, elapsed time: 155.801\n",
            "Imputing row 201/23485 with 0 missing, elapsed time: 155.833\n",
            "Imputing row 301/23485 with 2 missing, elapsed time: 155.859\n",
            "Imputing row 401/23485 with 1 missing, elapsed time: 155.886\n",
            "Imputing row 501/23485 with 1 missing, elapsed time: 155.914\n",
            "Imputing row 601/23485 with 4 missing, elapsed time: 155.944\n",
            "Imputing row 701/23485 with 2 missing, elapsed time: 155.976\n",
            "Imputing row 801/23485 with 2 missing, elapsed time: 156.013\n",
            "Imputing row 901/23485 with 3 missing, elapsed time: 156.043\n",
            "Imputing row 1001/23485 with 0 missing, elapsed time: 156.074\n",
            "Imputing row 1101/23485 with 0 missing, elapsed time: 156.101\n",
            "Imputing row 1201/23485 with 2 missing, elapsed time: 156.131\n",
            "Imputing row 1301/23485 with 3 missing, elapsed time: 156.163\n",
            "Imputing row 1401/23485 with 3 missing, elapsed time: 156.194\n",
            "Imputing row 1501/23485 with 3 missing, elapsed time: 156.224\n",
            "Imputing row 1601/23485 with 3 missing, elapsed time: 156.252\n",
            "Imputing row 1701/23485 with 1 missing, elapsed time: 156.282\n",
            "Imputing row 1801/23485 with 2 missing, elapsed time: 156.311\n",
            "Imputing row 1901/23485 with 0 missing, elapsed time: 156.341\n",
            "Imputing row 2001/23485 with 1 missing, elapsed time: 156.372\n",
            "Imputing row 2101/23485 with 6 missing, elapsed time: 156.404\n",
            "Imputing row 2201/23485 with 4 missing, elapsed time: 156.434\n",
            "Imputing row 2301/23485 with 2 missing, elapsed time: 156.464\n",
            "Imputing row 2401/23485 with 4 missing, elapsed time: 156.501\n",
            "Imputing row 2501/23485 with 2 missing, elapsed time: 156.530\n",
            "Imputing row 2601/23485 with 1 missing, elapsed time: 156.563\n",
            "Imputing row 2701/23485 with 1 missing, elapsed time: 156.594\n",
            "Imputing row 2801/23485 with 2 missing, elapsed time: 156.630\n",
            "Imputing row 2901/23485 with 0 missing, elapsed time: 156.661\n",
            "Imputing row 3001/23485 with 3 missing, elapsed time: 156.697\n",
            "Imputing row 3101/23485 with 1 missing, elapsed time: 156.729\n",
            "Imputing row 3201/23485 with 3 missing, elapsed time: 156.758\n",
            "Imputing row 3301/23485 with 4 missing, elapsed time: 156.788\n",
            "Imputing row 3401/23485 with 5 missing, elapsed time: 156.816\n",
            "Imputing row 3501/23485 with 1 missing, elapsed time: 156.854\n",
            "Imputing row 3601/23485 with 5 missing, elapsed time: 156.882\n",
            "Imputing row 3701/23485 with 3 missing, elapsed time: 156.911\n",
            "Imputing row 3801/23485 with 2 missing, elapsed time: 156.939\n",
            "Imputing row 3901/23485 with 4 missing, elapsed time: 156.966\n",
            "Imputing row 4001/23485 with 2 missing, elapsed time: 156.995\n",
            "Imputing row 4101/23485 with 2 missing, elapsed time: 157.025\n",
            "Imputing row 4201/23485 with 2 missing, elapsed time: 157.054\n",
            "Imputing row 4301/23485 with 2 missing, elapsed time: 157.087\n",
            "Imputing row 4401/23485 with 1 missing, elapsed time: 157.117\n",
            "Imputing row 4501/23485 with 2 missing, elapsed time: 157.147\n",
            "Imputing row 4601/23485 with 4 missing, elapsed time: 157.174\n",
            "Imputing row 4701/23485 with 2 missing, elapsed time: 157.208\n",
            "Imputing row 4801/23485 with 2 missing, elapsed time: 157.237\n",
            "Imputing row 4901/23485 with 3 missing, elapsed time: 157.266\n",
            "Imputing row 5001/23485 with 3 missing, elapsed time: 157.300\n",
            "Imputing row 5101/23485 with 3 missing, elapsed time: 157.335\n",
            "Imputing row 5201/23485 with 3 missing, elapsed time: 157.363\n",
            "Imputing row 5301/23485 with 2 missing, elapsed time: 157.397\n",
            "Imputing row 5401/23485 with 4 missing, elapsed time: 157.426\n",
            "Imputing row 5501/23485 with 4 missing, elapsed time: 157.458\n",
            "Imputing row 5601/23485 with 2 missing, elapsed time: 157.491\n",
            "Imputing row 5701/23485 with 2 missing, elapsed time: 157.524\n",
            "Imputing row 5801/23485 with 3 missing, elapsed time: 157.559\n",
            "Imputing row 5901/23485 with 2 missing, elapsed time: 157.596\n",
            "Imputing row 6001/23485 with 3 missing, elapsed time: 157.625\n",
            "Imputing row 6101/23485 with 2 missing, elapsed time: 157.653\n",
            "Imputing row 6201/23485 with 4 missing, elapsed time: 157.684\n",
            "Imputing row 6301/23485 with 1 missing, elapsed time: 157.712\n",
            "Imputing row 6401/23485 with 3 missing, elapsed time: 157.746\n",
            "Imputing row 6501/23485 with 1 missing, elapsed time: 157.775\n",
            "Imputing row 6601/23485 with 3 missing, elapsed time: 157.809\n",
            "Imputing row 6701/23485 with 2 missing, elapsed time: 157.839\n",
            "Imputing row 6801/23485 with 1 missing, elapsed time: 157.865\n",
            "Imputing row 6901/23485 with 3 missing, elapsed time: 157.895\n",
            "Imputing row 7001/23485 with 1 missing, elapsed time: 157.931\n",
            "Imputing row 7101/23485 with 2 missing, elapsed time: 157.962\n",
            "Imputing row 7201/23485 with 0 missing, elapsed time: 157.992\n",
            "Imputing row 7301/23485 with 2 missing, elapsed time: 158.021\n",
            "Imputing row 7401/23485 with 2 missing, elapsed time: 158.049\n",
            "Imputing row 7501/23485 with 2 missing, elapsed time: 158.080\n",
            "Imputing row 7601/23485 with 5 missing, elapsed time: 158.114\n",
            "Imputing row 7701/23485 with 2 missing, elapsed time: 158.146\n",
            "Imputing row 7801/23485 with 1 missing, elapsed time: 158.179\n",
            "Imputing row 7901/23485 with 1 missing, elapsed time: 158.210\n",
            "Imputing row 8001/23485 with 2 missing, elapsed time: 158.239\n",
            "Imputing row 8101/23485 with 4 missing, elapsed time: 158.271\n",
            "Imputing row 8201/23485 with 2 missing, elapsed time: 158.307\n",
            "Imputing row 8301/23485 with 3 missing, elapsed time: 158.339\n",
            "Imputing row 8401/23485 with 1 missing, elapsed time: 158.370\n",
            "Imputing row 8501/23485 with 2 missing, elapsed time: 158.403\n",
            "Imputing row 8601/23485 with 2 missing, elapsed time: 158.431\n",
            "Imputing row 8701/23485 with 4 missing, elapsed time: 158.460\n",
            "Imputing row 8801/23485 with 2 missing, elapsed time: 158.492\n",
            "Imputing row 8901/23485 with 4 missing, elapsed time: 158.532\n",
            "Imputing row 9001/23485 with 1 missing, elapsed time: 158.562\n",
            "Imputing row 9101/23485 with 1 missing, elapsed time: 158.599\n",
            "Imputing row 9201/23485 with 1 missing, elapsed time: 158.633\n",
            "Imputing row 9301/23485 with 5 missing, elapsed time: 158.662\n",
            "Imputing row 9401/23485 with 7 missing, elapsed time: 158.689\n",
            "Imputing row 9501/23485 with 0 missing, elapsed time: 158.717\n",
            "Imputing row 9601/23485 with 3 missing, elapsed time: 158.746\n",
            "Imputing row 9701/23485 with 3 missing, elapsed time: 158.774\n",
            "Imputing row 9801/23485 with 3 missing, elapsed time: 158.806\n",
            "Imputing row 9901/23485 with 1 missing, elapsed time: 158.836\n",
            "Imputing row 10001/23485 with 3 missing, elapsed time: 158.870\n",
            "Imputing row 10101/23485 with 3 missing, elapsed time: 158.898\n",
            "Imputing row 10201/23485 with 3 missing, elapsed time: 158.928\n",
            "Imputing row 10301/23485 with 1 missing, elapsed time: 158.962\n",
            "Imputing row 10401/23485 with 3 missing, elapsed time: 158.990\n",
            "Imputing row 10501/23485 with 4 missing, elapsed time: 159.032\n",
            "Imputing row 10601/23485 with 2 missing, elapsed time: 159.072\n",
            "Imputing row 10701/23485 with 1 missing, elapsed time: 159.105\n",
            "Imputing row 10801/23485 with 3 missing, elapsed time: 159.135\n",
            "Imputing row 10901/23485 with 1 missing, elapsed time: 159.165\n",
            "Imputing row 11001/23485 with 3 missing, elapsed time: 159.196\n",
            "Imputing row 11101/23485 with 3 missing, elapsed time: 159.228\n",
            "Imputing row 11201/23485 with 2 missing, elapsed time: 159.260\n",
            "Imputing row 11301/23485 with 1 missing, elapsed time: 159.297\n",
            "Imputing row 11401/23485 with 2 missing, elapsed time: 159.327\n",
            "Imputing row 11501/23485 with 2 missing, elapsed time: 159.355\n",
            "Imputing row 11601/23485 with 2 missing, elapsed time: 159.380\n",
            "Imputing row 11701/23485 with 1 missing, elapsed time: 159.412\n",
            "Imputing row 11801/23485 with 3 missing, elapsed time: 159.445\n",
            "Imputing row 11901/23485 with 2 missing, elapsed time: 159.473\n",
            "Imputing row 12001/23485 with 2 missing, elapsed time: 159.510\n",
            "Imputing row 12101/23485 with 1 missing, elapsed time: 159.541\n",
            "Imputing row 12201/23485 with 3 missing, elapsed time: 159.572\n",
            "Imputing row 12301/23485 with 3 missing, elapsed time: 159.606\n",
            "Imputing row 12401/23485 with 5 missing, elapsed time: 159.641\n",
            "Imputing row 12501/23485 with 2 missing, elapsed time: 159.672\n",
            "Imputing row 12601/23485 with 4 missing, elapsed time: 159.703\n",
            "Imputing row 12701/23485 with 2 missing, elapsed time: 159.735\n",
            "Imputing row 12801/23485 with 4 missing, elapsed time: 159.765\n",
            "Imputing row 12901/23485 with 2 missing, elapsed time: 159.798\n",
            "Imputing row 13001/23485 with 3 missing, elapsed time: 159.827\n",
            "Imputing row 13101/23485 with 2 missing, elapsed time: 159.860\n",
            "Imputing row 13201/23485 with 2 missing, elapsed time: 159.890\n",
            "Imputing row 13301/23485 with 1 missing, elapsed time: 159.922\n",
            "Imputing row 13401/23485 with 2 missing, elapsed time: 159.957\n",
            "Imputing row 13501/23485 with 3 missing, elapsed time: 159.985\n",
            "Imputing row 13601/23485 with 2 missing, elapsed time: 160.011\n",
            "Imputing row 13701/23485 with 3 missing, elapsed time: 160.043\n",
            "Imputing row 13801/23485 with 3 missing, elapsed time: 160.076\n",
            "Imputing row 13901/23485 with 1 missing, elapsed time: 160.107\n",
            "Imputing row 14001/23485 with 5 missing, elapsed time: 160.138\n",
            "Imputing row 14101/23485 with 1 missing, elapsed time: 160.176\n",
            "Imputing row 14201/23485 with 0 missing, elapsed time: 160.207\n",
            "Imputing row 14301/23485 with 2 missing, elapsed time: 160.236\n",
            "Imputing row 14401/23485 with 3 missing, elapsed time: 160.267\n",
            "Imputing row 14501/23485 with 1 missing, elapsed time: 160.298\n",
            "Imputing row 14601/23485 with 0 missing, elapsed time: 160.333\n",
            "Imputing row 14701/23485 with 1 missing, elapsed time: 160.362\n",
            "Imputing row 14801/23485 with 2 missing, elapsed time: 160.395\n",
            "Imputing row 14901/23485 with 3 missing, elapsed time: 160.426\n",
            "Imputing row 15001/23485 with 2 missing, elapsed time: 160.456\n",
            "Imputing row 15101/23485 with 1 missing, elapsed time: 160.489\n",
            "Imputing row 15201/23485 with 1 missing, elapsed time: 160.524\n",
            "Imputing row 15301/23485 with 3 missing, elapsed time: 160.554\n",
            "Imputing row 15401/23485 with 1 missing, elapsed time: 160.583\n",
            "Imputing row 15501/23485 with 2 missing, elapsed time: 160.618\n",
            "Imputing row 15601/23485 with 1 missing, elapsed time: 160.657\n",
            "Imputing row 15701/23485 with 3 missing, elapsed time: 160.687\n",
            "Imputing row 15801/23485 with 2 missing, elapsed time: 160.714\n",
            "Imputing row 15901/23485 with 2 missing, elapsed time: 160.746\n",
            "Imputing row 16001/23485 with 1 missing, elapsed time: 160.775\n",
            "Imputing row 16101/23485 with 2 missing, elapsed time: 160.805\n",
            "Imputing row 16201/23485 with 0 missing, elapsed time: 160.842\n",
            "Imputing row 16301/23485 with 1 missing, elapsed time: 160.873\n",
            "Imputing row 16401/23485 with 3 missing, elapsed time: 160.902\n",
            "Imputing row 16501/23485 with 1 missing, elapsed time: 160.929\n",
            "Imputing row 16601/23485 with 4 missing, elapsed time: 160.958\n",
            "Imputing row 16701/23485 with 3 missing, elapsed time: 160.992\n",
            "Imputing row 16801/23485 with 2 missing, elapsed time: 161.027\n",
            "Imputing row 16901/23485 with 4 missing, elapsed time: 161.060\n",
            "Imputing row 17001/23485 with 4 missing, elapsed time: 161.087\n",
            "Imputing row 17101/23485 with 6 missing, elapsed time: 161.120\n",
            "Imputing row 17201/23485 with 3 missing, elapsed time: 161.151\n",
            "Imputing row 17301/23485 with 1 missing, elapsed time: 161.180\n",
            "Imputing row 17401/23485 with 5 missing, elapsed time: 161.210\n",
            "Imputing row 17501/23485 with 4 missing, elapsed time: 161.246\n",
            "Imputing row 17601/23485 with 0 missing, elapsed time: 161.278\n",
            "Imputing row 17701/23485 with 0 missing, elapsed time: 161.310\n",
            "Imputing row 17801/23485 with 4 missing, elapsed time: 161.340\n",
            "Imputing row 17901/23485 with 1 missing, elapsed time: 161.371\n",
            "Imputing row 18001/23485 with 2 missing, elapsed time: 161.405\n",
            "Imputing row 18101/23485 with 5 missing, elapsed time: 161.438\n",
            "Imputing row 18201/23485 with 2 missing, elapsed time: 161.467\n",
            "Imputing row 18301/23485 with 6 missing, elapsed time: 161.506\n",
            "Imputing row 18401/23485 with 5 missing, elapsed time: 161.533\n",
            "Imputing row 18501/23485 with 1 missing, elapsed time: 161.563\n",
            "Imputing row 18601/23485 with 0 missing, elapsed time: 161.594\n",
            "Imputing row 18701/23485 with 4 missing, elapsed time: 161.622\n",
            "Imputing row 18801/23485 with 3 missing, elapsed time: 161.654\n",
            "Imputing row 18901/23485 with 1 missing, elapsed time: 161.692\n",
            "Imputing row 19001/23485 with 1 missing, elapsed time: 161.725\n",
            "Imputing row 19101/23485 with 2 missing, elapsed time: 161.762\n",
            "Imputing row 19201/23485 with 2 missing, elapsed time: 161.789\n",
            "Imputing row 19301/23485 with 2 missing, elapsed time: 161.820\n",
            "Imputing row 19401/23485 with 6 missing, elapsed time: 161.852\n",
            "Imputing row 19501/23485 with 3 missing, elapsed time: 161.884\n",
            "Imputing row 19601/23485 with 5 missing, elapsed time: 161.916\n",
            "Imputing row 19701/23485 with 3 missing, elapsed time: 161.948\n",
            "Imputing row 19801/23485 with 4 missing, elapsed time: 161.979\n",
            "Imputing row 19901/23485 with 3 missing, elapsed time: 162.007\n",
            "Imputing row 20001/23485 with 3 missing, elapsed time: 162.034\n",
            "Imputing row 20101/23485 with 2 missing, elapsed time: 162.065\n",
            "Imputing row 20201/23485 with 0 missing, elapsed time: 162.099\n",
            "Imputing row 20301/23485 with 2 missing, elapsed time: 162.134\n",
            "Imputing row 20401/23485 with 3 missing, elapsed time: 162.168\n",
            "Imputing row 20501/23485 with 1 missing, elapsed time: 162.200\n",
            "Imputing row 20601/23485 with 3 missing, elapsed time: 162.230\n",
            "Imputing row 20701/23485 with 0 missing, elapsed time: 162.260\n",
            "Imputing row 20801/23485 with 0 missing, elapsed time: 162.292\n",
            "Imputing row 20901/23485 with 2 missing, elapsed time: 162.326\n",
            "Imputing row 21001/23485 with 2 missing, elapsed time: 162.353\n",
            "Imputing row 21101/23485 with 4 missing, elapsed time: 162.386\n",
            "Imputing row 21201/23485 with 2 missing, elapsed time: 162.415\n",
            "Imputing row 21301/23485 with 3 missing, elapsed time: 162.441\n",
            "Imputing row 21401/23485 with 3 missing, elapsed time: 162.469\n",
            "Imputing row 21501/23485 with 2 missing, elapsed time: 162.508\n",
            "Imputing row 21601/23485 with 0 missing, elapsed time: 162.537\n",
            "Imputing row 21701/23485 with 3 missing, elapsed time: 162.565\n",
            "Imputing row 21801/23485 with 4 missing, elapsed time: 162.597\n",
            "Imputing row 21901/23485 with 2 missing, elapsed time: 162.628\n",
            "Imputing row 22001/23485 with 0 missing, elapsed time: 162.655\n",
            "Imputing row 22101/23485 with 3 missing, elapsed time: 162.689\n",
            "Imputing row 22201/23485 with 2 missing, elapsed time: 162.716\n",
            "Imputing row 22301/23485 with 2 missing, elapsed time: 162.744\n",
            "Imputing row 22401/23485 with 2 missing, elapsed time: 162.777\n",
            "Imputing row 22501/23485 with 2 missing, elapsed time: 162.813\n",
            "Imputing row 22601/23485 with 1 missing, elapsed time: 162.843\n",
            "Imputing row 22701/23485 with 1 missing, elapsed time: 162.872\n",
            "Imputing row 22801/23485 with 2 missing, elapsed time: 162.906\n",
            "Imputing row 22901/23485 with 3 missing, elapsed time: 162.938\n",
            "Imputing row 23001/23485 with 3 missing, elapsed time: 162.968\n",
            "Imputing row 23101/23485 with 2 missing, elapsed time: 163.000\n",
            "Imputing row 23201/23485 with 1 missing, elapsed time: 163.032\n",
            "Imputing row 23301/23485 with 3 missing, elapsed time: 163.062\n",
            "Imputing row 23401/23485 with 3 missing, elapsed time: 163.093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_np[3])\n",
        "feature, test_data = data_np[:xtrain.shape[0],:], data_np[xtrain.shape[0]:,:]\n",
        "print(feature.shape)\n",
        "ytrain = np.reshape(ytrain,(ytrain.shape[0],1))\n",
        "feature = np.concatenate((ytrain,feature),axis=1)\n",
        "print(feature.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo7Ka39Qbzvz",
        "outputId": "f04ea53d-d262-499b-e7ba-4c64c3b37dc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.09584584e-01  7.76528498e+00 -6.25100000e+00  2.77000000e-02\n",
            "  3.79641600e-03  2.11999054e-02  1.00000000e-03  3.71806581e-01\n",
            "  1.50220000e+02  2.65000000e+05  2.02457327e+08  9.97035000e+05\n",
            "  3.99661898e+08  1.00000000e+00  9.99999948e-01  2.43500000e+04]\n",
            "(17170, 16)\n",
            "17170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IyLGsIqeWbL",
        "outputId": "1f76c277-3d4d-4f12-ca9a-6fa2544690bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00000000e+00 2.73359449e-04 3.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 3.07000000e+03]\n",
            " [0.00000000e+00 1.84220009e-01 7.00000000e+00 ... 0.00000000e+00\n",
            "  1.39303658e-01 1.22000000e+02]\n",
            " [0.00000000e+00 2.42020446e-02 6.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.22000000e+02]\n",
            " ...\n",
            " [1.00000000e+00 5.71787000e-01 4.00000000e+00 ... 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 4.51217663e-01 9.00000000e+00 ... 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [2.00000000e+00 8.25293672e-01 6.00000000e+00 ... 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EuIQ-7tKuocP"
      },
      "outputs": [],
      "source": [
        "def same_seed(seed): \n",
        "    '''Fixes random number generator seeds for reproducibility.'''\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_valid_split(data_set, valid_ratio, seed):\n",
        "    '''Split provided training data into training set and validation set'''\n",
        "    valid_set_size = int(valid_ratio * len(data_set)) \n",
        "    train_set_size = len(data_set) - valid_set_size\n",
        "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
        "    return np.array(train_set), np.array(valid_set)\n",
        "\n",
        "def predict(test_loader, model, device):\n",
        "    model.eval() # Set your model to evaluation mode.\n",
        "    preds = []\n",
        "    for x in tqdm(test_loader):\n",
        "        x = x.to(device)                        \n",
        "        with torch.no_grad():                   \n",
        "            pred = torch.clamp(torch.round(model(x)),0,9)\n",
        "                          \n",
        "            preds.append(pred.detach().cpu())   \n",
        "    preds = torch.cat(preds, dim=0).numpy()  \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D320CZu7nkU"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ypSQGM5i7q_g"
      },
      "outputs": [],
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    '''\n",
        "    x: Features.\n",
        "    y: Targets, if none, do prediction.\n",
        "    '''\n",
        "    def __init__(self, x, y=None):\n",
        "        if y is None:\n",
        "            self.y = y\n",
        "        else:\n",
        "            self.y = torch.FloatTensor(y)\n",
        "        self.x = torch.FloatTensor(x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.x[idx]\n",
        "        else:\n",
        "            return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dFunF8QtoP_"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zIkJ9iRauAy8"
      },
      "outputs": [],
      "source": [
        "class My_Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(My_Model, self).__init__()\n",
        "        # TODO: modify model's structure, be aware of dimensions. \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "                                     \n",
        "        )\n",
        "        self.maxpoollayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.BatchNorm1d(16),\n",
        "        )\n",
        "        self.finallayer = nn.Sequential(\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(16, 10),\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "    def forward(self, x):\n",
        "        #x = x.view(-1, 16)\n",
        "        x = self.layers(x)\n",
        "        x = self.maxpoollayer(x)\n",
        "        x = self.maxpoollayer(x)\n",
        "        x = self.maxpoollayer(x)\n",
        "        x = self.finallayer(x)\n",
        "        x = self.softmax(x)\n",
        "        #x = self.fc(x)\n",
        "        #print(x)\n",
        "        out = torch.linspace(0,9,10).to(device)\n",
        "        \n",
        "        #x = torch.sigmoid(x)\n",
        "        #x = torch.mul(x,9)\n",
        "        x = torch.mul(x,out)\n",
        "        x = torch.sum(x, dim=1)\n",
        "        #print(x)\n",
        "        x = x.squeeze(-1) # (B, 1) -> (B)\n",
        "        return x\n",
        "class wideL1loss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(wideL1loss, self).__init__()\n",
        "  def forward(self, x,y):\n",
        "    loss = torch.mean(torch.sigmoid(torch.subtract(torch.clamp(torch.abs(x-y),2.5,10),2.5)))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EXYkYmuQzB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "khMhvW1duO9l"
      },
      "outputs": [],
      "source": [
        "def trainer(train_data, model, config, device):\n",
        "\n",
        "    criterion = nn.SmoothL1Loss(beta=0.5) # Define your loss function, do not modify this.\n",
        "    loss2 = wideL1loss()\n",
        "    criterion2 = nn.L1Loss()\n",
        "\n",
        "    # Define your optimization algorithm. \n",
        "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
        "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr= config['learning_rate'] , weight_decay=0.01, amsgrad=False) \n",
        "    writer = SummaryWriter() # Writer of tensoboard.\n",
        "\n",
        "    if not os.path.isdir('./models'):\n",
        "        os.mkdir('./models') # Create directory of saving models.\n",
        "\n",
        "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
        "\n",
        "\n",
        "    np.random.seed(config['seed'])\n",
        "    splits = np.array_split(train_data, 5)\n",
        "    train =  np.concatenate(splits[:0]+splits[0+1:])\n",
        "    valid= splits[0]\n",
        "    x_train = train[...,1:]\n",
        "    x_valid = valid[...,1:]\n",
        "    y_train = train[...,0]\n",
        "    y_valid = valid[...,0]\n",
        "    train_dataset  = COVID19Dataset(x_train, y_train)\n",
        "    valid_dataset = COVID19Dataset(x_valid, y_valid)\n",
        "    \n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      lossvalid_record = []\n",
        "      loss_record = []\n",
        "      realloss =[]\n",
        "      for i in range(1):\n",
        "        model.train() # Set your model to train mode.\n",
        "         \n",
        "        \n",
        "        \n",
        "        # Pytorch data loader loads pytorch dataset into batches.\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()               # Set gradient to zero.\n",
        "            x, y = x.to(device), y.to(device)   # Move your data to device. \n",
        "            pred = model(x)\n",
        "            \n",
        "            #loss = criterion(pred, y)\n",
        "            l1_norm = sum(p.abs().sum()for p in model.parameters())\n",
        "                \n",
        "            loss = criterion(pred, y)+config['loss2']*loss2(pred, y)+config['regulizer']*l1_norm\n",
        "            #print(loss)\n",
        "            loss.backward()                     # Compute gradient(backpropagation).\n",
        "            optimizer.step()                    # Update parameters.\n",
        "            step += 1\n",
        "            \n",
        "            loss_record.append(loss.detach().item())\n",
        "            \n",
        "         \n",
        "        model.eval() # Set your model to evaluation mode.\n",
        "        \n",
        "        for x, y in valid_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = torch.clamp(torch.round(model(x)),0,9)\n",
        "                #loss = criterion(pred, y)\n",
        "                l1_norm = sum(p.abs().sum()for p in model.parameters()) \n",
        "                loss = criterion(pred, y)+config['regulizer']*l1_norm+config['loss2']*loss2(pred, y)\n",
        "                acloss = criterion2(pred, y)\n",
        "            lossvalid_record.append(loss.item())\n",
        "            realloss.append(acloss.item())\n",
        "\n",
        "\n",
        "        \n",
        "      mean_train_loss = sum(loss_record)/len(loss_record)         ###compute CV loss\n",
        "      writer.add_scalar('Loss/train', mean_train_loss, step)\n",
        "      reloss = sum(realloss)/len(realloss)\n",
        "      mean_valid_loss = sum(lossvalid_record)/len(lossvalid_record)\n",
        "      print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, L1 loss: {reloss:.4f}')\n",
        "      # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
        "\n",
        "      if mean_valid_loss < best_loss:\n",
        "          best_loss = mean_valid_loss\n",
        "          torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
        "          print('Saving model with loss {:.3f}...'.format(best_loss))\n",
        "          early_stop_count = 0\n",
        "      else: \n",
        "          early_stop_count += 1\n",
        "\n",
        "      if early_stop_count >= config['early_stop']:\n",
        "          print('\\nModel is not improving, so we halt the training session.')\n",
        "          return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Hg8aMlzaqk"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dB14dhABzVxO"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "config = {\n",
        "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
        "    'select_all': True,   # Whether to use all features.\n",
        "    'n_fold': 5,   # validation_size = train_size * valid_ratio\n",
        "    'n_epochs': 30000,     # Number of epochs.            \n",
        "    'batch_size': 128, \n",
        "    'learning_rate': 1e-3,\n",
        "    'regulizer': 0,  \n",
        "    'loss2'  :1.5,          \n",
        "    'early_stop': 1500,    # If model has not improved for this many consecutive epochs, stop training.     \n",
        "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DqtxFou74eQr"
      },
      "outputs": [],
      "source": [
        "same_seed(config['seed'])\n",
        "\n",
        "#imp_median.fit(test)\n",
        "#test_data = imp_median.transform(test)\n",
        "train_data= feature\n",
        "#train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
        "\n",
        "x_test =  test_data\n",
        "test_dataset = COVID19Dataset(x_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zpP7hF1y18"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hn82Vj0R06Yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8863a291-1cc8-444a-a630-c2f28e126d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30000]: Train loss: 2.4884, Valid loss: 2.4103, L1 loss: 2.3634\n",
            "Saving model with loss 2.410...\n",
            "Epoch [2/30000]: Train loss: 2.2891, Valid loss: 2.1452, L1 loss: 2.1413\n",
            "Saving model with loss 2.145...\n",
            "Epoch [3/30000]: Train loss: 2.1168, Valid loss: 1.9939, L1 loss: 2.0106\n",
            "Saving model with loss 1.994...\n",
            "Epoch [4/30000]: Train loss: 2.0096, Valid loss: 1.9509, L1 loss: 1.9644\n",
            "Saving model with loss 1.951...\n",
            "Epoch [5/30000]: Train loss: 1.9490, Valid loss: 1.8935, L1 loss: 1.9182\n",
            "Saving model with loss 1.894...\n",
            "Epoch [6/30000]: Train loss: 1.9163, Valid loss: 1.8044, L1 loss: 1.8384\n",
            "Saving model with loss 1.804...\n",
            "Epoch [7/30000]: Train loss: 1.8763, Valid loss: 1.8060, L1 loss: 1.8344\n",
            "Epoch [8/30000]: Train loss: 1.8414, Valid loss: 1.7909, L1 loss: 1.8214\n",
            "Saving model with loss 1.791...\n",
            "Epoch [9/30000]: Train loss: 1.8312, Valid loss: 1.7667, L1 loss: 1.8016\n",
            "Saving model with loss 1.767...\n",
            "Epoch [10/30000]: Train loss: 1.8084, Valid loss: 1.7394, L1 loss: 1.7748\n",
            "Saving model with loss 1.739...\n",
            "Epoch [11/30000]: Train loss: 1.7895, Valid loss: 1.7305, L1 loss: 1.7666\n",
            "Saving model with loss 1.731...\n",
            "Epoch [12/30000]: Train loss: 1.7783, Valid loss: 1.7385, L1 loss: 1.7697\n",
            "Epoch [13/30000]: Train loss: 1.7616, Valid loss: 1.7132, L1 loss: 1.7448\n",
            "Saving model with loss 1.713...\n",
            "Epoch [14/30000]: Train loss: 1.7570, Valid loss: 1.7138, L1 loss: 1.7381\n",
            "Epoch [15/30000]: Train loss: 1.7457, Valid loss: 1.7135, L1 loss: 1.7429\n",
            "Epoch [16/30000]: Train loss: 1.7391, Valid loss: 1.6991, L1 loss: 1.7261\n",
            "Saving model with loss 1.699...\n",
            "Epoch [17/30000]: Train loss: 1.7382, Valid loss: 1.7021, L1 loss: 1.7326\n",
            "Epoch [18/30000]: Train loss: 1.7396, Valid loss: 1.6951, L1 loss: 1.7219\n",
            "Saving model with loss 1.695...\n",
            "Epoch [19/30000]: Train loss: 1.7238, Valid loss: 1.6961, L1 loss: 1.7272\n",
            "Epoch [20/30000]: Train loss: 1.7249, Valid loss: 1.6842, L1 loss: 1.7160\n",
            "Saving model with loss 1.684...\n",
            "Epoch [21/30000]: Train loss: 1.7238, Valid loss: 1.6765, L1 loss: 1.7110\n",
            "Saving model with loss 1.676...\n",
            "Epoch [22/30000]: Train loss: 1.7158, Valid loss: 1.6974, L1 loss: 1.7247\n",
            "Epoch [23/30000]: Train loss: 1.7193, Valid loss: 1.6860, L1 loss: 1.7178\n",
            "Epoch [24/30000]: Train loss: 1.7117, Valid loss: 1.6749, L1 loss: 1.7123\n",
            "Saving model with loss 1.675...\n",
            "Epoch [25/30000]: Train loss: 1.7085, Valid loss: 1.6835, L1 loss: 1.7173\n",
            "Epoch [26/30000]: Train loss: 1.7041, Valid loss: 1.6702, L1 loss: 1.7068\n",
            "Saving model with loss 1.670...\n",
            "Epoch [27/30000]: Train loss: 1.7009, Valid loss: 1.6899, L1 loss: 1.7199\n",
            "Epoch [28/30000]: Train loss: 1.6971, Valid loss: 1.6680, L1 loss: 1.7020\n",
            "Saving model with loss 1.668...\n",
            "Epoch [29/30000]: Train loss: 1.7040, Valid loss: 1.6724, L1 loss: 1.7076\n",
            "Epoch [30/30000]: Train loss: 1.6954, Valid loss: 1.6657, L1 loss: 1.7028\n",
            "Saving model with loss 1.666...\n",
            "Epoch [31/30000]: Train loss: 1.6896, Valid loss: 1.6861, L1 loss: 1.7144\n",
            "Epoch [32/30000]: Train loss: 1.6849, Valid loss: 1.6626, L1 loss: 1.6937\n",
            "Saving model with loss 1.663...\n",
            "Epoch [33/30000]: Train loss: 1.6865, Valid loss: 1.7078, L1 loss: 1.7363\n",
            "Epoch [34/30000]: Train loss: 1.6849, Valid loss: 1.6839, L1 loss: 1.7162\n",
            "Epoch [35/30000]: Train loss: 1.6883, Valid loss: 1.6825, L1 loss: 1.7116\n",
            "Epoch [36/30000]: Train loss: 1.6929, Valid loss: 1.6881, L1 loss: 1.7158\n",
            "Epoch [37/30000]: Train loss: 1.6835, Valid loss: 1.6893, L1 loss: 1.7195\n",
            "Epoch [38/30000]: Train loss: 1.6673, Valid loss: 1.6969, L1 loss: 1.7187\n",
            "Epoch [39/30000]: Train loss: 1.6817, Valid loss: 1.6683, L1 loss: 1.6993\n",
            "Epoch [40/30000]: Train loss: 1.6721, Valid loss: 1.6728, L1 loss: 1.7044\n",
            "Epoch [41/30000]: Train loss: 1.6830, Valid loss: 1.7048, L1 loss: 1.7244\n",
            "Epoch [42/30000]: Train loss: 1.6734, Valid loss: 1.6872, L1 loss: 1.7151\n",
            "Epoch [43/30000]: Train loss: 1.6698, Valid loss: 1.6897, L1 loss: 1.7182\n",
            "Epoch [44/30000]: Train loss: 1.6726, Valid loss: 1.6703, L1 loss: 1.7055\n",
            "Epoch [45/30000]: Train loss: 1.6728, Valid loss: 1.7098, L1 loss: 1.7293\n",
            "Epoch [46/30000]: Train loss: 1.6573, Valid loss: 1.6972, L1 loss: 1.7170\n",
            "Epoch [47/30000]: Train loss: 1.6694, Valid loss: 1.6788, L1 loss: 1.7101\n",
            "Epoch [48/30000]: Train loss: 1.6642, Valid loss: 1.7008, L1 loss: 1.7266\n",
            "Epoch [49/30000]: Train loss: 1.6627, Valid loss: 1.7110, L1 loss: 1.7346\n",
            "Epoch [50/30000]: Train loss: 1.6733, Valid loss: 1.7147, L1 loss: 1.7338\n",
            "Epoch [51/30000]: Train loss: 1.6651, Valid loss: 1.7057, L1 loss: 1.7330\n",
            "Epoch [52/30000]: Train loss: 1.6598, Valid loss: 1.6880, L1 loss: 1.7179\n",
            "Epoch [53/30000]: Train loss: 1.6594, Valid loss: 1.7145, L1 loss: 1.7371\n",
            "Epoch [54/30000]: Train loss: 1.6576, Valid loss: 1.6806, L1 loss: 1.7085\n",
            "Epoch [55/30000]: Train loss: 1.6618, Valid loss: 1.7086, L1 loss: 1.7305\n",
            "Epoch [56/30000]: Train loss: 1.6676, Valid loss: 1.6918, L1 loss: 1.7223\n",
            "Epoch [57/30000]: Train loss: 1.6448, Valid loss: 1.7428, L1 loss: 1.7596\n",
            "Epoch [58/30000]: Train loss: 1.6530, Valid loss: 1.6928, L1 loss: 1.7223\n",
            "Epoch [59/30000]: Train loss: 1.6420, Valid loss: 1.7170, L1 loss: 1.7423\n",
            "Epoch [60/30000]: Train loss: 1.6487, Valid loss: 1.7075, L1 loss: 1.7327\n",
            "Epoch [61/30000]: Train loss: 1.6493, Valid loss: 1.7048, L1 loss: 1.7246\n",
            "Epoch [62/30000]: Train loss: 1.6457, Valid loss: 1.7277, L1 loss: 1.7478\n",
            "Epoch [63/30000]: Train loss: 1.6345, Valid loss: 1.7370, L1 loss: 1.7562\n",
            "Epoch [64/30000]: Train loss: 1.6485, Valid loss: 1.6860, L1 loss: 1.7185\n",
            "Epoch [65/30000]: Train loss: 1.6374, Valid loss: 1.6902, L1 loss: 1.7202\n",
            "Epoch [66/30000]: Train loss: 1.6495, Valid loss: 1.7057, L1 loss: 1.7311\n",
            "Epoch [67/30000]: Train loss: 1.6408, Valid loss: 1.6857, L1 loss: 1.7135\n",
            "Epoch [68/30000]: Train loss: 1.6412, Valid loss: 1.7152, L1 loss: 1.7428\n",
            "Epoch [69/30000]: Train loss: 1.6356, Valid loss: 1.7581, L1 loss: 1.7699\n",
            "Epoch [70/30000]: Train loss: 1.6420, Valid loss: 1.7033, L1 loss: 1.7331\n",
            "Epoch [71/30000]: Train loss: 1.6418, Valid loss: 1.7108, L1 loss: 1.7388\n",
            "Epoch [72/30000]: Train loss: 1.6382, Valid loss: 1.7220, L1 loss: 1.7434\n",
            "Epoch [73/30000]: Train loss: 1.6327, Valid loss: 1.6839, L1 loss: 1.7202\n",
            "Epoch [74/30000]: Train loss: 1.6399, Valid loss: 1.7020, L1 loss: 1.7281\n",
            "Epoch [75/30000]: Train loss: 1.6368, Valid loss: 1.7010, L1 loss: 1.7283\n",
            "Epoch [76/30000]: Train loss: 1.6369, Valid loss: 1.6879, L1 loss: 1.7205\n",
            "Epoch [77/30000]: Train loss: 1.6229, Valid loss: 1.7210, L1 loss: 1.7531\n",
            "Epoch [78/30000]: Train loss: 1.6378, Valid loss: 1.6755, L1 loss: 1.7100\n",
            "Epoch [79/30000]: Train loss: 1.6416, Valid loss: 1.6872, L1 loss: 1.7174\n",
            "Epoch [80/30000]: Train loss: 1.6345, Valid loss: 1.6873, L1 loss: 1.7180\n",
            "Epoch [81/30000]: Train loss: 1.6333, Valid loss: 1.6922, L1 loss: 1.7279\n",
            "Epoch [82/30000]: Train loss: 1.6278, Valid loss: 1.6883, L1 loss: 1.7204\n",
            "Epoch [83/30000]: Train loss: 1.6344, Valid loss: 1.6957, L1 loss: 1.7253\n",
            "Epoch [84/30000]: Train loss: 1.6372, Valid loss: 1.6687, L1 loss: 1.7056\n",
            "Epoch [85/30000]: Train loss: 1.6241, Valid loss: 1.6651, L1 loss: 1.7059\n",
            "Epoch [86/30000]: Train loss: 1.6286, Valid loss: 1.7016, L1 loss: 1.7347\n",
            "Epoch [87/30000]: Train loss: 1.6313, Valid loss: 1.6967, L1 loss: 1.7298\n",
            "Epoch [88/30000]: Train loss: 1.6276, Valid loss: 1.7101, L1 loss: 1.7378\n",
            "Epoch [89/30000]: Train loss: 1.6391, Valid loss: 1.6774, L1 loss: 1.7172\n",
            "Epoch [90/30000]: Train loss: 1.6307, Valid loss: 1.6998, L1 loss: 1.7344\n",
            "Epoch [91/30000]: Train loss: 1.6279, Valid loss: 1.6602, L1 loss: 1.7026\n",
            "Saving model with loss 1.660...\n",
            "Epoch [92/30000]: Train loss: 1.6304, Valid loss: 1.6645, L1 loss: 1.6980\n",
            "Epoch [93/30000]: Train loss: 1.6185, Valid loss: 1.6870, L1 loss: 1.7204\n",
            "Epoch [94/30000]: Train loss: 1.6320, Valid loss: 1.7145, L1 loss: 1.7401\n",
            "Epoch [95/30000]: Train loss: 1.6338, Valid loss: 1.6579, L1 loss: 1.6962\n",
            "Saving model with loss 1.658...\n",
            "Epoch [96/30000]: Train loss: 1.6225, Valid loss: 1.6953, L1 loss: 1.7282\n",
            "Epoch [97/30000]: Train loss: 1.6294, Valid loss: 1.6900, L1 loss: 1.7226\n",
            "Epoch [98/30000]: Train loss: 1.6226, Valid loss: 1.6816, L1 loss: 1.7142\n",
            "Epoch [99/30000]: Train loss: 1.6238, Valid loss: 1.7073, L1 loss: 1.7329\n",
            "Epoch [100/30000]: Train loss: 1.6348, Valid loss: 1.6916, L1 loss: 1.7233\n",
            "Epoch [101/30000]: Train loss: 1.6335, Valid loss: 1.6744, L1 loss: 1.7089\n",
            "Epoch [102/30000]: Train loss: 1.6120, Valid loss: 1.6857, L1 loss: 1.7206\n",
            "Epoch [103/30000]: Train loss: 1.6212, Valid loss: 1.7140, L1 loss: 1.7458\n",
            "Epoch [104/30000]: Train loss: 1.6199, Valid loss: 1.6903, L1 loss: 1.7238\n",
            "Epoch [105/30000]: Train loss: 1.6225, Valid loss: 1.6594, L1 loss: 1.6947\n",
            "Epoch [106/30000]: Train loss: 1.6182, Valid loss: 1.6647, L1 loss: 1.7076\n",
            "Epoch [107/30000]: Train loss: 1.6263, Valid loss: 1.6709, L1 loss: 1.7066\n",
            "Epoch [108/30000]: Train loss: 1.6180, Valid loss: 1.7019, L1 loss: 1.7284\n",
            "Epoch [109/30000]: Train loss: 1.6237, Valid loss: 1.7098, L1 loss: 1.7329\n",
            "Epoch [110/30000]: Train loss: 1.6237, Valid loss: 1.7427, L1 loss: 1.7655\n",
            "Epoch [111/30000]: Train loss: 1.6118, Valid loss: 1.7200, L1 loss: 1.7479\n",
            "Epoch [112/30000]: Train loss: 1.6167, Valid loss: 1.7058, L1 loss: 1.7396\n",
            "Epoch [113/30000]: Train loss: 1.6184, Valid loss: 1.6754, L1 loss: 1.7160\n",
            "Epoch [114/30000]: Train loss: 1.6240, Valid loss: 1.6792, L1 loss: 1.7194\n",
            "Epoch [115/30000]: Train loss: 1.6229, Valid loss: 1.6633, L1 loss: 1.7053\n",
            "Epoch [116/30000]: Train loss: 1.6088, Valid loss: 1.6956, L1 loss: 1.7214\n",
            "Epoch [117/30000]: Train loss: 1.6049, Valid loss: 1.6836, L1 loss: 1.7181\n",
            "Epoch [118/30000]: Train loss: 1.6371, Valid loss: 1.6933, L1 loss: 1.7309\n",
            "Epoch [119/30000]: Train loss: 1.6192, Valid loss: 1.6872, L1 loss: 1.7201\n",
            "Epoch [120/30000]: Train loss: 1.6363, Valid loss: 1.7184, L1 loss: 1.7446\n",
            "Epoch [121/30000]: Train loss: 1.6282, Valid loss: 1.6809, L1 loss: 1.7166\n",
            "Epoch [122/30000]: Train loss: 1.6160, Valid loss: 1.7015, L1 loss: 1.7320\n",
            "Epoch [123/30000]: Train loss: 1.6148, Valid loss: 1.6848, L1 loss: 1.7245\n",
            "Epoch [124/30000]: Train loss: 1.6058, Valid loss: 1.6932, L1 loss: 1.7298\n",
            "Epoch [125/30000]: Train loss: 1.6146, Valid loss: 1.6944, L1 loss: 1.7301\n",
            "Epoch [126/30000]: Train loss: 1.6106, Valid loss: 1.7038, L1 loss: 1.7369\n",
            "Epoch [127/30000]: Train loss: 1.6166, Valid loss: 1.7050, L1 loss: 1.7374\n",
            "Epoch [128/30000]: Train loss: 1.6212, Valid loss: 1.6996, L1 loss: 1.7287\n",
            "Epoch [129/30000]: Train loss: 1.6190, Valid loss: 1.7039, L1 loss: 1.7319\n",
            "Epoch [130/30000]: Train loss: 1.6089, Valid loss: 1.6907, L1 loss: 1.7241\n",
            "Epoch [131/30000]: Train loss: 1.6130, Valid loss: 1.6917, L1 loss: 1.7248\n",
            "Epoch [132/30000]: Train loss: 1.6115, Valid loss: 1.7012, L1 loss: 1.7313\n",
            "Epoch [133/30000]: Train loss: 1.6098, Valid loss: 1.7130, L1 loss: 1.7382\n",
            "Epoch [134/30000]: Train loss: 1.6071, Valid loss: 1.6842, L1 loss: 1.7187\n",
            "Epoch [135/30000]: Train loss: 1.6179, Valid loss: 1.7061, L1 loss: 1.7359\n",
            "Epoch [136/30000]: Train loss: 1.6084, Valid loss: 1.7136, L1 loss: 1.7385\n",
            "Epoch [137/30000]: Train loss: 1.6033, Valid loss: 1.6953, L1 loss: 1.7340\n",
            "Epoch [138/30000]: Train loss: 1.6075, Valid loss: 1.6927, L1 loss: 1.7282\n",
            "Epoch [139/30000]: Train loss: 1.6127, Valid loss: 1.7060, L1 loss: 1.7401\n",
            "Epoch [140/30000]: Train loss: 1.5990, Valid loss: 1.7033, L1 loss: 1.7347\n",
            "Epoch [141/30000]: Train loss: 1.6160, Valid loss: 1.7003, L1 loss: 1.7369\n",
            "Epoch [142/30000]: Train loss: 1.6114, Valid loss: 1.6933, L1 loss: 1.7282\n",
            "Epoch [143/30000]: Train loss: 1.6125, Valid loss: 1.6776, L1 loss: 1.7169\n",
            "Epoch [144/30000]: Train loss: 1.6157, Valid loss: 1.6820, L1 loss: 1.7205\n",
            "Epoch [145/30000]: Train loss: 1.6047, Valid loss: 1.6847, L1 loss: 1.7213\n",
            "Epoch [146/30000]: Train loss: 1.6088, Valid loss: 1.6867, L1 loss: 1.7184\n",
            "Epoch [147/30000]: Train loss: 1.6210, Valid loss: 1.6793, L1 loss: 1.7210\n",
            "Epoch [148/30000]: Train loss: 1.6054, Valid loss: 1.6792, L1 loss: 1.7147\n",
            "Epoch [149/30000]: Train loss: 1.6138, Valid loss: 1.6855, L1 loss: 1.7285\n",
            "Epoch [150/30000]: Train loss: 1.6055, Valid loss: 1.7035, L1 loss: 1.7442\n",
            "Epoch [151/30000]: Train loss: 1.6096, Valid loss: 1.6778, L1 loss: 1.7171\n",
            "Epoch [152/30000]: Train loss: 1.6110, Valid loss: 1.6899, L1 loss: 1.7285\n",
            "Epoch [153/30000]: Train loss: 1.6136, Valid loss: 1.6777, L1 loss: 1.7178\n",
            "Epoch [154/30000]: Train loss: 1.6077, Valid loss: 1.6706, L1 loss: 1.7035\n",
            "Epoch [155/30000]: Train loss: 1.6092, Valid loss: 1.6877, L1 loss: 1.7251\n",
            "Epoch [156/30000]: Train loss: 1.6024, Valid loss: 1.6839, L1 loss: 1.7219\n",
            "Epoch [157/30000]: Train loss: 1.6041, Valid loss: 1.6776, L1 loss: 1.7140\n",
            "Epoch [158/30000]: Train loss: 1.6013, Valid loss: 1.6777, L1 loss: 1.7131\n",
            "Epoch [159/30000]: Train loss: 1.6101, Valid loss: 1.6846, L1 loss: 1.7222\n",
            "Epoch [160/30000]: Train loss: 1.6019, Valid loss: 1.6655, L1 loss: 1.7045\n",
            "Epoch [161/30000]: Train loss: 1.6029, Valid loss: 1.6869, L1 loss: 1.7195\n",
            "Epoch [162/30000]: Train loss: 1.6084, Valid loss: 1.6984, L1 loss: 1.7347\n",
            "Epoch [163/30000]: Train loss: 1.6015, Valid loss: 1.6911, L1 loss: 1.7234\n",
            "Epoch [164/30000]: Train loss: 1.6022, Valid loss: 1.6749, L1 loss: 1.7175\n",
            "Epoch [165/30000]: Train loss: 1.6081, Valid loss: 1.6789, L1 loss: 1.7193\n",
            "Epoch [166/30000]: Train loss: 1.5988, Valid loss: 1.7027, L1 loss: 1.7369\n",
            "Epoch [167/30000]: Train loss: 1.6023, Valid loss: 1.6969, L1 loss: 1.7330\n",
            "Epoch [168/30000]: Train loss: 1.5995, Valid loss: 1.6885, L1 loss: 1.7261\n",
            "Epoch [169/30000]: Train loss: 1.6138, Valid loss: 1.7215, L1 loss: 1.7494\n",
            "Epoch [170/30000]: Train loss: 1.6004, Valid loss: 1.6934, L1 loss: 1.7293\n",
            "Epoch [171/30000]: Train loss: 1.6062, Valid loss: 1.7121, L1 loss: 1.7448\n",
            "Epoch [172/30000]: Train loss: 1.5912, Valid loss: 1.6914, L1 loss: 1.7294\n",
            "Epoch [173/30000]: Train loss: 1.5935, Valid loss: 1.6999, L1 loss: 1.7309\n",
            "Epoch [174/30000]: Train loss: 1.5953, Valid loss: 1.7111, L1 loss: 1.7423\n",
            "Epoch [175/30000]: Train loss: 1.5997, Valid loss: 1.6928, L1 loss: 1.7252\n",
            "Epoch [176/30000]: Train loss: 1.6011, Valid loss: 1.6945, L1 loss: 1.7277\n",
            "Epoch [177/30000]: Train loss: 1.6109, Valid loss: 1.7133, L1 loss: 1.7480\n",
            "Epoch [178/30000]: Train loss: 1.5884, Valid loss: 1.6914, L1 loss: 1.7249\n",
            "Epoch [179/30000]: Train loss: 1.6061, Valid loss: 1.7286, L1 loss: 1.7585\n",
            "Epoch [180/30000]: Train loss: 1.6036, Valid loss: 1.7337, L1 loss: 1.7535\n",
            "Epoch [181/30000]: Train loss: 1.5864, Valid loss: 1.7108, L1 loss: 1.7448\n",
            "Epoch [182/30000]: Train loss: 1.6085, Valid loss: 1.7131, L1 loss: 1.7464\n",
            "Epoch [183/30000]: Train loss: 1.5968, Valid loss: 1.7182, L1 loss: 1.7459\n",
            "Epoch [184/30000]: Train loss: 1.5897, Valid loss: 1.6948, L1 loss: 1.7284\n",
            "Epoch [185/30000]: Train loss: 1.5932, Valid loss: 1.7071, L1 loss: 1.7445\n",
            "Epoch [186/30000]: Train loss: 1.6193, Valid loss: 1.7055, L1 loss: 1.7363\n",
            "Epoch [187/30000]: Train loss: 1.5829, Valid loss: 1.7036, L1 loss: 1.7330\n",
            "Epoch [188/30000]: Train loss: 1.6001, Valid loss: 1.6795, L1 loss: 1.7169\n",
            "Epoch [189/30000]: Train loss: 1.5995, Valid loss: 1.6930, L1 loss: 1.7292\n",
            "Epoch [190/30000]: Train loss: 1.5887, Valid loss: 1.7220, L1 loss: 1.7560\n",
            "Epoch [191/30000]: Train loss: 1.5883, Valid loss: 1.6944, L1 loss: 1.7294\n",
            "Epoch [192/30000]: Train loss: 1.5929, Valid loss: 1.7411, L1 loss: 1.7676\n",
            "Epoch [193/30000]: Train loss: 1.6051, Valid loss: 1.7169, L1 loss: 1.7479\n",
            "Epoch [194/30000]: Train loss: 1.5890, Valid loss: 1.7283, L1 loss: 1.7599\n",
            "Epoch [195/30000]: Train loss: 1.5866, Valid loss: 1.7154, L1 loss: 1.7432\n",
            "Epoch [196/30000]: Train loss: 1.5929, Valid loss: 1.6786, L1 loss: 1.7191\n",
            "Epoch [197/30000]: Train loss: 1.5997, Valid loss: 1.7272, L1 loss: 1.7550\n",
            "Epoch [198/30000]: Train loss: 1.5825, Valid loss: 1.7142, L1 loss: 1.7456\n",
            "Epoch [199/30000]: Train loss: 1.5919, Valid loss: 1.7240, L1 loss: 1.7509\n",
            "Epoch [200/30000]: Train loss: 1.6062, Valid loss: 1.6770, L1 loss: 1.7126\n",
            "Epoch [201/30000]: Train loss: 1.5956, Valid loss: 1.6890, L1 loss: 1.7214\n",
            "Epoch [202/30000]: Train loss: 1.5831, Valid loss: 1.7091, L1 loss: 1.7357\n",
            "Epoch [203/30000]: Train loss: 1.5955, Valid loss: 1.6823, L1 loss: 1.7151\n",
            "Epoch [204/30000]: Train loss: 1.5857, Valid loss: 1.6999, L1 loss: 1.7318\n",
            "Epoch [205/30000]: Train loss: 1.5981, Valid loss: 1.6964, L1 loss: 1.7326\n",
            "Epoch [206/30000]: Train loss: 1.5901, Valid loss: 1.7138, L1 loss: 1.7405\n",
            "Epoch [207/30000]: Train loss: 1.5845, Valid loss: 1.7113, L1 loss: 1.7420\n",
            "Epoch [208/30000]: Train loss: 1.5994, Valid loss: 1.6815, L1 loss: 1.7173\n",
            "Epoch [209/30000]: Train loss: 1.5872, Valid loss: 1.6821, L1 loss: 1.7225\n",
            "Epoch [210/30000]: Train loss: 1.5978, Valid loss: 1.6880, L1 loss: 1.7235\n",
            "Epoch [211/30000]: Train loss: 1.5950, Valid loss: 1.6852, L1 loss: 1.7223\n",
            "Epoch [212/30000]: Train loss: 1.5917, Valid loss: 1.6757, L1 loss: 1.7138\n",
            "Epoch [213/30000]: Train loss: 1.5769, Valid loss: 1.6953, L1 loss: 1.7246\n",
            "Epoch [214/30000]: Train loss: 1.6008, Valid loss: 1.7151, L1 loss: 1.7490\n",
            "Epoch [215/30000]: Train loss: 1.5966, Valid loss: 1.7092, L1 loss: 1.7389\n",
            "Epoch [216/30000]: Train loss: 1.5909, Valid loss: 1.6990, L1 loss: 1.7337\n",
            "Epoch [217/30000]: Train loss: 1.5860, Valid loss: 1.7259, L1 loss: 1.7543\n",
            "Epoch [218/30000]: Train loss: 1.5977, Valid loss: 1.6850, L1 loss: 1.7201\n",
            "Epoch [219/30000]: Train loss: 1.5784, Valid loss: 1.7099, L1 loss: 1.7478\n",
            "Epoch [220/30000]: Train loss: 1.5900, Valid loss: 1.7135, L1 loss: 1.7432\n",
            "Epoch [221/30000]: Train loss: 1.6021, Valid loss: 1.7093, L1 loss: 1.7382\n",
            "Epoch [222/30000]: Train loss: 1.5858, Valid loss: 1.7050, L1 loss: 1.7361\n",
            "Epoch [223/30000]: Train loss: 1.5852, Valid loss: 1.7046, L1 loss: 1.7380\n",
            "Epoch [224/30000]: Train loss: 1.5907, Valid loss: 1.7170, L1 loss: 1.7455\n",
            "Epoch [225/30000]: Train loss: 1.5934, Valid loss: 1.7039, L1 loss: 1.7366\n",
            "Epoch [226/30000]: Train loss: 1.5802, Valid loss: 1.7058, L1 loss: 1.7369\n",
            "Epoch [227/30000]: Train loss: 1.5850, Valid loss: 1.7007, L1 loss: 1.7321\n",
            "Epoch [228/30000]: Train loss: 1.5969, Valid loss: 1.6990, L1 loss: 1.7365\n",
            "Epoch [229/30000]: Train loss: 1.5958, Valid loss: 1.7198, L1 loss: 1.7495\n",
            "Epoch [230/30000]: Train loss: 1.5857, Valid loss: 1.6825, L1 loss: 1.7212\n",
            "Epoch [231/30000]: Train loss: 1.5977, Valid loss: 1.6764, L1 loss: 1.7184\n",
            "Epoch [232/30000]: Train loss: 1.5727, Valid loss: 1.7153, L1 loss: 1.7455\n",
            "Epoch [233/30000]: Train loss: 1.5927, Valid loss: 1.6982, L1 loss: 1.7312\n",
            "Epoch [234/30000]: Train loss: 1.5851, Valid loss: 1.6916, L1 loss: 1.7276\n",
            "Epoch [235/30000]: Train loss: 1.5895, Valid loss: 1.7164, L1 loss: 1.7476\n",
            "Epoch [236/30000]: Train loss: 1.5887, Valid loss: 1.7064, L1 loss: 1.7347\n",
            "Epoch [237/30000]: Train loss: 1.5848, Valid loss: 1.7132, L1 loss: 1.7480\n",
            "Epoch [238/30000]: Train loss: 1.5792, Valid loss: 1.7033, L1 loss: 1.7409\n",
            "Epoch [239/30000]: Train loss: 1.5831, Valid loss: 1.6933, L1 loss: 1.7268\n",
            "Epoch [240/30000]: Train loss: 1.5906, Valid loss: 1.7293, L1 loss: 1.7586\n",
            "Epoch [241/30000]: Train loss: 1.5861, Valid loss: 1.7031, L1 loss: 1.7431\n",
            "Epoch [242/30000]: Train loss: 1.5914, Valid loss: 1.7222, L1 loss: 1.7557\n",
            "Epoch [243/30000]: Train loss: 1.5918, Valid loss: 1.7189, L1 loss: 1.7491\n",
            "Epoch [244/30000]: Train loss: 1.5853, Valid loss: 1.6945, L1 loss: 1.7338\n",
            "Epoch [245/30000]: Train loss: 1.5787, Valid loss: 1.7002, L1 loss: 1.7397\n",
            "Epoch [246/30000]: Train loss: 1.5889, Valid loss: 1.7181, L1 loss: 1.7494\n",
            "Epoch [247/30000]: Train loss: 1.5957, Valid loss: 1.6802, L1 loss: 1.7185\n",
            "Epoch [248/30000]: Train loss: 1.5791, Valid loss: 1.6975, L1 loss: 1.7319\n",
            "Epoch [249/30000]: Train loss: 1.5831, Valid loss: 1.6904, L1 loss: 1.7333\n",
            "Epoch [250/30000]: Train loss: 1.6027, Valid loss: 1.7132, L1 loss: 1.7431\n",
            "Epoch [251/30000]: Train loss: 1.5742, Valid loss: 1.7075, L1 loss: 1.7473\n",
            "Epoch [252/30000]: Train loss: 1.5801, Valid loss: 1.6986, L1 loss: 1.7383\n",
            "Epoch [253/30000]: Train loss: 1.5891, Valid loss: 1.7049, L1 loss: 1.7417\n",
            "Epoch [254/30000]: Train loss: 1.5834, Valid loss: 1.7157, L1 loss: 1.7538\n",
            "Epoch [255/30000]: Train loss: 1.5824, Valid loss: 1.7028, L1 loss: 1.7440\n",
            "Epoch [256/30000]: Train loss: 1.5765, Valid loss: 1.6830, L1 loss: 1.7249\n",
            "Epoch [257/30000]: Train loss: 1.5815, Valid loss: 1.6975, L1 loss: 1.7346\n",
            "Epoch [258/30000]: Train loss: 1.5757, Valid loss: 1.7185, L1 loss: 1.7520\n",
            "Epoch [259/30000]: Train loss: 1.5786, Valid loss: 1.6821, L1 loss: 1.7193\n",
            "Epoch [260/30000]: Train loss: 1.5806, Valid loss: 1.7058, L1 loss: 1.7386\n",
            "Epoch [261/30000]: Train loss: 1.5914, Valid loss: 1.7133, L1 loss: 1.7513\n",
            "Epoch [262/30000]: Train loss: 1.5905, Valid loss: 1.7153, L1 loss: 1.7539\n",
            "Epoch [263/30000]: Train loss: 1.5984, Valid loss: 1.7033, L1 loss: 1.7459\n",
            "Epoch [264/30000]: Train loss: 1.5922, Valid loss: 1.7108, L1 loss: 1.7463\n",
            "Epoch [265/30000]: Train loss: 1.5794, Valid loss: 1.7169, L1 loss: 1.7566\n",
            "Epoch [266/30000]: Train loss: 1.5799, Valid loss: 1.7069, L1 loss: 1.7459\n",
            "Epoch [267/30000]: Train loss: 1.5897, Valid loss: 1.6824, L1 loss: 1.7274\n",
            "Epoch [268/30000]: Train loss: 1.5899, Valid loss: 1.7087, L1 loss: 1.7413\n",
            "Epoch [269/30000]: Train loss: 1.5758, Valid loss: 1.7062, L1 loss: 1.7438\n",
            "Epoch [270/30000]: Train loss: 1.5814, Valid loss: 1.7108, L1 loss: 1.7494\n",
            "Epoch [271/30000]: Train loss: 1.5860, Valid loss: 1.7165, L1 loss: 1.7534\n",
            "Epoch [272/30000]: Train loss: 1.5807, Valid loss: 1.7104, L1 loss: 1.7451\n",
            "Epoch [273/30000]: Train loss: 1.5850, Valid loss: 1.6984, L1 loss: 1.7357\n",
            "Epoch [274/30000]: Train loss: 1.5796, Valid loss: 1.7097, L1 loss: 1.7497\n",
            "Epoch [275/30000]: Train loss: 1.5714, Valid loss: 1.6988, L1 loss: 1.7372\n",
            "Epoch [276/30000]: Train loss: 1.5791, Valid loss: 1.7017, L1 loss: 1.7401\n",
            "Epoch [277/30000]: Train loss: 1.5790, Valid loss: 1.7346, L1 loss: 1.7658\n",
            "Epoch [278/30000]: Train loss: 1.5829, Valid loss: 1.6976, L1 loss: 1.7404\n",
            "Epoch [279/30000]: Train loss: 1.5841, Valid loss: 1.7106, L1 loss: 1.7473\n",
            "Epoch [280/30000]: Train loss: 1.5774, Valid loss: 1.7232, L1 loss: 1.7570\n",
            "Epoch [281/30000]: Train loss: 1.5778, Valid loss: 1.7041, L1 loss: 1.7395\n",
            "Epoch [282/30000]: Train loss: 1.5740, Valid loss: 1.7382, L1 loss: 1.7666\n",
            "Epoch [283/30000]: Train loss: 1.5854, Valid loss: 1.7162, L1 loss: 1.7501\n",
            "Epoch [284/30000]: Train loss: 1.5749, Valid loss: 1.7164, L1 loss: 1.7482\n",
            "Epoch [285/30000]: Train loss: 1.5913, Valid loss: 1.7159, L1 loss: 1.7490\n",
            "Epoch [286/30000]: Train loss: 1.5792, Valid loss: 1.7316, L1 loss: 1.7568\n",
            "Epoch [287/30000]: Train loss: 1.5779, Valid loss: 1.7165, L1 loss: 1.7468\n",
            "Epoch [288/30000]: Train loss: 1.5836, Valid loss: 1.7041, L1 loss: 1.7405\n",
            "Epoch [289/30000]: Train loss: 1.5905, Valid loss: 1.6993, L1 loss: 1.7377\n",
            "Epoch [290/30000]: Train loss: 1.5880, Valid loss: 1.7302, L1 loss: 1.7642\n",
            "Epoch [291/30000]: Train loss: 1.5859, Valid loss: 1.6992, L1 loss: 1.7407\n",
            "Epoch [292/30000]: Train loss: 1.5987, Valid loss: 1.6937, L1 loss: 1.7352\n",
            "Epoch [293/30000]: Train loss: 1.5764, Valid loss: 1.6825, L1 loss: 1.7271\n",
            "Epoch [294/30000]: Train loss: 1.5830, Valid loss: 1.6845, L1 loss: 1.7262\n",
            "Epoch [295/30000]: Train loss: 1.5741, Valid loss: 1.7097, L1 loss: 1.7458\n",
            "Epoch [296/30000]: Train loss: 1.5757, Valid loss: 1.6985, L1 loss: 1.7377\n",
            "Epoch [297/30000]: Train loss: 1.5848, Valid loss: 1.7134, L1 loss: 1.7534\n",
            "Epoch [298/30000]: Train loss: 1.5700, Valid loss: 1.7207, L1 loss: 1.7587\n",
            "Epoch [299/30000]: Train loss: 1.5860, Valid loss: 1.7286, L1 loss: 1.7650\n",
            "Epoch [300/30000]: Train loss: 1.5742, Valid loss: 1.7172, L1 loss: 1.7505\n",
            "Epoch [301/30000]: Train loss: 1.5854, Valid loss: 1.7274, L1 loss: 1.7695\n",
            "Epoch [302/30000]: Train loss: 1.5764, Valid loss: 1.7372, L1 loss: 1.7736\n",
            "Epoch [303/30000]: Train loss: 1.5748, Valid loss: 1.7135, L1 loss: 1.7504\n",
            "Epoch [304/30000]: Train loss: 1.5758, Valid loss: 1.6948, L1 loss: 1.7343\n",
            "Epoch [305/30000]: Train loss: 1.5761, Valid loss: 1.7105, L1 loss: 1.7468\n",
            "Epoch [306/30000]: Train loss: 1.5829, Valid loss: 1.7023, L1 loss: 1.7410\n",
            "Epoch [307/30000]: Train loss: 1.5727, Valid loss: 1.6946, L1 loss: 1.7343\n",
            "Epoch [308/30000]: Train loss: 1.5891, Valid loss: 1.7153, L1 loss: 1.7512\n",
            "Epoch [309/30000]: Train loss: 1.5774, Valid loss: 1.6940, L1 loss: 1.7364\n",
            "Epoch [310/30000]: Train loss: 1.5748, Valid loss: 1.7017, L1 loss: 1.7441\n",
            "Epoch [311/30000]: Train loss: 1.5823, Valid loss: 1.6900, L1 loss: 1.7374\n",
            "Epoch [312/30000]: Train loss: 1.5675, Valid loss: 1.6913, L1 loss: 1.7318\n",
            "Epoch [313/30000]: Train loss: 1.5706, Valid loss: 1.6969, L1 loss: 1.7339\n",
            "Epoch [314/30000]: Train loss: 1.5678, Valid loss: 1.6863, L1 loss: 1.7267\n",
            "Epoch [315/30000]: Train loss: 1.5836, Valid loss: 1.7001, L1 loss: 1.7390\n",
            "Epoch [316/30000]: Train loss: 1.5828, Valid loss: 1.6988, L1 loss: 1.7414\n",
            "Epoch [317/30000]: Train loss: 1.5711, Valid loss: 1.7012, L1 loss: 1.7383\n",
            "Epoch [318/30000]: Train loss: 1.5807, Valid loss: 1.7045, L1 loss: 1.7432\n",
            "Epoch [319/30000]: Train loss: 1.5815, Valid loss: 1.7106, L1 loss: 1.7489\n",
            "Epoch [320/30000]: Train loss: 1.5758, Valid loss: 1.6950, L1 loss: 1.7337\n",
            "Epoch [321/30000]: Train loss: 1.5656, Valid loss: 1.7053, L1 loss: 1.7431\n",
            "Epoch [322/30000]: Train loss: 1.5818, Valid loss: 1.7020, L1 loss: 1.7415\n",
            "Epoch [323/30000]: Train loss: 1.5608, Valid loss: 1.7197, L1 loss: 1.7527\n",
            "Epoch [324/30000]: Train loss: 1.5821, Valid loss: 1.6994, L1 loss: 1.7322\n",
            "Epoch [325/30000]: Train loss: 1.5739, Valid loss: 1.6813, L1 loss: 1.7239\n",
            "Epoch [326/30000]: Train loss: 1.5787, Valid loss: 1.6855, L1 loss: 1.7192\n",
            "Epoch [327/30000]: Train loss: 1.5816, Valid loss: 1.7193, L1 loss: 1.7526\n",
            "Epoch [328/30000]: Train loss: 1.5774, Valid loss: 1.7172, L1 loss: 1.7521\n",
            "Epoch [329/30000]: Train loss: 1.5774, Valid loss: 1.6963, L1 loss: 1.7352\n",
            "Epoch [330/30000]: Train loss: 1.5799, Valid loss: 1.7166, L1 loss: 1.7482\n",
            "Epoch [331/30000]: Train loss: 1.5693, Valid loss: 1.7107, L1 loss: 1.7487\n",
            "Epoch [332/30000]: Train loss: 1.5866, Valid loss: 1.7134, L1 loss: 1.7477\n",
            "Epoch [333/30000]: Train loss: 1.5673, Valid loss: 1.7111, L1 loss: 1.7481\n",
            "Epoch [334/30000]: Train loss: 1.5824, Valid loss: 1.7020, L1 loss: 1.7389\n",
            "Epoch [335/30000]: Train loss: 1.5660, Valid loss: 1.7042, L1 loss: 1.7370\n",
            "Epoch [336/30000]: Train loss: 1.5686, Valid loss: 1.7281, L1 loss: 1.7650\n",
            "Epoch [337/30000]: Train loss: 1.5770, Valid loss: 1.6979, L1 loss: 1.7351\n",
            "Epoch [338/30000]: Train loss: 1.5726, Valid loss: 1.7012, L1 loss: 1.7396\n",
            "Epoch [339/30000]: Train loss: 1.5728, Valid loss: 1.7085, L1 loss: 1.7456\n",
            "Epoch [340/30000]: Train loss: 1.5709, Valid loss: 1.7198, L1 loss: 1.7493\n",
            "Epoch [341/30000]: Train loss: 1.5809, Valid loss: 1.7293, L1 loss: 1.7679\n",
            "Epoch [342/30000]: Train loss: 1.5804, Valid loss: 1.6894, L1 loss: 1.7287\n",
            "Epoch [343/30000]: Train loss: 1.5678, Valid loss: 1.7014, L1 loss: 1.7384\n",
            "Epoch [344/30000]: Train loss: 1.5799, Valid loss: 1.6967, L1 loss: 1.7346\n",
            "Epoch [345/30000]: Train loss: 1.5783, Valid loss: 1.7147, L1 loss: 1.7486\n",
            "Epoch [346/30000]: Train loss: 1.5771, Valid loss: 1.6851, L1 loss: 1.7230\n",
            "Epoch [347/30000]: Train loss: 1.5806, Valid loss: 1.7122, L1 loss: 1.7485\n",
            "Epoch [348/30000]: Train loss: 1.5837, Valid loss: 1.6956, L1 loss: 1.7305\n",
            "Epoch [349/30000]: Train loss: 1.5712, Valid loss: 1.7099, L1 loss: 1.7446\n",
            "Epoch [350/30000]: Train loss: 1.5676, Valid loss: 1.7260, L1 loss: 1.7570\n",
            "Epoch [351/30000]: Train loss: 1.5769, Valid loss: 1.7210, L1 loss: 1.7545\n",
            "Epoch [352/30000]: Train loss: 1.5810, Valid loss: 1.7236, L1 loss: 1.7545\n",
            "Epoch [353/30000]: Train loss: 1.5808, Valid loss: 1.7174, L1 loss: 1.7502\n",
            "Epoch [354/30000]: Train loss: 1.5700, Valid loss: 1.7137, L1 loss: 1.7464\n",
            "Epoch [355/30000]: Train loss: 1.5756, Valid loss: 1.7001, L1 loss: 1.7300\n",
            "Epoch [356/30000]: Train loss: 1.5961, Valid loss: 1.7147, L1 loss: 1.7491\n",
            "Epoch [357/30000]: Train loss: 1.5691, Valid loss: 1.7009, L1 loss: 1.7384\n",
            "Epoch [358/30000]: Train loss: 1.5806, Valid loss: 1.7122, L1 loss: 1.7472\n",
            "Epoch [359/30000]: Train loss: 1.5854, Valid loss: 1.6995, L1 loss: 1.7378\n",
            "Epoch [360/30000]: Train loss: 1.5704, Valid loss: 1.7064, L1 loss: 1.7444\n",
            "Epoch [361/30000]: Train loss: 1.5843, Valid loss: 1.7231, L1 loss: 1.7586\n",
            "Epoch [362/30000]: Train loss: 1.5802, Valid loss: 1.7028, L1 loss: 1.7440\n",
            "Epoch [363/30000]: Train loss: 1.5815, Valid loss: 1.7018, L1 loss: 1.7409\n",
            "Epoch [364/30000]: Train loss: 1.5609, Valid loss: 1.7145, L1 loss: 1.7586\n",
            "Epoch [365/30000]: Train loss: 1.5684, Valid loss: 1.6993, L1 loss: 1.7421\n",
            "Epoch [366/30000]: Train loss: 1.5739, Valid loss: 1.7066, L1 loss: 1.7463\n",
            "Epoch [367/30000]: Train loss: 1.5768, Valid loss: 1.7139, L1 loss: 1.7505\n",
            "Epoch [368/30000]: Train loss: 1.5687, Valid loss: 1.6899, L1 loss: 1.7370\n",
            "Epoch [369/30000]: Train loss: 1.5689, Valid loss: 1.6888, L1 loss: 1.7306\n",
            "Epoch [370/30000]: Train loss: 1.5780, Valid loss: 1.6907, L1 loss: 1.7310\n",
            "Epoch [371/30000]: Train loss: 1.5663, Valid loss: 1.6919, L1 loss: 1.7301\n",
            "Epoch [372/30000]: Train loss: 1.5768, Valid loss: 1.7034, L1 loss: 1.7378\n",
            "Epoch [373/30000]: Train loss: 1.5775, Valid loss: 1.7015, L1 loss: 1.7448\n",
            "Epoch [374/30000]: Train loss: 1.5720, Valid loss: 1.7050, L1 loss: 1.7521\n",
            "Epoch [375/30000]: Train loss: 1.5762, Valid loss: 1.7136, L1 loss: 1.7578\n",
            "Epoch [376/30000]: Train loss: 1.5735, Valid loss: 1.7191, L1 loss: 1.7639\n",
            "Epoch [377/30000]: Train loss: 1.5722, Valid loss: 1.7024, L1 loss: 1.7494\n",
            "Epoch [378/30000]: Train loss: 1.5647, Valid loss: 1.7151, L1 loss: 1.7563\n",
            "Epoch [379/30000]: Train loss: 1.5764, Valid loss: 1.7110, L1 loss: 1.7503\n",
            "Epoch [380/30000]: Train loss: 1.5700, Valid loss: 1.7159, L1 loss: 1.7539\n",
            "Epoch [381/30000]: Train loss: 1.5780, Valid loss: 1.6957, L1 loss: 1.7396\n",
            "Epoch [382/30000]: Train loss: 1.5671, Valid loss: 1.7089, L1 loss: 1.7494\n",
            "Epoch [383/30000]: Train loss: 1.5698, Valid loss: 1.7045, L1 loss: 1.7458\n",
            "Epoch [384/30000]: Train loss: 1.5743, Valid loss: 1.7100, L1 loss: 1.7479\n",
            "Epoch [385/30000]: Train loss: 1.5631, Valid loss: 1.7120, L1 loss: 1.7532\n",
            "Epoch [386/30000]: Train loss: 1.5795, Valid loss: 1.7065, L1 loss: 1.7496\n",
            "Epoch [387/30000]: Train loss: 1.5851, Valid loss: 1.7205, L1 loss: 1.7633\n",
            "Epoch [388/30000]: Train loss: 1.5720, Valid loss: 1.7163, L1 loss: 1.7569\n",
            "Epoch [389/30000]: Train loss: 1.5692, Valid loss: 1.7223, L1 loss: 1.7590\n",
            "Epoch [390/30000]: Train loss: 1.5748, Valid loss: 1.7110, L1 loss: 1.7539\n",
            "Epoch [391/30000]: Train loss: 1.5712, Valid loss: 1.7089, L1 loss: 1.7518\n",
            "Epoch [392/30000]: Train loss: 1.5805, Valid loss: 1.7176, L1 loss: 1.7529\n",
            "Epoch [393/30000]: Train loss: 1.5803, Valid loss: 1.7270, L1 loss: 1.7695\n",
            "Epoch [394/30000]: Train loss: 1.5711, Valid loss: 1.7066, L1 loss: 1.7424\n",
            "Epoch [395/30000]: Train loss: 1.5607, Valid loss: 1.7173, L1 loss: 1.7507\n",
            "Epoch [396/30000]: Train loss: 1.5735, Valid loss: 1.7134, L1 loss: 1.7540\n",
            "Epoch [397/30000]: Train loss: 1.5869, Valid loss: 1.7235, L1 loss: 1.7556\n",
            "Epoch [398/30000]: Train loss: 1.5736, Valid loss: 1.7166, L1 loss: 1.7513\n",
            "Epoch [399/30000]: Train loss: 1.5763, Valid loss: 1.7234, L1 loss: 1.7666\n",
            "Epoch [400/30000]: Train loss: 1.5564, Valid loss: 1.7186, L1 loss: 1.7579\n",
            "Epoch [401/30000]: Train loss: 1.5812, Valid loss: 1.7525, L1 loss: 1.7847\n",
            "Epoch [402/30000]: Train loss: 1.5647, Valid loss: 1.7378, L1 loss: 1.7725\n",
            "Epoch [403/30000]: Train loss: 1.5689, Valid loss: 1.7052, L1 loss: 1.7501\n",
            "Epoch [404/30000]: Train loss: 1.5728, Valid loss: 1.7126, L1 loss: 1.7496\n",
            "Epoch [405/30000]: Train loss: 1.5624, Valid loss: 1.7104, L1 loss: 1.7519\n",
            "Epoch [406/30000]: Train loss: 1.5747, Valid loss: 1.7026, L1 loss: 1.7517\n",
            "Epoch [407/30000]: Train loss: 1.5640, Valid loss: 1.7239, L1 loss: 1.7618\n",
            "Epoch [408/30000]: Train loss: 1.5645, Valid loss: 1.7152, L1 loss: 1.7589\n",
            "Epoch [409/30000]: Train loss: 1.5651, Valid loss: 1.7287, L1 loss: 1.7740\n",
            "Epoch [410/30000]: Train loss: 1.5547, Valid loss: 1.6965, L1 loss: 1.7449\n",
            "Epoch [411/30000]: Train loss: 1.5630, Valid loss: 1.7245, L1 loss: 1.7644\n",
            "Epoch [412/30000]: Train loss: 1.5728, Valid loss: 1.7223, L1 loss: 1.7649\n",
            "Epoch [413/30000]: Train loss: 1.5775, Valid loss: 1.7334, L1 loss: 1.7710\n",
            "Epoch [414/30000]: Train loss: 1.5574, Valid loss: 1.7163, L1 loss: 1.7563\n",
            "Epoch [415/30000]: Train loss: 1.5887, Valid loss: 1.7161, L1 loss: 1.7644\n",
            "Epoch [416/30000]: Train loss: 1.5705, Valid loss: 1.7113, L1 loss: 1.7514\n",
            "Epoch [417/30000]: Train loss: 1.5778, Valid loss: 1.7212, L1 loss: 1.7576\n",
            "Epoch [418/30000]: Train loss: 1.5740, Valid loss: 1.7049, L1 loss: 1.7541\n",
            "Epoch [419/30000]: Train loss: 1.5746, Valid loss: 1.7384, L1 loss: 1.7811\n",
            "Epoch [420/30000]: Train loss: 1.5679, Valid loss: 1.7355, L1 loss: 1.7766\n",
            "Epoch [421/30000]: Train loss: 1.5714, Valid loss: 1.7175, L1 loss: 1.7587\n",
            "Epoch [422/30000]: Train loss: 1.5731, Valid loss: 1.6947, L1 loss: 1.7426\n",
            "Epoch [423/30000]: Train loss: 1.5678, Valid loss: 1.7221, L1 loss: 1.7595\n",
            "Epoch [424/30000]: Train loss: 1.5677, Valid loss: 1.7044, L1 loss: 1.7525\n",
            "Epoch [425/30000]: Train loss: 1.5738, Valid loss: 1.7223, L1 loss: 1.7573\n",
            "Epoch [426/30000]: Train loss: 1.5682, Valid loss: 1.7169, L1 loss: 1.7509\n",
            "Epoch [427/30000]: Train loss: 1.5680, Valid loss: 1.7314, L1 loss: 1.7664\n",
            "Epoch [428/30000]: Train loss: 1.5697, Valid loss: 1.7433, L1 loss: 1.7771\n",
            "Epoch [429/30000]: Train loss: 1.5589, Valid loss: 1.7353, L1 loss: 1.7690\n",
            "Epoch [430/30000]: Train loss: 1.5732, Valid loss: 1.7152, L1 loss: 1.7489\n",
            "Epoch [431/30000]: Train loss: 1.5687, Valid loss: 1.7152, L1 loss: 1.7535\n",
            "Epoch [432/30000]: Train loss: 1.5656, Valid loss: 1.7276, L1 loss: 1.7683\n",
            "Epoch [433/30000]: Train loss: 1.5676, Valid loss: 1.7412, L1 loss: 1.7742\n",
            "Epoch [434/30000]: Train loss: 1.5677, Valid loss: 1.7301, L1 loss: 1.7672\n",
            "Epoch [435/30000]: Train loss: 1.5657, Valid loss: 1.7219, L1 loss: 1.7634\n",
            "Epoch [436/30000]: Train loss: 1.5674, Valid loss: 1.7076, L1 loss: 1.7512\n",
            "Epoch [437/30000]: Train loss: 1.5679, Valid loss: 1.7263, L1 loss: 1.7636\n",
            "Epoch [438/30000]: Train loss: 1.5722, Valid loss: 1.7163, L1 loss: 1.7548\n",
            "Epoch [439/30000]: Train loss: 1.5685, Valid loss: 1.7091, L1 loss: 1.7539\n",
            "Epoch [440/30000]: Train loss: 1.5641, Valid loss: 1.7151, L1 loss: 1.7536\n",
            "Epoch [441/30000]: Train loss: 1.5687, Valid loss: 1.7173, L1 loss: 1.7624\n",
            "Epoch [442/30000]: Train loss: 1.5765, Valid loss: 1.7417, L1 loss: 1.7812\n",
            "Epoch [443/30000]: Train loss: 1.5622, Valid loss: 1.7214, L1 loss: 1.7603\n",
            "Epoch [444/30000]: Train loss: 1.5616, Valid loss: 1.7291, L1 loss: 1.7627\n",
            "Epoch [445/30000]: Train loss: 1.5735, Valid loss: 1.7206, L1 loss: 1.7588\n",
            "Epoch [446/30000]: Train loss: 1.5614, Valid loss: 1.7155, L1 loss: 1.7539\n",
            "Epoch [447/30000]: Train loss: 1.5878, Valid loss: 1.7241, L1 loss: 1.7678\n",
            "Epoch [448/30000]: Train loss: 1.5681, Valid loss: 1.7245, L1 loss: 1.7639\n",
            "Epoch [449/30000]: Train loss: 1.5662, Valid loss: 1.7407, L1 loss: 1.7821\n",
            "Epoch [450/30000]: Train loss: 1.5640, Valid loss: 1.7075, L1 loss: 1.7525\n",
            "Epoch [451/30000]: Train loss: 1.5698, Valid loss: 1.7204, L1 loss: 1.7619\n",
            "Epoch [452/30000]: Train loss: 1.5759, Valid loss: 1.7063, L1 loss: 1.7570\n",
            "Epoch [453/30000]: Train loss: 1.5751, Valid loss: 1.7058, L1 loss: 1.7463\n",
            "Epoch [454/30000]: Train loss: 1.5719, Valid loss: 1.7109, L1 loss: 1.7523\n",
            "Epoch [455/30000]: Train loss: 1.5605, Valid loss: 1.7141, L1 loss: 1.7585\n",
            "Epoch [456/30000]: Train loss: 1.5739, Valid loss: 1.7454, L1 loss: 1.7883\n",
            "Epoch [457/30000]: Train loss: 1.5692, Valid loss: 1.7286, L1 loss: 1.7679\n",
            "Epoch [458/30000]: Train loss: 1.5732, Valid loss: 1.7308, L1 loss: 1.7716\n",
            "Epoch [459/30000]: Train loss: 1.5677, Valid loss: 1.7229, L1 loss: 1.7684\n",
            "Epoch [460/30000]: Train loss: 1.5691, Valid loss: 1.7433, L1 loss: 1.7861\n",
            "Epoch [461/30000]: Train loss: 1.5636, Valid loss: 1.7394, L1 loss: 1.7796\n",
            "Epoch [462/30000]: Train loss: 1.5708, Valid loss: 1.7577, L1 loss: 1.7932\n",
            "Epoch [463/30000]: Train loss: 1.5645, Valid loss: 1.7502, L1 loss: 1.7932\n",
            "Epoch [464/30000]: Train loss: 1.5661, Valid loss: 1.7239, L1 loss: 1.7694\n",
            "Epoch [465/30000]: Train loss: 1.5712, Valid loss: 1.7476, L1 loss: 1.7903\n",
            "Epoch [466/30000]: Train loss: 1.5662, Valid loss: 1.7357, L1 loss: 1.7790\n",
            "Epoch [467/30000]: Train loss: 1.5574, Valid loss: 1.7311, L1 loss: 1.7740\n",
            "Epoch [468/30000]: Train loss: 1.5754, Valid loss: 1.7221, L1 loss: 1.7683\n",
            "Epoch [469/30000]: Train loss: 1.5674, Valid loss: 1.7278, L1 loss: 1.7733\n",
            "Epoch [470/30000]: Train loss: 1.5635, Valid loss: 1.7464, L1 loss: 1.7879\n",
            "Epoch [471/30000]: Train loss: 1.5782, Valid loss: 1.7386, L1 loss: 1.7832\n",
            "Epoch [472/30000]: Train loss: 1.5803, Valid loss: 1.7291, L1 loss: 1.7765\n",
            "Epoch [473/30000]: Train loss: 1.5721, Valid loss: 1.7407, L1 loss: 1.7802\n",
            "Epoch [474/30000]: Train loss: 1.5716, Valid loss: 1.7474, L1 loss: 1.7908\n",
            "Epoch [475/30000]: Train loss: 1.5588, Valid loss: 1.7429, L1 loss: 1.7808\n",
            "Epoch [476/30000]: Train loss: 1.5698, Valid loss: 1.7316, L1 loss: 1.7755\n",
            "Epoch [477/30000]: Train loss: 1.5635, Valid loss: 1.7521, L1 loss: 1.7917\n",
            "Epoch [478/30000]: Train loss: 1.5600, Valid loss: 1.7630, L1 loss: 1.8018\n",
            "Epoch [479/30000]: Train loss: 1.5729, Valid loss: 1.7689, L1 loss: 1.8038\n",
            "Epoch [480/30000]: Train loss: 1.5624, Valid loss: 1.7205, L1 loss: 1.7631\n",
            "Epoch [481/30000]: Train loss: 1.5671, Valid loss: 1.7157, L1 loss: 1.7603\n",
            "Epoch [482/30000]: Train loss: 1.5693, Valid loss: 1.7543, L1 loss: 1.7929\n",
            "Epoch [483/30000]: Train loss: 1.5622, Valid loss: 1.7508, L1 loss: 1.7860\n",
            "Epoch [484/30000]: Train loss: 1.5783, Valid loss: 1.7239, L1 loss: 1.7657\n",
            "Epoch [485/30000]: Train loss: 1.5713, Valid loss: 1.7346, L1 loss: 1.7719\n",
            "Epoch [486/30000]: Train loss: 1.5710, Valid loss: 1.7384, L1 loss: 1.7768\n",
            "Epoch [487/30000]: Train loss: 1.5742, Valid loss: 1.7268, L1 loss: 1.7647\n",
            "Epoch [488/30000]: Train loss: 1.5546, Valid loss: 1.7573, L1 loss: 1.7902\n",
            "Epoch [489/30000]: Train loss: 1.5684, Valid loss: 1.7478, L1 loss: 1.7837\n",
            "Epoch [490/30000]: Train loss: 1.5681, Valid loss: 1.7547, L1 loss: 1.7978\n",
            "Epoch [491/30000]: Train loss: 1.5698, Valid loss: 1.7256, L1 loss: 1.7686\n",
            "Epoch [492/30000]: Train loss: 1.5606, Valid loss: 1.7708, L1 loss: 1.8080\n",
            "Epoch [493/30000]: Train loss: 1.5735, Valid loss: 1.7356, L1 loss: 1.7786\n",
            "Epoch [494/30000]: Train loss: 1.5639, Valid loss: 1.7647, L1 loss: 1.7986\n",
            "Epoch [495/30000]: Train loss: 1.5712, Valid loss: 1.7245, L1 loss: 1.7685\n",
            "Epoch [496/30000]: Train loss: 1.5770, Valid loss: 1.7569, L1 loss: 1.8044\n",
            "Epoch [497/30000]: Train loss: 1.5737, Valid loss: 1.7480, L1 loss: 1.7917\n",
            "Epoch [498/30000]: Train loss: 1.5587, Valid loss: 1.7450, L1 loss: 1.7870\n",
            "Epoch [499/30000]: Train loss: 1.5587, Valid loss: 1.7504, L1 loss: 1.7921\n",
            "Epoch [500/30000]: Train loss: 1.5654, Valid loss: 1.7325, L1 loss: 1.7756\n",
            "Epoch [501/30000]: Train loss: 1.5566, Valid loss: 1.7495, L1 loss: 1.7890\n",
            "Epoch [502/30000]: Train loss: 1.5738, Valid loss: 1.7602, L1 loss: 1.7978\n",
            "Epoch [503/30000]: Train loss: 1.5719, Valid loss: 1.7404, L1 loss: 1.7811\n",
            "Epoch [504/30000]: Train loss: 1.5569, Valid loss: 1.7517, L1 loss: 1.7949\n",
            "Epoch [505/30000]: Train loss: 1.5722, Valid loss: 1.7459, L1 loss: 1.7858\n",
            "Epoch [506/30000]: Train loss: 1.5817, Valid loss: 1.7467, L1 loss: 1.7858\n",
            "Epoch [507/30000]: Train loss: 1.5794, Valid loss: 1.7389, L1 loss: 1.7766\n",
            "Epoch [508/30000]: Train loss: 1.5596, Valid loss: 1.7644, L1 loss: 1.8026\n",
            "Epoch [509/30000]: Train loss: 1.5652, Valid loss: 1.7466, L1 loss: 1.7905\n",
            "Epoch [510/30000]: Train loss: 1.5644, Valid loss: 1.7414, L1 loss: 1.7909\n",
            "Epoch [511/30000]: Train loss: 1.5591, Valid loss: 1.7351, L1 loss: 1.7749\n",
            "Epoch [512/30000]: Train loss: 1.5597, Valid loss: 1.7583, L1 loss: 1.7979\n",
            "Epoch [513/30000]: Train loss: 1.5512, Valid loss: 1.7495, L1 loss: 1.7892\n",
            "Epoch [514/30000]: Train loss: 1.5727, Valid loss: 1.7493, L1 loss: 1.7934\n",
            "Epoch [515/30000]: Train loss: 1.5640, Valid loss: 1.7587, L1 loss: 1.7997\n",
            "Epoch [516/30000]: Train loss: 1.5653, Valid loss: 1.7656, L1 loss: 1.8092\n",
            "Epoch [517/30000]: Train loss: 1.5532, Valid loss: 1.7760, L1 loss: 1.8218\n",
            "Epoch [518/30000]: Train loss: 1.5604, Valid loss: 1.7804, L1 loss: 1.8219\n",
            "Epoch [519/30000]: Train loss: 1.5725, Valid loss: 1.7729, L1 loss: 1.8175\n",
            "Epoch [520/30000]: Train loss: 1.5541, Valid loss: 1.7774, L1 loss: 1.8111\n",
            "Epoch [521/30000]: Train loss: 1.5686, Valid loss: 1.7324, L1 loss: 1.7755\n",
            "Epoch [522/30000]: Train loss: 1.5689, Valid loss: 1.7574, L1 loss: 1.8002\n",
            "Epoch [523/30000]: Train loss: 1.5779, Valid loss: 1.7549, L1 loss: 1.7963\n",
            "Epoch [524/30000]: Train loss: 1.5631, Valid loss: 1.7500, L1 loss: 1.7979\n",
            "Epoch [525/30000]: Train loss: 1.5622, Valid loss: 1.7544, L1 loss: 1.8014\n",
            "Epoch [526/30000]: Train loss: 1.5647, Valid loss: 1.7678, L1 loss: 1.8104\n",
            "Epoch [527/30000]: Train loss: 1.5704, Valid loss: 1.7579, L1 loss: 1.8012\n",
            "Epoch [528/30000]: Train loss: 1.5648, Valid loss: 1.7620, L1 loss: 1.8070\n",
            "Epoch [529/30000]: Train loss: 1.5673, Valid loss: 1.7429, L1 loss: 1.7864\n",
            "Epoch [530/30000]: Train loss: 1.5614, Valid loss: 1.7627, L1 loss: 1.8006\n",
            "Epoch [531/30000]: Train loss: 1.5617, Valid loss: 1.7525, L1 loss: 1.7939\n",
            "Epoch [532/30000]: Train loss: 1.5654, Valid loss: 1.7698, L1 loss: 1.8068\n",
            "Epoch [533/30000]: Train loss: 1.5642, Valid loss: 1.7569, L1 loss: 1.7955\n",
            "Epoch [534/30000]: Train loss: 1.5597, Valid loss: 1.7602, L1 loss: 1.8021\n",
            "Epoch [535/30000]: Train loss: 1.5740, Valid loss: 1.7813, L1 loss: 1.8123\n",
            "Epoch [536/30000]: Train loss: 1.5719, Valid loss: 1.7618, L1 loss: 1.8018\n",
            "Epoch [537/30000]: Train loss: 1.5661, Valid loss: 1.7703, L1 loss: 1.8057\n",
            "Epoch [538/30000]: Train loss: 1.5559, Valid loss: 1.7610, L1 loss: 1.8057\n",
            "Epoch [539/30000]: Train loss: 1.5696, Valid loss: 1.7691, L1 loss: 1.8059\n",
            "Epoch [540/30000]: Train loss: 1.5655, Valid loss: 1.7884, L1 loss: 1.8198\n",
            "Epoch [541/30000]: Train loss: 1.5707, Valid loss: 1.8043, L1 loss: 1.8362\n",
            "Epoch [542/30000]: Train loss: 1.5644, Valid loss: 1.7670, L1 loss: 1.8011\n",
            "Epoch [543/30000]: Train loss: 1.5651, Valid loss: 1.7767, L1 loss: 1.8082\n",
            "Epoch [544/30000]: Train loss: 1.5519, Valid loss: 1.7718, L1 loss: 1.8133\n",
            "Epoch [545/30000]: Train loss: 1.5684, Valid loss: 1.8100, L1 loss: 1.8489\n",
            "Epoch [546/30000]: Train loss: 1.5487, Valid loss: 1.8016, L1 loss: 1.8414\n",
            "Epoch [547/30000]: Train loss: 1.5687, Valid loss: 1.7741, L1 loss: 1.8162\n",
            "Epoch [548/30000]: Train loss: 1.5635, Valid loss: 1.7834, L1 loss: 1.8207\n",
            "Epoch [549/30000]: Train loss: 1.5605, Valid loss: 1.7777, L1 loss: 1.8156\n",
            "Epoch [550/30000]: Train loss: 1.5614, Valid loss: 1.7805, L1 loss: 1.8196\n",
            "Epoch [551/30000]: Train loss: 1.5718, Valid loss: 1.7808, L1 loss: 1.8150\n",
            "Epoch [552/30000]: Train loss: 1.5610, Valid loss: 1.7884, L1 loss: 1.8268\n",
            "Epoch [553/30000]: Train loss: 1.5741, Valid loss: 1.7820, L1 loss: 1.8226\n",
            "Epoch [554/30000]: Train loss: 1.5632, Valid loss: 1.7808, L1 loss: 1.8260\n",
            "Epoch [555/30000]: Train loss: 1.5556, Valid loss: 1.7706, L1 loss: 1.8146\n",
            "Epoch [556/30000]: Train loss: 1.5593, Valid loss: 1.8014, L1 loss: 1.8394\n",
            "Epoch [557/30000]: Train loss: 1.5537, Valid loss: 1.7725, L1 loss: 1.8203\n",
            "Epoch [558/30000]: Train loss: 1.5604, Valid loss: 1.7970, L1 loss: 1.8416\n",
            "Epoch [559/30000]: Train loss: 1.5621, Valid loss: 1.7825, L1 loss: 1.8247\n",
            "Epoch [560/30000]: Train loss: 1.5492, Valid loss: 1.7855, L1 loss: 1.8222\n",
            "Epoch [561/30000]: Train loss: 1.5631, Valid loss: 1.8113, L1 loss: 1.8445\n",
            "Epoch [562/30000]: Train loss: 1.5758, Valid loss: 1.7554, L1 loss: 1.7969\n",
            "Epoch [563/30000]: Train loss: 1.5676, Valid loss: 1.7654, L1 loss: 1.8106\n",
            "Epoch [564/30000]: Train loss: 1.5668, Valid loss: 1.7783, L1 loss: 1.8241\n",
            "Epoch [565/30000]: Train loss: 1.5536, Valid loss: 1.7520, L1 loss: 1.7960\n",
            "Epoch [566/30000]: Train loss: 1.5615, Valid loss: 1.7859, L1 loss: 1.8240\n",
            "Epoch [567/30000]: Train loss: 1.5487, Valid loss: 1.7596, L1 loss: 1.8001\n",
            "Epoch [568/30000]: Train loss: 1.5652, Valid loss: 1.7657, L1 loss: 1.8094\n",
            "Epoch [569/30000]: Train loss: 1.5619, Valid loss: 1.7948, L1 loss: 1.8327\n",
            "Epoch [570/30000]: Train loss: 1.5620, Valid loss: 1.7802, L1 loss: 1.8168\n",
            "Epoch [571/30000]: Train loss: 1.5668, Valid loss: 1.7597, L1 loss: 1.8005\n",
            "Epoch [572/30000]: Train loss: 1.5542, Valid loss: 1.7753, L1 loss: 1.8059\n",
            "Epoch [573/30000]: Train loss: 1.5628, Valid loss: 1.7578, L1 loss: 1.8042\n",
            "Epoch [574/30000]: Train loss: 1.5593, Valid loss: 1.7413, L1 loss: 1.7862\n",
            "Epoch [575/30000]: Train loss: 1.5582, Valid loss: 1.7389, L1 loss: 1.7858\n",
            "Epoch [576/30000]: Train loss: 1.5575, Valid loss: 1.7465, L1 loss: 1.7926\n",
            "Epoch [577/30000]: Train loss: 1.5710, Valid loss: 1.7554, L1 loss: 1.8002\n",
            "Epoch [578/30000]: Train loss: 1.5584, Valid loss: 1.7691, L1 loss: 1.8054\n",
            "Epoch [579/30000]: Train loss: 1.5709, Valid loss: 1.7246, L1 loss: 1.7674\n",
            "Epoch [580/30000]: Train loss: 1.5695, Valid loss: 1.7783, L1 loss: 1.8170\n",
            "Epoch [581/30000]: Train loss: 1.5687, Valid loss: 1.7465, L1 loss: 1.7909\n",
            "Epoch [582/30000]: Train loss: 1.5615, Valid loss: 1.7881, L1 loss: 1.8180\n",
            "Epoch [583/30000]: Train loss: 1.5832, Valid loss: 1.7542, L1 loss: 1.7946\n",
            "Epoch [584/30000]: Train loss: 1.5535, Valid loss: 1.7712, L1 loss: 1.8112\n",
            "Epoch [585/30000]: Train loss: 1.5671, Valid loss: 1.7622, L1 loss: 1.8057\n",
            "Epoch [586/30000]: Train loss: 1.5651, Valid loss: 1.7630, L1 loss: 1.8019\n",
            "Epoch [587/30000]: Train loss: 1.5684, Valid loss: 1.7309, L1 loss: 1.7782\n",
            "Epoch [588/30000]: Train loss: 1.5704, Valid loss: 1.7863, L1 loss: 1.8223\n",
            "Epoch [589/30000]: Train loss: 1.5614, Valid loss: 1.7652, L1 loss: 1.8054\n",
            "Epoch [590/30000]: Train loss: 1.5683, Valid loss: 1.7731, L1 loss: 1.8186\n",
            "Epoch [591/30000]: Train loss: 1.5616, Valid loss: 1.7810, L1 loss: 1.8182\n",
            "Epoch [592/30000]: Train loss: 1.5545, Valid loss: 1.7793, L1 loss: 1.8208\n",
            "Epoch [593/30000]: Train loss: 1.5547, Valid loss: 1.7689, L1 loss: 1.8125\n",
            "Epoch [594/30000]: Train loss: 1.5656, Valid loss: 1.7498, L1 loss: 1.7938\n",
            "Epoch [595/30000]: Train loss: 1.5551, Valid loss: 1.8124, L1 loss: 1.8473\n",
            "Epoch [596/30000]: Train loss: 1.5609, Valid loss: 1.7559, L1 loss: 1.7993\n",
            "Epoch [597/30000]: Train loss: 1.5672, Valid loss: 1.7774, L1 loss: 1.8242\n",
            "Epoch [598/30000]: Train loss: 1.5606, Valid loss: 1.7795, L1 loss: 1.8238\n",
            "Epoch [599/30000]: Train loss: 1.5724, Valid loss: 1.8091, L1 loss: 1.8462\n",
            "Epoch [600/30000]: Train loss: 1.5569, Valid loss: 1.7687, L1 loss: 1.8105\n",
            "Epoch [601/30000]: Train loss: 1.5575, Valid loss: 1.7688, L1 loss: 1.8044\n",
            "Epoch [602/30000]: Train loss: 1.5705, Valid loss: 1.7860, L1 loss: 1.8123\n",
            "Epoch [603/30000]: Train loss: 1.5677, Valid loss: 1.7968, L1 loss: 1.8272\n",
            "Epoch [604/30000]: Train loss: 1.5531, Valid loss: 1.7772, L1 loss: 1.8140\n",
            "Epoch [605/30000]: Train loss: 1.5587, Valid loss: 1.7482, L1 loss: 1.7900\n",
            "Epoch [606/30000]: Train loss: 1.5629, Valid loss: 1.8195, L1 loss: 1.8549\n",
            "Epoch [607/30000]: Train loss: 1.5621, Valid loss: 1.7914, L1 loss: 1.8281\n",
            "Epoch [608/30000]: Train loss: 1.5614, Valid loss: 1.8032, L1 loss: 1.8439\n",
            "Epoch [609/30000]: Train loss: 1.5620, Valid loss: 1.7897, L1 loss: 1.8252\n",
            "Epoch [610/30000]: Train loss: 1.5528, Valid loss: 1.7869, L1 loss: 1.8287\n",
            "Epoch [611/30000]: Train loss: 1.5643, Valid loss: 1.8012, L1 loss: 1.8387\n",
            "Epoch [612/30000]: Train loss: 1.5540, Valid loss: 1.7791, L1 loss: 1.8202\n",
            "Epoch [613/30000]: Train loss: 1.5595, Valid loss: 1.8035, L1 loss: 1.8358\n",
            "Epoch [614/30000]: Train loss: 1.5646, Valid loss: 1.7904, L1 loss: 1.8326\n",
            "Epoch [615/30000]: Train loss: 1.5580, Valid loss: 1.7537, L1 loss: 1.7994\n",
            "Epoch [616/30000]: Train loss: 1.5727, Valid loss: 1.8107, L1 loss: 1.8407\n",
            "Epoch [617/30000]: Train loss: 1.5593, Valid loss: 1.8125, L1 loss: 1.8390\n",
            "Epoch [618/30000]: Train loss: 1.5718, Valid loss: 1.7724, L1 loss: 1.8120\n",
            "Epoch [619/30000]: Train loss: 1.5638, Valid loss: 1.7842, L1 loss: 1.8258\n",
            "Epoch [620/30000]: Train loss: 1.5704, Valid loss: 1.7862, L1 loss: 1.8261\n",
            "Epoch [621/30000]: Train loss: 1.5654, Valid loss: 1.8010, L1 loss: 1.8365\n",
            "Epoch [622/30000]: Train loss: 1.5544, Valid loss: 1.7789, L1 loss: 1.8196\n",
            "Epoch [623/30000]: Train loss: 1.5634, Valid loss: 1.7961, L1 loss: 1.8358\n",
            "Epoch [624/30000]: Train loss: 1.5629, Valid loss: 1.7796, L1 loss: 1.8183\n",
            "Epoch [625/30000]: Train loss: 1.5589, Valid loss: 1.7924, L1 loss: 1.8308\n",
            "Epoch [626/30000]: Train loss: 1.5661, Valid loss: 1.7879, L1 loss: 1.8326\n",
            "Epoch [627/30000]: Train loss: 1.5708, Valid loss: 1.8093, L1 loss: 1.8440\n",
            "Epoch [628/30000]: Train loss: 1.5584, Valid loss: 1.8206, L1 loss: 1.8567\n",
            "Epoch [629/30000]: Train loss: 1.5689, Valid loss: 1.8130, L1 loss: 1.8521\n",
            "Epoch [630/30000]: Train loss: 1.5724, Valid loss: 1.7952, L1 loss: 1.8335\n",
            "Epoch [631/30000]: Train loss: 1.5706, Valid loss: 1.8216, L1 loss: 1.8559\n",
            "Epoch [632/30000]: Train loss: 1.5683, Valid loss: 1.8099, L1 loss: 1.8444\n",
            "Epoch [633/30000]: Train loss: 1.5599, Valid loss: 1.7816, L1 loss: 1.8276\n",
            "Epoch [634/30000]: Train loss: 1.5428, Valid loss: 1.7987, L1 loss: 1.8346\n",
            "Epoch [635/30000]: Train loss: 1.5634, Valid loss: 1.7939, L1 loss: 1.8346\n",
            "Epoch [636/30000]: Train loss: 1.5697, Valid loss: 1.7616, L1 loss: 1.8025\n",
            "Epoch [637/30000]: Train loss: 1.5752, Valid loss: 1.7552, L1 loss: 1.8040\n",
            "Epoch [638/30000]: Train loss: 1.5674, Valid loss: 1.7983, L1 loss: 1.8404\n",
            "Epoch [639/30000]: Train loss: 1.5642, Valid loss: 1.7961, L1 loss: 1.8357\n",
            "Epoch [640/30000]: Train loss: 1.5687, Valid loss: 1.7798, L1 loss: 1.8234\n",
            "Epoch [641/30000]: Train loss: 1.5635, Valid loss: 1.7667, L1 loss: 1.8081\n",
            "Epoch [642/30000]: Train loss: 1.5618, Valid loss: 1.7968, L1 loss: 1.8352\n",
            "Epoch [643/30000]: Train loss: 1.5627, Valid loss: 1.8022, L1 loss: 1.8363\n",
            "Epoch [644/30000]: Train loss: 1.5674, Valid loss: 1.7970, L1 loss: 1.8332\n",
            "Epoch [645/30000]: Train loss: 1.5651, Valid loss: 1.7684, L1 loss: 1.8079\n",
            "Epoch [646/30000]: Train loss: 1.5728, Valid loss: 1.7876, L1 loss: 1.8208\n",
            "Epoch [647/30000]: Train loss: 1.5702, Valid loss: 1.7755, L1 loss: 1.8103\n",
            "Epoch [648/30000]: Train loss: 1.5748, Valid loss: 1.7361, L1 loss: 1.7813\n",
            "Epoch [649/30000]: Train loss: 1.5547, Valid loss: 1.7761, L1 loss: 1.8134\n",
            "Epoch [650/30000]: Train loss: 1.5603, Valid loss: 1.7750, L1 loss: 1.8087\n",
            "Epoch [651/30000]: Train loss: 1.5599, Valid loss: 1.7661, L1 loss: 1.8014\n",
            "Epoch [652/30000]: Train loss: 1.5685, Valid loss: 1.7608, L1 loss: 1.8014\n",
            "Epoch [653/30000]: Train loss: 1.5610, Valid loss: 1.7853, L1 loss: 1.8201\n",
            "Epoch [654/30000]: Train loss: 1.5593, Valid loss: 1.7711, L1 loss: 1.8049\n",
            "Epoch [655/30000]: Train loss: 1.5610, Valid loss: 1.7934, L1 loss: 1.8205\n",
            "Epoch [656/30000]: Train loss: 1.5522, Valid loss: 1.7618, L1 loss: 1.7970\n",
            "Epoch [657/30000]: Train loss: 1.5712, Valid loss: 1.7943, L1 loss: 1.8320\n",
            "Epoch [658/30000]: Train loss: 1.5665, Valid loss: 1.7967, L1 loss: 1.8331\n",
            "Epoch [659/30000]: Train loss: 1.5596, Valid loss: 1.8087, L1 loss: 1.8386\n",
            "Epoch [660/30000]: Train loss: 1.5700, Valid loss: 1.8122, L1 loss: 1.8480\n",
            "Epoch [661/30000]: Train loss: 1.5661, Valid loss: 1.8033, L1 loss: 1.8317\n",
            "Epoch [662/30000]: Train loss: 1.5597, Valid loss: 1.7812, L1 loss: 1.8188\n",
            "Epoch [663/30000]: Train loss: 1.5623, Valid loss: 1.8175, L1 loss: 1.8451\n",
            "Epoch [664/30000]: Train loss: 1.5761, Valid loss: 1.7807, L1 loss: 1.8141\n",
            "Epoch [665/30000]: Train loss: 1.5601, Valid loss: 1.7839, L1 loss: 1.8178\n",
            "Epoch [666/30000]: Train loss: 1.5584, Valid loss: 1.7711, L1 loss: 1.8088\n",
            "Epoch [667/30000]: Train loss: 1.5619, Valid loss: 1.7634, L1 loss: 1.8001\n",
            "Epoch [668/30000]: Train loss: 1.5753, Valid loss: 1.7645, L1 loss: 1.7991\n",
            "Epoch [669/30000]: Train loss: 1.5590, Valid loss: 1.7858, L1 loss: 1.8237\n",
            "Epoch [670/30000]: Train loss: 1.5667, Valid loss: 1.7624, L1 loss: 1.8011\n",
            "Epoch [671/30000]: Train loss: 1.5517, Valid loss: 1.7742, L1 loss: 1.8094\n",
            "Epoch [672/30000]: Train loss: 1.5644, Valid loss: 1.7660, L1 loss: 1.7974\n",
            "Epoch [673/30000]: Train loss: 1.5505, Valid loss: 1.7980, L1 loss: 1.8269\n",
            "Epoch [674/30000]: Train loss: 1.5722, Valid loss: 1.8221, L1 loss: 1.8493\n",
            "Epoch [675/30000]: Train loss: 1.5564, Valid loss: 1.7904, L1 loss: 1.8218\n",
            "Epoch [676/30000]: Train loss: 1.5556, Valid loss: 1.7887, L1 loss: 1.8201\n",
            "Epoch [677/30000]: Train loss: 1.5679, Valid loss: 1.7777, L1 loss: 1.8129\n",
            "Epoch [678/30000]: Train loss: 1.5564, Valid loss: 1.7859, L1 loss: 1.8164\n",
            "Epoch [679/30000]: Train loss: 1.5637, Valid loss: 1.7923, L1 loss: 1.8189\n",
            "Epoch [680/30000]: Train loss: 1.5694, Valid loss: 1.7872, L1 loss: 1.8295\n",
            "Epoch [681/30000]: Train loss: 1.5609, Valid loss: 1.8221, L1 loss: 1.8536\n",
            "Epoch [682/30000]: Train loss: 1.5705, Valid loss: 1.7978, L1 loss: 1.8338\n",
            "Epoch [683/30000]: Train loss: 1.5731, Valid loss: 1.8223, L1 loss: 1.8561\n",
            "Epoch [684/30000]: Train loss: 1.5714, Valid loss: 1.7716, L1 loss: 1.8134\n",
            "Epoch [685/30000]: Train loss: 1.5550, Valid loss: 1.7820, L1 loss: 1.8182\n",
            "Epoch [686/30000]: Train loss: 1.5560, Valid loss: 1.7959, L1 loss: 1.8331\n",
            "Epoch [687/30000]: Train loss: 1.5699, Valid loss: 1.7852, L1 loss: 1.8197\n",
            "Epoch [688/30000]: Train loss: 1.5689, Valid loss: 1.7669, L1 loss: 1.8112\n",
            "Epoch [689/30000]: Train loss: 1.5439, Valid loss: 1.7717, L1 loss: 1.8100\n",
            "Epoch [690/30000]: Train loss: 1.5648, Valid loss: 1.7847, L1 loss: 1.8208\n",
            "Epoch [691/30000]: Train loss: 1.5574, Valid loss: 1.7996, L1 loss: 1.8346\n",
            "Epoch [692/30000]: Train loss: 1.5559, Valid loss: 1.8028, L1 loss: 1.8405\n",
            "Epoch [693/30000]: Train loss: 1.5559, Valid loss: 1.8022, L1 loss: 1.8407\n",
            "Epoch [694/30000]: Train loss: 1.5451, Valid loss: 1.7948, L1 loss: 1.8322\n",
            "Epoch [695/30000]: Train loss: 1.5589, Valid loss: 1.7539, L1 loss: 1.7941\n",
            "Epoch [696/30000]: Train loss: 1.5705, Valid loss: 1.7756, L1 loss: 1.8163\n",
            "Epoch [697/30000]: Train loss: 1.5548, Valid loss: 1.7814, L1 loss: 1.8240\n",
            "Epoch [698/30000]: Train loss: 1.5683, Valid loss: 1.8076, L1 loss: 1.8421\n",
            "Epoch [699/30000]: Train loss: 1.5419, Valid loss: 1.7786, L1 loss: 1.8164\n",
            "Epoch [700/30000]: Train loss: 1.5681, Valid loss: 1.7551, L1 loss: 1.7981\n",
            "Epoch [701/30000]: Train loss: 1.5565, Valid loss: 1.7974, L1 loss: 1.8327\n",
            "Epoch [702/30000]: Train loss: 1.5648, Valid loss: 1.7711, L1 loss: 1.8129\n",
            "Epoch [703/30000]: Train loss: 1.5633, Valid loss: 1.7872, L1 loss: 1.8266\n",
            "Epoch [704/30000]: Train loss: 1.5585, Valid loss: 1.7939, L1 loss: 1.8317\n",
            "Epoch [705/30000]: Train loss: 1.5625, Valid loss: 1.8174, L1 loss: 1.8515\n",
            "Epoch [706/30000]: Train loss: 1.5588, Valid loss: 1.8109, L1 loss: 1.8457\n",
            "Epoch [707/30000]: Train loss: 1.5604, Valid loss: 1.7924, L1 loss: 1.8326\n",
            "Epoch [708/30000]: Train loss: 1.5593, Valid loss: 1.7727, L1 loss: 1.8122\n",
            "Epoch [709/30000]: Train loss: 1.5529, Valid loss: 1.8372, L1 loss: 1.8704\n",
            "Epoch [710/30000]: Train loss: 1.5582, Valid loss: 1.7891, L1 loss: 1.8250\n",
            "Epoch [711/30000]: Train loss: 1.5566, Valid loss: 1.8021, L1 loss: 1.8367\n",
            "Epoch [712/30000]: Train loss: 1.5664, Valid loss: 1.8040, L1 loss: 1.8358\n",
            "Epoch [713/30000]: Train loss: 1.5557, Valid loss: 1.8087, L1 loss: 1.8351\n",
            "Epoch [714/30000]: Train loss: 1.5601, Valid loss: 1.7554, L1 loss: 1.7970\n",
            "Epoch [715/30000]: Train loss: 1.5677, Valid loss: 1.8121, L1 loss: 1.8418\n",
            "Epoch [716/30000]: Train loss: 1.5604, Valid loss: 1.7709, L1 loss: 1.8061\n",
            "Epoch [717/30000]: Train loss: 1.5634, Valid loss: 1.8061, L1 loss: 1.8301\n",
            "Epoch [718/30000]: Train loss: 1.5668, Valid loss: 1.8484, L1 loss: 1.8772\n",
            "Epoch [719/30000]: Train loss: 1.5607, Valid loss: 1.8058, L1 loss: 1.8377\n",
            "Epoch [720/30000]: Train loss: 1.5610, Valid loss: 1.8344, L1 loss: 1.8618\n",
            "Epoch [721/30000]: Train loss: 1.5552, Valid loss: 1.8213, L1 loss: 1.8496\n",
            "Epoch [722/30000]: Train loss: 1.5501, Valid loss: 1.8090, L1 loss: 1.8455\n",
            "Epoch [723/30000]: Train loss: 1.5597, Valid loss: 1.8486, L1 loss: 1.8824\n",
            "Epoch [724/30000]: Train loss: 1.5526, Valid loss: 1.8330, L1 loss: 1.8590\n",
            "Epoch [725/30000]: Train loss: 1.5599, Valid loss: 1.8175, L1 loss: 1.8525\n",
            "Epoch [726/30000]: Train loss: 1.5716, Valid loss: 1.8003, L1 loss: 1.8385\n",
            "Epoch [727/30000]: Train loss: 1.5499, Valid loss: 1.8221, L1 loss: 1.8555\n",
            "Epoch [728/30000]: Train loss: 1.5578, Valid loss: 1.7979, L1 loss: 1.8340\n",
            "Epoch [729/30000]: Train loss: 1.5628, Valid loss: 1.8430, L1 loss: 1.8685\n",
            "Epoch [730/30000]: Train loss: 1.5580, Valid loss: 1.8445, L1 loss: 1.8754\n",
            "Epoch [731/30000]: Train loss: 1.5571, Valid loss: 1.8128, L1 loss: 1.8472\n",
            "Epoch [732/30000]: Train loss: 1.5649, Valid loss: 1.8174, L1 loss: 1.8450\n",
            "Epoch [733/30000]: Train loss: 1.5577, Valid loss: 1.7960, L1 loss: 1.8344\n",
            "Epoch [734/30000]: Train loss: 1.5531, Valid loss: 1.8408, L1 loss: 1.8709\n",
            "Epoch [735/30000]: Train loss: 1.5528, Valid loss: 1.8276, L1 loss: 1.8593\n",
            "Epoch [736/30000]: Train loss: 1.5606, Valid loss: 1.7639, L1 loss: 1.8060\n",
            "Epoch [737/30000]: Train loss: 1.5631, Valid loss: 1.8017, L1 loss: 1.8359\n",
            "Epoch [738/30000]: Train loss: 1.5511, Valid loss: 1.8138, L1 loss: 1.8392\n",
            "Epoch [739/30000]: Train loss: 1.5636, Valid loss: 1.8046, L1 loss: 1.8330\n",
            "Epoch [740/30000]: Train loss: 1.5420, Valid loss: 1.8525, L1 loss: 1.8768\n",
            "Epoch [741/30000]: Train loss: 1.5575, Valid loss: 1.8206, L1 loss: 1.8495\n",
            "Epoch [742/30000]: Train loss: 1.5563, Valid loss: 1.8050, L1 loss: 1.8309\n",
            "Epoch [743/30000]: Train loss: 1.5583, Valid loss: 1.7787, L1 loss: 1.8085\n",
            "Epoch [744/30000]: Train loss: 1.5487, Valid loss: 1.8498, L1 loss: 1.8701\n",
            "Epoch [745/30000]: Train loss: 1.5572, Valid loss: 1.8139, L1 loss: 1.8458\n",
            "Epoch [746/30000]: Train loss: 1.5534, Valid loss: 1.8474, L1 loss: 1.8717\n",
            "Epoch [747/30000]: Train loss: 1.5542, Valid loss: 1.8156, L1 loss: 1.8491\n",
            "Epoch [748/30000]: Train loss: 1.5611, Valid loss: 1.8346, L1 loss: 1.8665\n",
            "Epoch [749/30000]: Train loss: 1.5592, Valid loss: 1.8071, L1 loss: 1.8436\n",
            "Epoch [750/30000]: Train loss: 1.5612, Valid loss: 1.8039, L1 loss: 1.8426\n",
            "Epoch [751/30000]: Train loss: 1.5605, Valid loss: 1.8301, L1 loss: 1.8577\n",
            "Epoch [752/30000]: Train loss: 1.5646, Valid loss: 1.7635, L1 loss: 1.8015\n",
            "Epoch [753/30000]: Train loss: 1.5608, Valid loss: 1.8076, L1 loss: 1.8451\n",
            "Epoch [754/30000]: Train loss: 1.5565, Valid loss: 1.8069, L1 loss: 1.8457\n",
            "Epoch [755/30000]: Train loss: 1.5594, Valid loss: 1.8156, L1 loss: 1.8427\n",
            "Epoch [756/30000]: Train loss: 1.5513, Valid loss: 1.8369, L1 loss: 1.8646\n",
            "Epoch [757/30000]: Train loss: 1.5466, Valid loss: 1.7923, L1 loss: 1.8291\n",
            "Epoch [758/30000]: Train loss: 1.5619, Valid loss: 1.7776, L1 loss: 1.8103\n",
            "Epoch [759/30000]: Train loss: 1.5502, Valid loss: 1.7777, L1 loss: 1.8122\n",
            "Epoch [760/30000]: Train loss: 1.5667, Valid loss: 1.8083, L1 loss: 1.8418\n",
            "Epoch [761/30000]: Train loss: 1.5600, Valid loss: 1.7854, L1 loss: 1.8257\n",
            "Epoch [762/30000]: Train loss: 1.5561, Valid loss: 1.7815, L1 loss: 1.8226\n",
            "Epoch [763/30000]: Train loss: 1.5655, Valid loss: 1.7801, L1 loss: 1.8175\n",
            "Epoch [764/30000]: Train loss: 1.5560, Valid loss: 1.7901, L1 loss: 1.8283\n",
            "Epoch [765/30000]: Train loss: 1.5534, Valid loss: 1.7849, L1 loss: 1.8206\n",
            "Epoch [766/30000]: Train loss: 1.5522, Valid loss: 1.8206, L1 loss: 1.8548\n",
            "Epoch [767/30000]: Train loss: 1.5632, Valid loss: 1.7888, L1 loss: 1.8204\n",
            "Epoch [768/30000]: Train loss: 1.5420, Valid loss: 1.8020, L1 loss: 1.8393\n",
            "Epoch [769/30000]: Train loss: 1.5567, Valid loss: 1.7838, L1 loss: 1.8255\n",
            "Epoch [770/30000]: Train loss: 1.5582, Valid loss: 1.7691, L1 loss: 1.8073\n",
            "Epoch [771/30000]: Train loss: 1.5495, Valid loss: 1.8065, L1 loss: 1.8391\n",
            "Epoch [772/30000]: Train loss: 1.5656, Valid loss: 1.8048, L1 loss: 1.8343\n",
            "Epoch [773/30000]: Train loss: 1.5602, Valid loss: 1.7981, L1 loss: 1.8278\n",
            "Epoch [774/30000]: Train loss: 1.5543, Valid loss: 1.8214, L1 loss: 1.8507\n",
            "Epoch [775/30000]: Train loss: 1.5371, Valid loss: 1.8101, L1 loss: 1.8368\n",
            "Epoch [776/30000]: Train loss: 1.5602, Valid loss: 1.8139, L1 loss: 1.8487\n",
            "Epoch [777/30000]: Train loss: 1.5638, Valid loss: 1.7990, L1 loss: 1.8320\n",
            "Epoch [778/30000]: Train loss: 1.5475, Valid loss: 1.7950, L1 loss: 1.8345\n",
            "Epoch [779/30000]: Train loss: 1.5547, Valid loss: 1.8065, L1 loss: 1.8418\n",
            "Epoch [780/30000]: Train loss: 1.5577, Valid loss: 1.7912, L1 loss: 1.8257\n",
            "Epoch [781/30000]: Train loss: 1.5501, Valid loss: 1.8241, L1 loss: 1.8568\n",
            "Epoch [782/30000]: Train loss: 1.5536, Valid loss: 1.8249, L1 loss: 1.8541\n",
            "Epoch [783/30000]: Train loss: 1.5598, Valid loss: 1.7971, L1 loss: 1.8315\n",
            "Epoch [784/30000]: Train loss: 1.5634, Valid loss: 1.7748, L1 loss: 1.8144\n",
            "Epoch [785/30000]: Train loss: 1.5580, Valid loss: 1.8071, L1 loss: 1.8343\n",
            "Epoch [786/30000]: Train loss: 1.5621, Valid loss: 1.8025, L1 loss: 1.8334\n",
            "Epoch [787/30000]: Train loss: 1.5579, Valid loss: 1.7969, L1 loss: 1.8313\n",
            "Epoch [788/30000]: Train loss: 1.5588, Valid loss: 1.7978, L1 loss: 1.8317\n",
            "Epoch [789/30000]: Train loss: 1.5601, Valid loss: 1.8436, L1 loss: 1.8688\n",
            "Epoch [790/30000]: Train loss: 1.5566, Valid loss: 1.7711, L1 loss: 1.8070\n",
            "Epoch [791/30000]: Train loss: 1.5672, Valid loss: 1.8370, L1 loss: 1.8618\n",
            "Epoch [792/30000]: Train loss: 1.5611, Valid loss: 1.7705, L1 loss: 1.8056\n",
            "Epoch [793/30000]: Train loss: 1.5482, Valid loss: 1.8350, L1 loss: 1.8558\n",
            "Epoch [794/30000]: Train loss: 1.5552, Valid loss: 1.7805, L1 loss: 1.8119\n",
            "Epoch [795/30000]: Train loss: 1.5662, Valid loss: 1.8272, L1 loss: 1.8520\n",
            "Epoch [796/30000]: Train loss: 1.5513, Valid loss: 1.8238, L1 loss: 1.8492\n",
            "Epoch [797/30000]: Train loss: 1.5480, Valid loss: 1.8188, L1 loss: 1.8439\n",
            "Epoch [798/30000]: Train loss: 1.5496, Valid loss: 1.8183, L1 loss: 1.8444\n",
            "Epoch [799/30000]: Train loss: 1.5495, Valid loss: 1.8037, L1 loss: 1.8349\n",
            "Epoch [800/30000]: Train loss: 1.5617, Valid loss: 1.7752, L1 loss: 1.8117\n",
            "Epoch [801/30000]: Train loss: 1.5659, Valid loss: 1.7561, L1 loss: 1.7930\n",
            "Epoch [802/30000]: Train loss: 1.5530, Valid loss: 1.7430, L1 loss: 1.7881\n",
            "Epoch [803/30000]: Train loss: 1.5741, Valid loss: 1.7620, L1 loss: 1.7980\n",
            "Epoch [804/30000]: Train loss: 1.5546, Valid loss: 1.7890, L1 loss: 1.8275\n",
            "Epoch [805/30000]: Train loss: 1.5633, Valid loss: 1.7934, L1 loss: 1.8363\n",
            "Epoch [806/30000]: Train loss: 1.5648, Valid loss: 1.7475, L1 loss: 1.7950\n",
            "Epoch [807/30000]: Train loss: 1.5531, Valid loss: 1.7732, L1 loss: 1.8126\n",
            "Epoch [808/30000]: Train loss: 1.5593, Valid loss: 1.7883, L1 loss: 1.8291\n",
            "Epoch [809/30000]: Train loss: 1.5586, Valid loss: 1.7744, L1 loss: 1.8178\n",
            "Epoch [810/30000]: Train loss: 1.5656, Valid loss: 1.7760, L1 loss: 1.8230\n",
            "Epoch [811/30000]: Train loss: 1.5633, Valid loss: 1.7757, L1 loss: 1.8206\n",
            "Epoch [812/30000]: Train loss: 1.5484, Valid loss: 1.7775, L1 loss: 1.8129\n",
            "Epoch [813/30000]: Train loss: 1.5510, Valid loss: 1.7924, L1 loss: 1.8284\n",
            "Epoch [814/30000]: Train loss: 1.5743, Valid loss: 1.7748, L1 loss: 1.8106\n",
            "Epoch [815/30000]: Train loss: 1.5573, Valid loss: 1.7638, L1 loss: 1.8003\n",
            "Epoch [816/30000]: Train loss: 1.5503, Valid loss: 1.7466, L1 loss: 1.7822\n",
            "Epoch [817/30000]: Train loss: 1.5568, Valid loss: 1.7520, L1 loss: 1.7864\n",
            "Epoch [818/30000]: Train loss: 1.5654, Valid loss: 1.7385, L1 loss: 1.7822\n",
            "Epoch [819/30000]: Train loss: 1.5493, Valid loss: 1.8139, L1 loss: 1.8397\n",
            "Epoch [820/30000]: Train loss: 1.5618, Valid loss: 1.7710, L1 loss: 1.8077\n",
            "Epoch [821/30000]: Train loss: 1.5715, Valid loss: 1.7657, L1 loss: 1.7992\n",
            "Epoch [822/30000]: Train loss: 1.5641, Valid loss: 1.7518, L1 loss: 1.7925\n",
            "Epoch [823/30000]: Train loss: 1.5458, Valid loss: 1.7541, L1 loss: 1.7900\n",
            "Epoch [824/30000]: Train loss: 1.5572, Valid loss: 1.7357, L1 loss: 1.7709\n",
            "Epoch [825/30000]: Train loss: 1.5571, Valid loss: 1.7619, L1 loss: 1.7985\n",
            "Epoch [826/30000]: Train loss: 1.5599, Valid loss: 1.7464, L1 loss: 1.7816\n",
            "Epoch [827/30000]: Train loss: 1.5602, Valid loss: 1.7269, L1 loss: 1.7723\n",
            "Epoch [828/30000]: Train loss: 1.5569, Valid loss: 1.7704, L1 loss: 1.8080\n",
            "Epoch [829/30000]: Train loss: 1.5540, Valid loss: 1.7839, L1 loss: 1.8179\n",
            "Epoch [830/30000]: Train loss: 1.5562, Valid loss: 1.7706, L1 loss: 1.8067\n",
            "Epoch [831/30000]: Train loss: 1.5696, Valid loss: 1.7611, L1 loss: 1.8010\n",
            "Epoch [832/30000]: Train loss: 1.5668, Valid loss: 1.7521, L1 loss: 1.7945\n",
            "Epoch [833/30000]: Train loss: 1.5696, Valid loss: 1.7594, L1 loss: 1.7970\n",
            "Epoch [834/30000]: Train loss: 1.5536, Valid loss: 1.7798, L1 loss: 1.8201\n",
            "Epoch [835/30000]: Train loss: 1.5517, Valid loss: 1.7575, L1 loss: 1.7974\n",
            "Epoch [836/30000]: Train loss: 1.5457, Valid loss: 1.7650, L1 loss: 1.8052\n",
            "Epoch [837/30000]: Train loss: 1.5468, Valid loss: 1.7627, L1 loss: 1.8045\n",
            "Epoch [838/30000]: Train loss: 1.5590, Valid loss: 1.7682, L1 loss: 1.8107\n",
            "Epoch [839/30000]: Train loss: 1.5497, Valid loss: 1.7845, L1 loss: 1.8198\n",
            "Epoch [840/30000]: Train loss: 1.5603, Valid loss: 1.7748, L1 loss: 1.8091\n",
            "Epoch [841/30000]: Train loss: 1.5496, Valid loss: 1.7932, L1 loss: 1.8238\n",
            "Epoch [842/30000]: Train loss: 1.5642, Valid loss: 1.7907, L1 loss: 1.8246\n",
            "Epoch [843/30000]: Train loss: 1.5705, Valid loss: 1.7769, L1 loss: 1.8125\n",
            "Epoch [844/30000]: Train loss: 1.5687, Valid loss: 1.7822, L1 loss: 1.8201\n",
            "Epoch [845/30000]: Train loss: 1.5571, Valid loss: 1.7909, L1 loss: 1.8248\n",
            "Epoch [846/30000]: Train loss: 1.5527, Valid loss: 1.7455, L1 loss: 1.7873\n",
            "Epoch [847/30000]: Train loss: 1.5580, Valid loss: 1.7566, L1 loss: 1.7950\n",
            "Epoch [848/30000]: Train loss: 1.5454, Valid loss: 1.7541, L1 loss: 1.7955\n",
            "Epoch [849/30000]: Train loss: 1.5548, Valid loss: 1.7647, L1 loss: 1.8033\n",
            "Epoch [850/30000]: Train loss: 1.5592, Valid loss: 1.7554, L1 loss: 1.7974\n",
            "Epoch [851/30000]: Train loss: 1.5653, Valid loss: 1.7721, L1 loss: 1.8134\n",
            "Epoch [852/30000]: Train loss: 1.5561, Valid loss: 1.7698, L1 loss: 1.8121\n",
            "Epoch [853/30000]: Train loss: 1.5471, Valid loss: 1.7562, L1 loss: 1.7932\n",
            "Epoch [854/30000]: Train loss: 1.5508, Valid loss: 1.7768, L1 loss: 1.8081\n",
            "Epoch [855/30000]: Train loss: 1.5676, Valid loss: 1.7587, L1 loss: 1.7967\n",
            "Epoch [856/30000]: Train loss: 1.5569, Valid loss: 1.7933, L1 loss: 1.8230\n",
            "Epoch [857/30000]: Train loss: 1.5696, Valid loss: 1.7921, L1 loss: 1.8256\n",
            "Epoch [858/30000]: Train loss: 1.5591, Valid loss: 1.7822, L1 loss: 1.8184\n",
            "Epoch [859/30000]: Train loss: 1.5444, Valid loss: 1.7753, L1 loss: 1.8123\n",
            "Epoch [860/30000]: Train loss: 1.5556, Valid loss: 1.8016, L1 loss: 1.8371\n",
            "Epoch [861/30000]: Train loss: 1.5538, Valid loss: 1.7817, L1 loss: 1.8185\n",
            "Epoch [862/30000]: Train loss: 1.5526, Valid loss: 1.8164, L1 loss: 1.8434\n",
            "Epoch [863/30000]: Train loss: 1.5545, Valid loss: 1.7742, L1 loss: 1.8128\n",
            "Epoch [864/30000]: Train loss: 1.5584, Valid loss: 1.7869, L1 loss: 1.8228\n",
            "Epoch [865/30000]: Train loss: 1.5542, Valid loss: 1.8309, L1 loss: 1.8619\n",
            "Epoch [866/30000]: Train loss: 1.5496, Valid loss: 1.8111, L1 loss: 1.8399\n",
            "Epoch [867/30000]: Train loss: 1.5598, Valid loss: 1.7627, L1 loss: 1.8028\n",
            "Epoch [868/30000]: Train loss: 1.5603, Valid loss: 1.7748, L1 loss: 1.8126\n",
            "Epoch [869/30000]: Train loss: 1.5497, Valid loss: 1.7986, L1 loss: 1.8328\n",
            "Epoch [870/30000]: Train loss: 1.5504, Valid loss: 1.7749, L1 loss: 1.8090\n",
            "Epoch [871/30000]: Train loss: 1.5510, Valid loss: 1.8235, L1 loss: 1.8564\n",
            "Epoch [872/30000]: Train loss: 1.5564, Valid loss: 1.7898, L1 loss: 1.8223\n",
            "Epoch [873/30000]: Train loss: 1.5504, Valid loss: 1.7768, L1 loss: 1.8133\n",
            "Epoch [874/30000]: Train loss: 1.5415, Valid loss: 1.7646, L1 loss: 1.8002\n",
            "Epoch [875/30000]: Train loss: 1.5616, Valid loss: 1.7483, L1 loss: 1.7841\n",
            "Epoch [876/30000]: Train loss: 1.5558, Valid loss: 1.7614, L1 loss: 1.7970\n",
            "Epoch [877/30000]: Train loss: 1.5612, Valid loss: 1.7714, L1 loss: 1.8048\n",
            "Epoch [878/30000]: Train loss: 1.5642, Valid loss: 1.7722, L1 loss: 1.8068\n",
            "Epoch [879/30000]: Train loss: 1.5598, Valid loss: 1.7669, L1 loss: 1.7968\n",
            "Epoch [880/30000]: Train loss: 1.5476, Valid loss: 1.7721, L1 loss: 1.8126\n",
            "Epoch [881/30000]: Train loss: 1.5615, Valid loss: 1.7783, L1 loss: 1.8188\n",
            "Epoch [882/30000]: Train loss: 1.5568, Valid loss: 1.7743, L1 loss: 1.8180\n",
            "Epoch [883/30000]: Train loss: 1.5565, Valid loss: 1.7959, L1 loss: 1.8313\n",
            "Epoch [884/30000]: Train loss: 1.5633, Valid loss: 1.7987, L1 loss: 1.8336\n",
            "Epoch [885/30000]: Train loss: 1.5522, Valid loss: 1.8010, L1 loss: 1.8275\n",
            "Epoch [886/30000]: Train loss: 1.5456, Valid loss: 1.8104, L1 loss: 1.8361\n",
            "Epoch [887/30000]: Train loss: 1.5648, Valid loss: 1.7725, L1 loss: 1.8057\n",
            "Epoch [888/30000]: Train loss: 1.5515, Valid loss: 1.7670, L1 loss: 1.8057\n",
            "Epoch [889/30000]: Train loss: 1.5474, Valid loss: 1.7838, L1 loss: 1.8207\n",
            "Epoch [890/30000]: Train loss: 1.5518, Valid loss: 1.7823, L1 loss: 1.8159\n",
            "Epoch [891/30000]: Train loss: 1.5622, Valid loss: 1.8051, L1 loss: 1.8333\n",
            "Epoch [892/30000]: Train loss: 1.5558, Valid loss: 1.8142, L1 loss: 1.8424\n",
            "Epoch [893/30000]: Train loss: 1.5566, Valid loss: 1.8126, L1 loss: 1.8418\n",
            "Epoch [894/30000]: Train loss: 1.5552, Valid loss: 1.7795, L1 loss: 1.8141\n",
            "Epoch [895/30000]: Train loss: 1.5608, Valid loss: 1.7908, L1 loss: 1.8292\n",
            "Epoch [896/30000]: Train loss: 1.5484, Valid loss: 1.7785, L1 loss: 1.8177\n",
            "Epoch [897/30000]: Train loss: 1.5329, Valid loss: 1.8052, L1 loss: 1.8314\n",
            "Epoch [898/30000]: Train loss: 1.5369, Valid loss: 1.7863, L1 loss: 1.8232\n",
            "Epoch [899/30000]: Train loss: 1.5567, Valid loss: 1.8184, L1 loss: 1.8477\n",
            "Epoch [900/30000]: Train loss: 1.5782, Valid loss: 1.8088, L1 loss: 1.8416\n",
            "Epoch [901/30000]: Train loss: 1.5443, Valid loss: 1.7571, L1 loss: 1.7952\n",
            "Epoch [902/30000]: Train loss: 1.5564, Valid loss: 1.7525, L1 loss: 1.7868\n",
            "Epoch [903/30000]: Train loss: 1.5506, Valid loss: 1.7775, L1 loss: 1.8115\n",
            "Epoch [904/30000]: Train loss: 1.5524, Valid loss: 1.7778, L1 loss: 1.8115\n",
            "Epoch [905/30000]: Train loss: 1.5550, Valid loss: 1.7964, L1 loss: 1.8272\n",
            "Epoch [906/30000]: Train loss: 1.5601, Valid loss: 1.7717, L1 loss: 1.8091\n",
            "Epoch [907/30000]: Train loss: 1.5593, Valid loss: 1.7794, L1 loss: 1.8139\n",
            "Epoch [908/30000]: Train loss: 1.5538, Valid loss: 1.7955, L1 loss: 1.8271\n",
            "Epoch [909/30000]: Train loss: 1.5460, Valid loss: 1.8015, L1 loss: 1.8324\n",
            "Epoch [910/30000]: Train loss: 1.5425, Valid loss: 1.8034, L1 loss: 1.8260\n",
            "Epoch [911/30000]: Train loss: 1.5504, Valid loss: 1.7717, L1 loss: 1.8047\n",
            "Epoch [912/30000]: Train loss: 1.5654, Valid loss: 1.8432, L1 loss: 1.8644\n",
            "Epoch [913/30000]: Train loss: 1.5570, Valid loss: 1.8098, L1 loss: 1.8418\n",
            "Epoch [914/30000]: Train loss: 1.5518, Valid loss: 1.7677, L1 loss: 1.8063\n",
            "Epoch [915/30000]: Train loss: 1.5535, Valid loss: 1.7549, L1 loss: 1.7960\n",
            "Epoch [916/30000]: Train loss: 1.5435, Valid loss: 1.7731, L1 loss: 1.8108\n",
            "Epoch [917/30000]: Train loss: 1.5333, Valid loss: 1.7623, L1 loss: 1.8031\n",
            "Epoch [918/30000]: Train loss: 1.5583, Valid loss: 1.7582, L1 loss: 1.7953\n",
            "Epoch [919/30000]: Train loss: 1.5582, Valid loss: 1.7845, L1 loss: 1.8204\n",
            "Epoch [920/30000]: Train loss: 1.5480, Valid loss: 1.7878, L1 loss: 1.8147\n",
            "Epoch [921/30000]: Train loss: 1.5562, Valid loss: 1.7695, L1 loss: 1.8015\n",
            "Epoch [922/30000]: Train loss: 1.5494, Valid loss: 1.7925, L1 loss: 1.8168\n",
            "Epoch [923/30000]: Train loss: 1.5525, Valid loss: 1.7757, L1 loss: 1.8059\n",
            "Epoch [924/30000]: Train loss: 1.5576, Valid loss: 1.8053, L1 loss: 1.8317\n",
            "Epoch [925/30000]: Train loss: 1.5651, Valid loss: 1.7841, L1 loss: 1.8156\n",
            "Epoch [926/30000]: Train loss: 1.5489, Valid loss: 1.7724, L1 loss: 1.8094\n",
            "Epoch [927/30000]: Train loss: 1.5541, Valid loss: 1.7887, L1 loss: 1.8205\n",
            "Epoch [928/30000]: Train loss: 1.5434, Valid loss: 1.7646, L1 loss: 1.8012\n",
            "Epoch [929/30000]: Train loss: 1.5468, Valid loss: 1.8077, L1 loss: 1.8353\n",
            "Epoch [930/30000]: Train loss: 1.5562, Valid loss: 1.7669, L1 loss: 1.8096\n",
            "Epoch [931/30000]: Train loss: 1.5567, Valid loss: 1.7948, L1 loss: 1.8309\n",
            "Epoch [932/30000]: Train loss: 1.5581, Valid loss: 1.8155, L1 loss: 1.8511\n",
            "Epoch [933/30000]: Train loss: 1.5621, Valid loss: 1.8051, L1 loss: 1.8447\n",
            "Epoch [934/30000]: Train loss: 1.5633, Valid loss: 1.8053, L1 loss: 1.8350\n",
            "Epoch [935/30000]: Train loss: 1.5591, Valid loss: 1.8153, L1 loss: 1.8417\n",
            "Epoch [936/30000]: Train loss: 1.5541, Valid loss: 1.7961, L1 loss: 1.8257\n",
            "Epoch [937/30000]: Train loss: 1.5518, Valid loss: 1.8111, L1 loss: 1.8402\n",
            "Epoch [938/30000]: Train loss: 1.5524, Valid loss: 1.8306, L1 loss: 1.8584\n",
            "Epoch [939/30000]: Train loss: 1.5633, Valid loss: 1.7875, L1 loss: 1.8190\n",
            "Epoch [940/30000]: Train loss: 1.5503, Valid loss: 1.8338, L1 loss: 1.8608\n",
            "Epoch [941/30000]: Train loss: 1.5377, Valid loss: 1.7458, L1 loss: 1.7843\n",
            "Epoch [942/30000]: Train loss: 1.5490, Valid loss: 1.7953, L1 loss: 1.8296\n",
            "Epoch [943/30000]: Train loss: 1.5498, Valid loss: 1.7608, L1 loss: 1.8010\n",
            "Epoch [944/30000]: Train loss: 1.5553, Valid loss: 1.7958, L1 loss: 1.8336\n",
            "Epoch [945/30000]: Train loss: 1.5601, Valid loss: 1.7821, L1 loss: 1.8139\n",
            "Epoch [946/30000]: Train loss: 1.5385, Valid loss: 1.8023, L1 loss: 1.8336\n",
            "Epoch [947/30000]: Train loss: 1.5706, Valid loss: 1.7957, L1 loss: 1.8315\n",
            "Epoch [948/30000]: Train loss: 1.5493, Valid loss: 1.7906, L1 loss: 1.8195\n",
            "Epoch [949/30000]: Train loss: 1.5557, Valid loss: 1.7878, L1 loss: 1.8201\n",
            "Epoch [950/30000]: Train loss: 1.5401, Valid loss: 1.8005, L1 loss: 1.8284\n",
            "Epoch [951/30000]: Train loss: 1.5497, Valid loss: 1.7672, L1 loss: 1.8032\n",
            "Epoch [952/30000]: Train loss: 1.5439, Valid loss: 1.7875, L1 loss: 1.8213\n",
            "Epoch [953/30000]: Train loss: 1.5565, Valid loss: 1.7715, L1 loss: 1.8078\n",
            "Epoch [954/30000]: Train loss: 1.5616, Valid loss: 1.8035, L1 loss: 1.8325\n",
            "Epoch [955/30000]: Train loss: 1.5464, Valid loss: 1.7596, L1 loss: 1.7964\n",
            "Epoch [956/30000]: Train loss: 1.5475, Valid loss: 1.7764, L1 loss: 1.8102\n",
            "Epoch [957/30000]: Train loss: 1.5512, Valid loss: 1.8076, L1 loss: 1.8408\n",
            "Epoch [958/30000]: Train loss: 1.5647, Valid loss: 1.7811, L1 loss: 1.8121\n",
            "Epoch [959/30000]: Train loss: 1.5461, Valid loss: 1.8137, L1 loss: 1.8432\n",
            "Epoch [960/30000]: Train loss: 1.5565, Valid loss: 1.7985, L1 loss: 1.8288\n",
            "Epoch [961/30000]: Train loss: 1.5580, Valid loss: 1.8146, L1 loss: 1.8417\n",
            "Epoch [962/30000]: Train loss: 1.5523, Valid loss: 1.7824, L1 loss: 1.8161\n",
            "Epoch [963/30000]: Train loss: 1.5537, Valid loss: 1.7989, L1 loss: 1.8285\n",
            "Epoch [964/30000]: Train loss: 1.5494, Valid loss: 1.7984, L1 loss: 1.8274\n",
            "Epoch [965/30000]: Train loss: 1.5398, Valid loss: 1.8214, L1 loss: 1.8459\n",
            "Epoch [966/30000]: Train loss: 1.5561, Valid loss: 1.8058, L1 loss: 1.8328\n",
            "Epoch [967/30000]: Train loss: 1.5501, Valid loss: 1.7865, L1 loss: 1.8210\n",
            "Epoch [968/30000]: Train loss: 1.5509, Valid loss: 1.7929, L1 loss: 1.8196\n",
            "Epoch [969/30000]: Train loss: 1.5523, Valid loss: 1.8006, L1 loss: 1.8271\n",
            "Epoch [970/30000]: Train loss: 1.5482, Valid loss: 1.7742, L1 loss: 1.8064\n",
            "Epoch [971/30000]: Train loss: 1.5431, Valid loss: 1.7934, L1 loss: 1.8186\n",
            "Epoch [972/30000]: Train loss: 1.5656, Valid loss: 1.7915, L1 loss: 1.8215\n",
            "Epoch [973/30000]: Train loss: 1.5618, Valid loss: 1.8009, L1 loss: 1.8335\n",
            "Epoch [974/30000]: Train loss: 1.5516, Valid loss: 1.7842, L1 loss: 1.8155\n",
            "Epoch [975/30000]: Train loss: 1.5612, Valid loss: 1.7799, L1 loss: 1.8142\n",
            "Epoch [976/30000]: Train loss: 1.5358, Valid loss: 1.7850, L1 loss: 1.8134\n",
            "Epoch [977/30000]: Train loss: 1.5526, Valid loss: 1.7827, L1 loss: 1.8133\n",
            "Epoch [978/30000]: Train loss: 1.5520, Valid loss: 1.8165, L1 loss: 1.8364\n",
            "Epoch [979/30000]: Train loss: 1.5527, Valid loss: 1.7825, L1 loss: 1.8111\n",
            "Epoch [980/30000]: Train loss: 1.5615, Valid loss: 1.7772, L1 loss: 1.8085\n",
            "Epoch [981/30000]: Train loss: 1.5524, Valid loss: 1.7605, L1 loss: 1.7947\n",
            "Epoch [982/30000]: Train loss: 1.5379, Valid loss: 1.7773, L1 loss: 1.8122\n",
            "Epoch [983/30000]: Train loss: 1.5495, Valid loss: 1.7799, L1 loss: 1.8104\n",
            "Epoch [984/30000]: Train loss: 1.5615, Valid loss: 1.7654, L1 loss: 1.8010\n",
            "Epoch [985/30000]: Train loss: 1.5553, Valid loss: 1.7798, L1 loss: 1.8062\n",
            "Epoch [986/30000]: Train loss: 1.5389, Valid loss: 1.8180, L1 loss: 1.8437\n",
            "Epoch [987/30000]: Train loss: 1.5661, Valid loss: 1.8302, L1 loss: 1.8458\n",
            "Epoch [988/30000]: Train loss: 1.5436, Valid loss: 1.8005, L1 loss: 1.8216\n",
            "Epoch [989/30000]: Train loss: 1.5498, Valid loss: 1.7704, L1 loss: 1.8026\n",
            "Epoch [990/30000]: Train loss: 1.5569, Valid loss: 1.8138, L1 loss: 1.8362\n",
            "Epoch [991/30000]: Train loss: 1.5524, Valid loss: 1.7888, L1 loss: 1.8145\n",
            "Epoch [992/30000]: Train loss: 1.5481, Valid loss: 1.8297, L1 loss: 1.8496\n",
            "Epoch [993/30000]: Train loss: 1.5570, Valid loss: 1.8220, L1 loss: 1.8429\n",
            "Epoch [994/30000]: Train loss: 1.5572, Valid loss: 1.7520, L1 loss: 1.7852\n",
            "Epoch [995/30000]: Train loss: 1.5478, Valid loss: 1.8058, L1 loss: 1.8235\n",
            "Epoch [996/30000]: Train loss: 1.5507, Valid loss: 1.8087, L1 loss: 1.8322\n",
            "Epoch [997/30000]: Train loss: 1.5532, Valid loss: 1.8328, L1 loss: 1.8496\n",
            "Epoch [998/30000]: Train loss: 1.5589, Valid loss: 1.7455, L1 loss: 1.7846\n",
            "Epoch [999/30000]: Train loss: 1.5363, Valid loss: 1.8243, L1 loss: 1.8452\n",
            "Epoch [1000/30000]: Train loss: 1.5528, Valid loss: 1.7799, L1 loss: 1.8085\n",
            "Epoch [1001/30000]: Train loss: 1.5484, Valid loss: 1.8207, L1 loss: 1.8392\n",
            "Epoch [1002/30000]: Train loss: 1.5468, Valid loss: 1.7667, L1 loss: 1.7963\n",
            "Epoch [1003/30000]: Train loss: 1.5476, Valid loss: 1.7942, L1 loss: 1.8180\n",
            "Epoch [1004/30000]: Train loss: 1.5628, Valid loss: 1.8308, L1 loss: 1.8514\n",
            "Epoch [1005/30000]: Train loss: 1.5505, Valid loss: 1.8024, L1 loss: 1.8325\n",
            "Epoch [1006/30000]: Train loss: 1.5478, Valid loss: 1.8208, L1 loss: 1.8445\n",
            "Epoch [1007/30000]: Train loss: 1.5433, Valid loss: 1.8286, L1 loss: 1.8528\n",
            "Epoch [1008/30000]: Train loss: 1.5520, Valid loss: 1.7981, L1 loss: 1.8260\n",
            "Epoch [1009/30000]: Train loss: 1.5545, Valid loss: 1.8147, L1 loss: 1.8364\n",
            "Epoch [1010/30000]: Train loss: 1.5469, Valid loss: 1.8057, L1 loss: 1.8333\n",
            "Epoch [1011/30000]: Train loss: 1.5605, Valid loss: 1.7952, L1 loss: 1.8259\n",
            "Epoch [1012/30000]: Train loss: 1.5567, Valid loss: 1.8085, L1 loss: 1.8307\n",
            "Epoch [1013/30000]: Train loss: 1.5525, Valid loss: 1.8182, L1 loss: 1.8465\n",
            "Epoch [1014/30000]: Train loss: 1.5398, Valid loss: 1.8146, L1 loss: 1.8356\n",
            "Epoch [1015/30000]: Train loss: 1.5591, Valid loss: 1.8088, L1 loss: 1.8341\n",
            "Epoch [1016/30000]: Train loss: 1.5480, Valid loss: 1.8365, L1 loss: 1.8613\n",
            "Epoch [1017/30000]: Train loss: 1.5546, Valid loss: 1.8579, L1 loss: 1.8772\n",
            "Epoch [1018/30000]: Train loss: 1.5559, Valid loss: 1.7877, L1 loss: 1.8248\n",
            "Epoch [1019/30000]: Train loss: 1.5402, Valid loss: 1.7956, L1 loss: 1.8239\n",
            "Epoch [1020/30000]: Train loss: 1.5488, Valid loss: 1.7835, L1 loss: 1.8154\n",
            "Epoch [1021/30000]: Train loss: 1.5492, Valid loss: 1.8060, L1 loss: 1.8320\n",
            "Epoch [1022/30000]: Train loss: 1.5447, Valid loss: 1.8077, L1 loss: 1.8390\n",
            "Epoch [1023/30000]: Train loss: 1.5547, Valid loss: 1.7891, L1 loss: 1.8192\n",
            "Epoch [1024/30000]: Train loss: 1.5618, Valid loss: 1.8203, L1 loss: 1.8408\n",
            "Epoch [1025/30000]: Train loss: 1.5464, Valid loss: 1.8262, L1 loss: 1.8497\n",
            "Epoch [1026/30000]: Train loss: 1.5516, Valid loss: 1.7996, L1 loss: 1.8318\n",
            "Epoch [1027/30000]: Train loss: 1.5525, Valid loss: 1.7892, L1 loss: 1.8178\n",
            "Epoch [1028/30000]: Train loss: 1.5583, Valid loss: 1.7976, L1 loss: 1.8244\n",
            "Epoch [1029/30000]: Train loss: 1.5472, Valid loss: 1.7672, L1 loss: 1.8004\n",
            "Epoch [1030/30000]: Train loss: 1.5539, Valid loss: 1.8087, L1 loss: 1.8397\n",
            "Epoch [1031/30000]: Train loss: 1.5480, Valid loss: 1.7603, L1 loss: 1.7993\n",
            "Epoch [1032/30000]: Train loss: 1.5403, Valid loss: 1.7576, L1 loss: 1.7939\n",
            "Epoch [1033/30000]: Train loss: 1.5595, Valid loss: 1.8035, L1 loss: 1.8299\n",
            "Epoch [1034/30000]: Train loss: 1.5451, Valid loss: 1.7911, L1 loss: 1.8178\n",
            "Epoch [1035/30000]: Train loss: 1.5499, Valid loss: 1.8011, L1 loss: 1.8282\n",
            "Epoch [1036/30000]: Train loss: 1.5618, Valid loss: 1.7887, L1 loss: 1.8223\n",
            "Epoch [1037/30000]: Train loss: 1.5502, Valid loss: 1.7720, L1 loss: 1.8036\n",
            "Epoch [1038/30000]: Train loss: 1.5598, Valid loss: 1.7870, L1 loss: 1.8185\n",
            "Epoch [1039/30000]: Train loss: 1.5405, Valid loss: 1.7921, L1 loss: 1.8152\n",
            "Epoch [1040/30000]: Train loss: 1.5430, Valid loss: 1.7843, L1 loss: 1.8073\n",
            "Epoch [1041/30000]: Train loss: 1.5411, Valid loss: 1.7803, L1 loss: 1.8099\n",
            "Epoch [1042/30000]: Train loss: 1.5467, Valid loss: 1.8510, L1 loss: 1.8722\n",
            "Epoch [1043/30000]: Train loss: 1.5503, Valid loss: 1.7760, L1 loss: 1.8074\n",
            "Epoch [1044/30000]: Train loss: 1.5521, Valid loss: 1.7941, L1 loss: 1.8180\n",
            "Epoch [1045/30000]: Train loss: 1.5507, Valid loss: 1.7871, L1 loss: 1.8127\n",
            "Epoch [1046/30000]: Train loss: 1.5497, Valid loss: 1.7980, L1 loss: 1.8201\n",
            "Epoch [1047/30000]: Train loss: 1.5467, Valid loss: 1.8215, L1 loss: 1.8386\n",
            "Epoch [1048/30000]: Train loss: 1.5492, Valid loss: 1.7757, L1 loss: 1.8013\n",
            "Epoch [1049/30000]: Train loss: 1.5676, Valid loss: 1.7713, L1 loss: 1.7990\n",
            "Epoch [1050/30000]: Train loss: 1.5613, Valid loss: 1.7573, L1 loss: 1.7873\n",
            "Epoch [1051/30000]: Train loss: 1.5523, Valid loss: 1.7754, L1 loss: 1.8006\n",
            "Epoch [1052/30000]: Train loss: 1.5468, Valid loss: 1.7767, L1 loss: 1.8028\n",
            "Epoch [1053/30000]: Train loss: 1.5518, Valid loss: 1.7800, L1 loss: 1.8020\n",
            "Epoch [1054/30000]: Train loss: 1.5505, Valid loss: 1.8007, L1 loss: 1.8255\n",
            "Epoch [1055/30000]: Train loss: 1.5560, Valid loss: 1.7817, L1 loss: 1.8046\n",
            "Epoch [1056/30000]: Train loss: 1.5468, Valid loss: 1.7784, L1 loss: 1.8044\n",
            "Epoch [1057/30000]: Train loss: 1.5584, Valid loss: 1.7778, L1 loss: 1.8101\n",
            "Epoch [1058/30000]: Train loss: 1.5533, Valid loss: 1.7867, L1 loss: 1.8114\n",
            "Epoch [1059/30000]: Train loss: 1.5502, Valid loss: 1.7790, L1 loss: 1.8092\n",
            "Epoch [1060/30000]: Train loss: 1.5346, Valid loss: 1.8235, L1 loss: 1.8444\n",
            "Epoch [1061/30000]: Train loss: 1.5541, Valid loss: 1.8136, L1 loss: 1.8348\n",
            "Epoch [1062/30000]: Train loss: 1.5612, Valid loss: 1.8354, L1 loss: 1.8548\n",
            "Epoch [1063/30000]: Train loss: 1.5496, Valid loss: 1.8177, L1 loss: 1.8413\n",
            "Epoch [1064/30000]: Train loss: 1.5490, Valid loss: 1.8132, L1 loss: 1.8317\n",
            "Epoch [1065/30000]: Train loss: 1.5466, Valid loss: 1.8102, L1 loss: 1.8370\n",
            "Epoch [1066/30000]: Train loss: 1.5529, Valid loss: 1.8186, L1 loss: 1.8441\n",
            "Epoch [1067/30000]: Train loss: 1.5597, Valid loss: 1.8021, L1 loss: 1.8273\n",
            "Epoch [1068/30000]: Train loss: 1.5534, Valid loss: 1.8237, L1 loss: 1.8360\n",
            "Epoch [1069/30000]: Train loss: 1.5639, Valid loss: 1.8171, L1 loss: 1.8378\n",
            "Epoch [1070/30000]: Train loss: 1.5492, Valid loss: 1.8329, L1 loss: 1.8526\n",
            "Epoch [1071/30000]: Train loss: 1.5603, Valid loss: 1.7859, L1 loss: 1.8161\n",
            "Epoch [1072/30000]: Train loss: 1.5584, Valid loss: 1.7769, L1 loss: 1.8100\n",
            "Epoch [1073/30000]: Train loss: 1.5400, Valid loss: 1.8016, L1 loss: 1.8240\n",
            "Epoch [1074/30000]: Train loss: 1.5587, Valid loss: 1.8374, L1 loss: 1.8592\n",
            "Epoch [1075/30000]: Train loss: 1.5429, Valid loss: 1.8075, L1 loss: 1.8288\n",
            "Epoch [1076/30000]: Train loss: 1.5439, Valid loss: 1.8212, L1 loss: 1.8399\n",
            "Epoch [1077/30000]: Train loss: 1.5411, Valid loss: 1.8425, L1 loss: 1.8585\n",
            "Epoch [1078/30000]: Train loss: 1.5527, Valid loss: 1.8112, L1 loss: 1.8329\n",
            "Epoch [1079/30000]: Train loss: 1.5516, Valid loss: 1.7906, L1 loss: 1.8130\n",
            "Epoch [1080/30000]: Train loss: 1.5509, Valid loss: 1.8029, L1 loss: 1.8242\n",
            "Epoch [1081/30000]: Train loss: 1.5492, Valid loss: 1.8147, L1 loss: 1.8413\n",
            "Epoch [1082/30000]: Train loss: 1.5532, Valid loss: 1.8060, L1 loss: 1.8321\n",
            "Epoch [1083/30000]: Train loss: 1.5541, Valid loss: 1.8336, L1 loss: 1.8516\n",
            "Epoch [1084/30000]: Train loss: 1.5519, Valid loss: 1.8206, L1 loss: 1.8395\n",
            "Epoch [1085/30000]: Train loss: 1.5591, Valid loss: 1.7849, L1 loss: 1.8022\n",
            "Epoch [1086/30000]: Train loss: 1.5548, Valid loss: 1.8192, L1 loss: 1.8338\n",
            "Epoch [1087/30000]: Train loss: 1.5526, Valid loss: 1.8022, L1 loss: 1.8253\n",
            "Epoch [1088/30000]: Train loss: 1.5613, Valid loss: 1.8257, L1 loss: 1.8413\n",
            "Epoch [1089/30000]: Train loss: 1.5517, Valid loss: 1.8348, L1 loss: 1.8481\n",
            "Epoch [1090/30000]: Train loss: 1.5479, Valid loss: 1.8127, L1 loss: 1.8304\n",
            "Epoch [1091/30000]: Train loss: 1.5639, Valid loss: 1.7918, L1 loss: 1.8197\n",
            "Epoch [1092/30000]: Train loss: 1.5475, Valid loss: 1.8328, L1 loss: 1.8526\n",
            "Epoch [1093/30000]: Train loss: 1.5640, Valid loss: 1.8111, L1 loss: 1.8298\n",
            "Epoch [1094/30000]: Train loss: 1.5489, Valid loss: 1.8068, L1 loss: 1.8326\n",
            "Epoch [1095/30000]: Train loss: 1.5461, Valid loss: 1.8223, L1 loss: 1.8424\n",
            "Epoch [1096/30000]: Train loss: 1.5583, Valid loss: 1.7985, L1 loss: 1.8204\n",
            "Epoch [1097/30000]: Train loss: 1.5498, Valid loss: 1.8143, L1 loss: 1.8309\n",
            "Epoch [1098/30000]: Train loss: 1.5517, Valid loss: 1.7918, L1 loss: 1.8143\n",
            "Epoch [1099/30000]: Train loss: 1.5542, Valid loss: 1.8143, L1 loss: 1.8345\n",
            "Epoch [1100/30000]: Train loss: 1.5542, Valid loss: 1.8383, L1 loss: 1.8616\n",
            "Epoch [1101/30000]: Train loss: 1.5471, Valid loss: 1.7896, L1 loss: 1.8141\n",
            "Epoch [1102/30000]: Train loss: 1.5637, Valid loss: 1.8240, L1 loss: 1.8405\n",
            "Epoch [1103/30000]: Train loss: 1.5423, Valid loss: 1.8263, L1 loss: 1.8479\n",
            "Epoch [1104/30000]: Train loss: 1.5545, Valid loss: 1.7768, L1 loss: 1.8005\n",
            "Epoch [1105/30000]: Train loss: 1.5551, Valid loss: 1.8251, L1 loss: 1.8471\n",
            "Epoch [1106/30000]: Train loss: 1.5559, Valid loss: 1.8002, L1 loss: 1.8255\n",
            "Epoch [1107/30000]: Train loss: 1.5425, Valid loss: 1.8161, L1 loss: 1.8388\n",
            "Epoch [1108/30000]: Train loss: 1.5478, Valid loss: 1.8328, L1 loss: 1.8501\n",
            "Epoch [1109/30000]: Train loss: 1.5436, Valid loss: 1.8362, L1 loss: 1.8524\n",
            "Epoch [1110/30000]: Train loss: 1.5531, Valid loss: 1.7843, L1 loss: 1.8136\n",
            "Epoch [1111/30000]: Train loss: 1.5506, Valid loss: 1.7762, L1 loss: 1.8041\n",
            "Epoch [1112/30000]: Train loss: 1.5324, Valid loss: 1.8162, L1 loss: 1.8409\n",
            "Epoch [1113/30000]: Train loss: 1.5658, Valid loss: 1.7746, L1 loss: 1.8084\n",
            "Epoch [1114/30000]: Train loss: 1.5499, Valid loss: 1.8177, L1 loss: 1.8460\n",
            "Epoch [1115/30000]: Train loss: 1.5523, Valid loss: 1.8100, L1 loss: 1.8389\n",
            "Epoch [1116/30000]: Train loss: 1.5504, Valid loss: 1.8073, L1 loss: 1.8365\n",
            "Epoch [1117/30000]: Train loss: 1.5522, Valid loss: 1.8086, L1 loss: 1.8286\n",
            "Epoch [1118/30000]: Train loss: 1.5559, Valid loss: 1.7794, L1 loss: 1.8033\n",
            "Epoch [1119/30000]: Train loss: 1.5489, Valid loss: 1.8231, L1 loss: 1.8399\n",
            "Epoch [1120/30000]: Train loss: 1.5494, Valid loss: 1.8158, L1 loss: 1.8364\n",
            "Epoch [1121/30000]: Train loss: 1.5536, Valid loss: 1.7916, L1 loss: 1.8125\n",
            "Epoch [1122/30000]: Train loss: 1.5342, Valid loss: 1.8241, L1 loss: 1.8426\n",
            "Epoch [1123/30000]: Train loss: 1.5596, Valid loss: 1.8057, L1 loss: 1.8266\n",
            "Epoch [1124/30000]: Train loss: 1.5305, Valid loss: 1.8372, L1 loss: 1.8527\n",
            "Epoch [1125/30000]: Train loss: 1.5483, Valid loss: 1.8246, L1 loss: 1.8388\n",
            "Epoch [1126/30000]: Train loss: 1.5584, Valid loss: 1.7733, L1 loss: 1.7981\n",
            "Epoch [1127/30000]: Train loss: 1.5504, Valid loss: 1.8012, L1 loss: 1.8177\n",
            "Epoch [1128/30000]: Train loss: 1.5506, Valid loss: 1.8085, L1 loss: 1.8293\n",
            "Epoch [1129/30000]: Train loss: 1.5540, Valid loss: 1.8013, L1 loss: 1.8279\n",
            "Epoch [1130/30000]: Train loss: 1.5432, Valid loss: 1.8131, L1 loss: 1.8353\n",
            "Epoch [1131/30000]: Train loss: 1.5447, Valid loss: 1.7873, L1 loss: 1.8128\n",
            "Epoch [1132/30000]: Train loss: 1.5496, Valid loss: 1.8057, L1 loss: 1.8359\n",
            "Epoch [1133/30000]: Train loss: 1.5493, Valid loss: 1.8083, L1 loss: 1.8285\n",
            "Epoch [1134/30000]: Train loss: 1.5518, Valid loss: 1.8259, L1 loss: 1.8446\n",
            "Epoch [1135/30000]: Train loss: 1.5395, Valid loss: 1.8001, L1 loss: 1.8234\n",
            "Epoch [1136/30000]: Train loss: 1.5586, Valid loss: 1.7712, L1 loss: 1.7993\n",
            "Epoch [1137/30000]: Train loss: 1.5566, Valid loss: 1.7740, L1 loss: 1.7993\n",
            "Epoch [1138/30000]: Train loss: 1.5386, Valid loss: 1.8047, L1 loss: 1.8229\n",
            "Epoch [1139/30000]: Train loss: 1.5490, Valid loss: 1.7494, L1 loss: 1.7811\n",
            "Epoch [1140/30000]: Train loss: 1.5615, Valid loss: 1.7756, L1 loss: 1.8071\n",
            "Epoch [1141/30000]: Train loss: 1.5604, Valid loss: 1.8001, L1 loss: 1.8179\n",
            "Epoch [1142/30000]: Train loss: 1.5413, Valid loss: 1.7928, L1 loss: 1.8153\n",
            "Epoch [1143/30000]: Train loss: 1.5542, Valid loss: 1.7991, L1 loss: 1.8235\n",
            "Epoch [1144/30000]: Train loss: 1.5424, Valid loss: 1.7758, L1 loss: 1.8017\n",
            "Epoch [1145/30000]: Train loss: 1.5521, Valid loss: 1.7903, L1 loss: 1.8137\n",
            "Epoch [1146/30000]: Train loss: 1.5543, Valid loss: 1.7806, L1 loss: 1.8053\n",
            "Epoch [1147/30000]: Train loss: 1.5446, Valid loss: 1.8087, L1 loss: 1.8324\n",
            "Epoch [1148/30000]: Train loss: 1.5487, Valid loss: 1.8048, L1 loss: 1.8235\n",
            "Epoch [1149/30000]: Train loss: 1.5466, Valid loss: 1.8072, L1 loss: 1.8305\n",
            "Epoch [1150/30000]: Train loss: 1.5409, Valid loss: 1.8043, L1 loss: 1.8302\n",
            "Epoch [1151/30000]: Train loss: 1.5408, Valid loss: 1.7803, L1 loss: 1.8107\n",
            "Epoch [1152/30000]: Train loss: 1.5529, Valid loss: 1.7724, L1 loss: 1.7976\n",
            "Epoch [1153/30000]: Train loss: 1.5404, Valid loss: 1.8093, L1 loss: 1.8326\n",
            "Epoch [1154/30000]: Train loss: 1.5489, Valid loss: 1.7669, L1 loss: 1.7961\n",
            "Epoch [1155/30000]: Train loss: 1.5316, Valid loss: 1.7687, L1 loss: 1.7946\n",
            "Epoch [1156/30000]: Train loss: 1.5557, Valid loss: 1.7730, L1 loss: 1.7914\n",
            "Epoch [1157/30000]: Train loss: 1.5424, Valid loss: 1.7719, L1 loss: 1.7939\n",
            "Epoch [1158/30000]: Train loss: 1.5515, Valid loss: 1.7468, L1 loss: 1.7839\n",
            "Epoch [1159/30000]: Train loss: 1.5553, Valid loss: 1.7544, L1 loss: 1.7809\n",
            "Epoch [1160/30000]: Train loss: 1.5396, Valid loss: 1.7636, L1 loss: 1.7949\n",
            "Epoch [1161/30000]: Train loss: 1.5492, Valid loss: 1.7659, L1 loss: 1.7983\n",
            "Epoch [1162/30000]: Train loss: 1.5444, Valid loss: 1.7818, L1 loss: 1.8088\n",
            "Epoch [1163/30000]: Train loss: 1.5405, Valid loss: 1.7780, L1 loss: 1.8056\n",
            "Epoch [1164/30000]: Train loss: 1.5488, Valid loss: 1.7570, L1 loss: 1.7820\n",
            "Epoch [1165/30000]: Train loss: 1.5487, Valid loss: 1.8109, L1 loss: 1.8316\n",
            "Epoch [1166/30000]: Train loss: 1.5506, Valid loss: 1.7811, L1 loss: 1.8053\n",
            "Epoch [1167/30000]: Train loss: 1.5526, Valid loss: 1.8058, L1 loss: 1.8239\n",
            "Epoch [1168/30000]: Train loss: 1.5429, Valid loss: 1.7911, L1 loss: 1.8162\n",
            "Epoch [1169/30000]: Train loss: 1.5467, Valid loss: 1.8001, L1 loss: 1.8202\n",
            "Epoch [1170/30000]: Train loss: 1.5448, Valid loss: 1.7830, L1 loss: 1.8084\n",
            "Epoch [1171/30000]: Train loss: 1.5583, Valid loss: 1.8283, L1 loss: 1.8531\n",
            "Epoch [1172/30000]: Train loss: 1.5364, Valid loss: 1.8190, L1 loss: 1.8447\n",
            "Epoch [1173/30000]: Train loss: 1.5408, Valid loss: 1.7854, L1 loss: 1.8153\n",
            "Epoch [1174/30000]: Train loss: 1.5487, Valid loss: 1.8073, L1 loss: 1.8310\n",
            "Epoch [1175/30000]: Train loss: 1.5425, Valid loss: 1.8036, L1 loss: 1.8268\n",
            "Epoch [1176/30000]: Train loss: 1.5423, Valid loss: 1.7984, L1 loss: 1.8226\n",
            "Epoch [1177/30000]: Train loss: 1.5376, Valid loss: 1.7742, L1 loss: 1.8029\n",
            "Epoch [1178/30000]: Train loss: 1.5371, Valid loss: 1.7824, L1 loss: 1.8073\n",
            "Epoch [1179/30000]: Train loss: 1.5574, Valid loss: 1.7993, L1 loss: 1.8211\n",
            "Epoch [1180/30000]: Train loss: 1.5540, Valid loss: 1.7605, L1 loss: 1.7938\n",
            "Epoch [1181/30000]: Train loss: 1.5442, Valid loss: 1.7961, L1 loss: 1.8187\n",
            "Epoch [1182/30000]: Train loss: 1.5452, Valid loss: 1.7462, L1 loss: 1.7752\n",
            "Epoch [1183/30000]: Train loss: 1.5457, Valid loss: 1.7574, L1 loss: 1.7894\n",
            "Epoch [1184/30000]: Train loss: 1.5410, Valid loss: 1.7512, L1 loss: 1.7822\n",
            "Epoch [1185/30000]: Train loss: 1.5396, Valid loss: 1.7584, L1 loss: 1.7886\n",
            "Epoch [1186/30000]: Train loss: 1.5592, Valid loss: 1.7626, L1 loss: 1.7935\n",
            "Epoch [1187/30000]: Train loss: 1.5520, Valid loss: 1.7545, L1 loss: 1.7829\n",
            "Epoch [1188/30000]: Train loss: 1.5438, Valid loss: 1.7582, L1 loss: 1.7857\n",
            "Epoch [1189/30000]: Train loss: 1.5565, Valid loss: 1.7943, L1 loss: 1.8157\n",
            "Epoch [1190/30000]: Train loss: 1.5442, Valid loss: 1.7751, L1 loss: 1.7994\n",
            "Epoch [1191/30000]: Train loss: 1.5500, Valid loss: 1.7656, L1 loss: 1.7883\n",
            "Epoch [1192/30000]: Train loss: 1.5494, Valid loss: 1.7961, L1 loss: 1.8137\n",
            "Epoch [1193/30000]: Train loss: 1.5375, Valid loss: 1.8202, L1 loss: 1.8382\n",
            "Epoch [1194/30000]: Train loss: 1.5476, Valid loss: 1.8032, L1 loss: 1.8234\n",
            "Epoch [1195/30000]: Train loss: 1.5453, Valid loss: 1.7747, L1 loss: 1.8025\n",
            "Epoch [1196/30000]: Train loss: 1.5436, Valid loss: 1.7699, L1 loss: 1.7987\n",
            "Epoch [1197/30000]: Train loss: 1.5397, Valid loss: 1.7725, L1 loss: 1.7997\n",
            "Epoch [1198/30000]: Train loss: 1.5424, Valid loss: 1.7790, L1 loss: 1.8093\n",
            "Epoch [1199/30000]: Train loss: 1.5488, Valid loss: 1.7806, L1 loss: 1.8075\n",
            "Epoch [1200/30000]: Train loss: 1.5391, Valid loss: 1.7304, L1 loss: 1.7601\n",
            "Epoch [1201/30000]: Train loss: 1.5465, Valid loss: 1.7517, L1 loss: 1.7772\n",
            "Epoch [1202/30000]: Train loss: 1.5527, Valid loss: 1.8014, L1 loss: 1.8155\n",
            "Epoch [1203/30000]: Train loss: 1.5436, Valid loss: 1.7500, L1 loss: 1.7715\n",
            "Epoch [1204/30000]: Train loss: 1.5442, Valid loss: 1.7555, L1 loss: 1.7811\n",
            "Epoch [1205/30000]: Train loss: 1.5546, Valid loss: 1.7857, L1 loss: 1.8071\n",
            "Epoch [1206/30000]: Train loss: 1.5405, Valid loss: 1.7717, L1 loss: 1.7911\n",
            "Epoch [1207/30000]: Train loss: 1.5466, Valid loss: 1.7475, L1 loss: 1.7756\n",
            "Epoch [1208/30000]: Train loss: 1.5506, Valid loss: 1.7747, L1 loss: 1.7913\n",
            "Epoch [1209/30000]: Train loss: 1.5450, Valid loss: 1.7744, L1 loss: 1.7938\n",
            "Epoch [1210/30000]: Train loss: 1.5441, Valid loss: 1.7487, L1 loss: 1.7764\n",
            "Epoch [1211/30000]: Train loss: 1.5435, Valid loss: 1.7666, L1 loss: 1.7904\n",
            "Epoch [1212/30000]: Train loss: 1.5526, Valid loss: 1.7760, L1 loss: 1.8009\n",
            "Epoch [1213/30000]: Train loss: 1.5476, Valid loss: 1.7701, L1 loss: 1.7958\n",
            "Epoch [1214/30000]: Train loss: 1.5408, Valid loss: 1.8093, L1 loss: 1.8249\n",
            "Epoch [1215/30000]: Train loss: 1.5553, Valid loss: 1.7528, L1 loss: 1.7811\n",
            "Epoch [1216/30000]: Train loss: 1.5413, Valid loss: 1.7646, L1 loss: 1.7951\n",
            "Epoch [1217/30000]: Train loss: 1.5496, Valid loss: 1.7790, L1 loss: 1.8053\n",
            "Epoch [1218/30000]: Train loss: 1.5530, Valid loss: 1.7830, L1 loss: 1.8100\n",
            "Epoch [1219/30000]: Train loss: 1.5526, Valid loss: 1.7772, L1 loss: 1.8089\n",
            "Epoch [1220/30000]: Train loss: 1.5586, Valid loss: 1.7607, L1 loss: 1.7893\n",
            "Epoch [1221/30000]: Train loss: 1.5440, Valid loss: 1.7563, L1 loss: 1.7863\n",
            "Epoch [1222/30000]: Train loss: 1.5445, Valid loss: 1.7616, L1 loss: 1.7897\n",
            "Epoch [1223/30000]: Train loss: 1.5382, Valid loss: 1.8167, L1 loss: 1.8346\n",
            "Epoch [1224/30000]: Train loss: 1.5493, Valid loss: 1.7613, L1 loss: 1.7921\n",
            "Epoch [1225/30000]: Train loss: 1.5504, Valid loss: 1.7667, L1 loss: 1.7897\n",
            "Epoch [1226/30000]: Train loss: 1.5428, Valid loss: 1.7687, L1 loss: 1.7997\n",
            "Epoch [1227/30000]: Train loss: 1.5495, Valid loss: 1.7981, L1 loss: 1.8262\n",
            "Epoch [1228/30000]: Train loss: 1.5443, Valid loss: 1.7786, L1 loss: 1.8042\n",
            "Epoch [1229/30000]: Train loss: 1.5396, Valid loss: 1.7951, L1 loss: 1.8210\n",
            "Epoch [1230/30000]: Train loss: 1.5639, Valid loss: 1.7571, L1 loss: 1.7878\n",
            "Epoch [1231/30000]: Train loss: 1.5543, Valid loss: 1.7542, L1 loss: 1.7830\n",
            "Epoch [1232/30000]: Train loss: 1.5497, Valid loss: 1.7831, L1 loss: 1.8061\n",
            "Epoch [1233/30000]: Train loss: 1.5438, Valid loss: 1.7753, L1 loss: 1.8011\n",
            "Epoch [1234/30000]: Train loss: 1.5427, Valid loss: 1.7660, L1 loss: 1.7915\n",
            "Epoch [1235/30000]: Train loss: 1.5537, Valid loss: 1.7628, L1 loss: 1.7900\n",
            "Epoch [1236/30000]: Train loss: 1.5495, Valid loss: 1.7652, L1 loss: 1.7922\n",
            "Epoch [1237/30000]: Train loss: 1.5558, Valid loss: 1.7597, L1 loss: 1.7832\n",
            "Epoch [1238/30000]: Train loss: 1.5529, Valid loss: 1.7661, L1 loss: 1.7864\n",
            "Epoch [1239/30000]: Train loss: 1.5491, Valid loss: 1.7697, L1 loss: 1.7987\n",
            "Epoch [1240/30000]: Train loss: 1.5609, Valid loss: 1.7859, L1 loss: 1.8099\n",
            "Epoch [1241/30000]: Train loss: 1.5502, Valid loss: 1.7776, L1 loss: 1.7993\n",
            "Epoch [1242/30000]: Train loss: 1.5491, Valid loss: 1.7871, L1 loss: 1.8055\n",
            "Epoch [1243/30000]: Train loss: 1.5503, Valid loss: 1.7870, L1 loss: 1.8111\n",
            "Epoch [1244/30000]: Train loss: 1.5477, Valid loss: 1.7690, L1 loss: 1.7935\n",
            "Epoch [1245/30000]: Train loss: 1.5455, Valid loss: 1.7686, L1 loss: 1.7923\n",
            "Epoch [1246/30000]: Train loss: 1.5457, Valid loss: 1.7623, L1 loss: 1.7913\n",
            "Epoch [1247/30000]: Train loss: 1.5431, Valid loss: 1.7846, L1 loss: 1.8094\n",
            "Epoch [1248/30000]: Train loss: 1.5356, Valid loss: 1.7786, L1 loss: 1.7995\n",
            "Epoch [1249/30000]: Train loss: 1.5523, Valid loss: 1.7928, L1 loss: 1.8108\n",
            "Epoch [1250/30000]: Train loss: 1.5436, Valid loss: 1.7879, L1 loss: 1.8014\n",
            "Epoch [1251/30000]: Train loss: 1.5443, Valid loss: 1.8025, L1 loss: 1.8135\n",
            "Epoch [1252/30000]: Train loss: 1.5441, Valid loss: 1.7820, L1 loss: 1.8025\n",
            "Epoch [1253/30000]: Train loss: 1.5486, Valid loss: 1.7802, L1 loss: 1.7983\n",
            "Epoch [1254/30000]: Train loss: 1.5304, Valid loss: 1.7899, L1 loss: 1.8108\n",
            "Epoch [1255/30000]: Train loss: 1.5397, Valid loss: 1.7935, L1 loss: 1.8117\n",
            "Epoch [1256/30000]: Train loss: 1.5497, Valid loss: 1.7844, L1 loss: 1.8012\n",
            "Epoch [1257/30000]: Train loss: 1.5546, Valid loss: 1.7600, L1 loss: 1.7837\n",
            "Epoch [1258/30000]: Train loss: 1.5492, Valid loss: 1.7698, L1 loss: 1.7877\n",
            "Epoch [1259/30000]: Train loss: 1.5397, Valid loss: 1.7636, L1 loss: 1.7862\n",
            "Epoch [1260/30000]: Train loss: 1.5359, Valid loss: 1.7836, L1 loss: 1.8069\n",
            "Epoch [1261/30000]: Train loss: 1.5499, Valid loss: 1.7897, L1 loss: 1.8098\n",
            "Epoch [1262/30000]: Train loss: 1.5381, Valid loss: 1.7918, L1 loss: 1.8059\n",
            "Epoch [1263/30000]: Train loss: 1.5485, Valid loss: 1.7541, L1 loss: 1.7844\n",
            "Epoch [1264/30000]: Train loss: 1.5544, Valid loss: 1.7610, L1 loss: 1.7913\n",
            "Epoch [1265/30000]: Train loss: 1.5445, Valid loss: 1.7724, L1 loss: 1.7983\n",
            "Epoch [1266/30000]: Train loss: 1.5525, Valid loss: 1.7675, L1 loss: 1.7936\n",
            "Epoch [1267/30000]: Train loss: 1.5446, Valid loss: 1.7572, L1 loss: 1.7833\n",
            "Epoch [1268/30000]: Train loss: 1.5464, Valid loss: 1.7783, L1 loss: 1.7997\n",
            "Epoch [1269/30000]: Train loss: 1.5500, Valid loss: 1.7684, L1 loss: 1.7948\n",
            "Epoch [1270/30000]: Train loss: 1.5425, Valid loss: 1.7562, L1 loss: 1.7876\n",
            "Epoch [1271/30000]: Train loss: 1.5387, Valid loss: 1.7672, L1 loss: 1.7925\n",
            "Epoch [1272/30000]: Train loss: 1.5358, Valid loss: 1.7900, L1 loss: 1.8048\n",
            "Epoch [1273/30000]: Train loss: 1.5554, Valid loss: 1.7714, L1 loss: 1.7952\n",
            "Epoch [1274/30000]: Train loss: 1.5463, Valid loss: 1.7691, L1 loss: 1.7914\n",
            "Epoch [1275/30000]: Train loss: 1.5450, Valid loss: 1.7486, L1 loss: 1.7750\n",
            "Epoch [1276/30000]: Train loss: 1.5458, Valid loss: 1.7754, L1 loss: 1.7973\n",
            "Epoch [1277/30000]: Train loss: 1.5493, Valid loss: 1.7569, L1 loss: 1.7807\n",
            "Epoch [1278/30000]: Train loss: 1.5470, Valid loss: 1.7836, L1 loss: 1.8047\n",
            "Epoch [1279/30000]: Train loss: 1.5406, Valid loss: 1.7591, L1 loss: 1.7859\n",
            "Epoch [1280/30000]: Train loss: 1.5429, Valid loss: 1.7859, L1 loss: 1.8153\n",
            "Epoch [1281/30000]: Train loss: 1.5507, Valid loss: 1.7851, L1 loss: 1.8085\n",
            "Epoch [1282/30000]: Train loss: 1.5544, Valid loss: 1.7800, L1 loss: 1.8037\n",
            "Epoch [1283/30000]: Train loss: 1.5410, Valid loss: 1.7934, L1 loss: 1.8132\n",
            "Epoch [1284/30000]: Train loss: 1.5356, Valid loss: 1.7979, L1 loss: 1.8127\n",
            "Epoch [1285/30000]: Train loss: 1.5386, Valid loss: 1.7858, L1 loss: 1.8002\n",
            "Epoch [1286/30000]: Train loss: 1.5480, Valid loss: 1.7721, L1 loss: 1.7944\n",
            "Epoch [1287/30000]: Train loss: 1.5508, Valid loss: 1.7668, L1 loss: 1.7971\n",
            "Epoch [1288/30000]: Train loss: 1.5440, Valid loss: 1.7755, L1 loss: 1.7992\n",
            "Epoch [1289/30000]: Train loss: 1.5511, Valid loss: 1.7856, L1 loss: 1.8086\n",
            "Epoch [1290/30000]: Train loss: 1.5425, Valid loss: 1.8094, L1 loss: 1.8238\n",
            "Epoch [1291/30000]: Train loss: 1.5425, Valid loss: 1.8055, L1 loss: 1.8144\n",
            "Epoch [1292/30000]: Train loss: 1.5492, Valid loss: 1.7829, L1 loss: 1.7995\n",
            "Epoch [1293/30000]: Train loss: 1.5499, Valid loss: 1.8067, L1 loss: 1.8139\n",
            "Epoch [1294/30000]: Train loss: 1.5403, Valid loss: 1.7942, L1 loss: 1.8106\n",
            "Epoch [1295/30000]: Train loss: 1.5400, Valid loss: 1.7848, L1 loss: 1.8075\n",
            "Epoch [1296/30000]: Train loss: 1.5350, Valid loss: 1.7967, L1 loss: 1.8160\n",
            "Epoch [1297/30000]: Train loss: 1.5524, Valid loss: 1.8082, L1 loss: 1.8256\n",
            "Epoch [1298/30000]: Train loss: 1.5553, Valid loss: 1.8276, L1 loss: 1.8367\n",
            "Epoch [1299/30000]: Train loss: 1.5484, Valid loss: 1.8288, L1 loss: 1.8333\n",
            "Epoch [1300/30000]: Train loss: 1.5480, Valid loss: 1.8372, L1 loss: 1.8456\n",
            "Epoch [1301/30000]: Train loss: 1.5418, Valid loss: 1.8123, L1 loss: 1.8248\n",
            "Epoch [1302/30000]: Train loss: 1.5340, Valid loss: 1.8001, L1 loss: 1.8170\n",
            "Epoch [1303/30000]: Train loss: 1.5448, Valid loss: 1.8114, L1 loss: 1.8267\n",
            "Epoch [1304/30000]: Train loss: 1.5544, Valid loss: 1.7863, L1 loss: 1.8052\n",
            "Epoch [1305/30000]: Train loss: 1.5462, Valid loss: 1.8182, L1 loss: 1.8281\n",
            "Epoch [1306/30000]: Train loss: 1.5346, Valid loss: 1.7967, L1 loss: 1.8152\n",
            "Epoch [1307/30000]: Train loss: 1.5356, Valid loss: 1.8355, L1 loss: 1.8412\n",
            "Epoch [1308/30000]: Train loss: 1.5316, Valid loss: 1.7981, L1 loss: 1.8118\n",
            "Epoch [1309/30000]: Train loss: 1.5442, Valid loss: 1.7934, L1 loss: 1.8076\n",
            "Epoch [1310/30000]: Train loss: 1.5492, Valid loss: 1.8040, L1 loss: 1.8191\n",
            "Epoch [1311/30000]: Train loss: 1.5455, Valid loss: 1.8341, L1 loss: 1.8405\n",
            "Epoch [1312/30000]: Train loss: 1.5377, Valid loss: 1.8028, L1 loss: 1.8222\n",
            "Epoch [1313/30000]: Train loss: 1.5346, Valid loss: 1.7926, L1 loss: 1.8080\n",
            "Epoch [1314/30000]: Train loss: 1.5435, Valid loss: 1.8030, L1 loss: 1.8155\n",
            "Epoch [1315/30000]: Train loss: 1.5573, Valid loss: 1.8040, L1 loss: 1.8184\n",
            "Epoch [1316/30000]: Train loss: 1.5420, Valid loss: 1.8218, L1 loss: 1.8322\n",
            "Epoch [1317/30000]: Train loss: 1.5369, Valid loss: 1.8038, L1 loss: 1.8157\n",
            "Epoch [1318/30000]: Train loss: 1.5537, Valid loss: 1.8173, L1 loss: 1.8281\n",
            "Epoch [1319/30000]: Train loss: 1.5467, Valid loss: 1.8261, L1 loss: 1.8389\n",
            "Epoch [1320/30000]: Train loss: 1.5453, Valid loss: 1.8175, L1 loss: 1.8341\n",
            "Epoch [1321/30000]: Train loss: 1.5440, Valid loss: 1.8156, L1 loss: 1.8261\n",
            "Epoch [1322/30000]: Train loss: 1.5443, Valid loss: 1.8200, L1 loss: 1.8353\n",
            "Epoch [1323/30000]: Train loss: 1.5376, Valid loss: 1.8148, L1 loss: 1.8277\n",
            "Epoch [1324/30000]: Train loss: 1.5376, Valid loss: 1.7931, L1 loss: 1.8111\n",
            "Epoch [1325/30000]: Train loss: 1.5542, Valid loss: 1.8005, L1 loss: 1.8160\n",
            "Epoch [1326/30000]: Train loss: 1.5466, Valid loss: 1.7874, L1 loss: 1.8109\n",
            "Epoch [1327/30000]: Train loss: 1.5314, Valid loss: 1.7996, L1 loss: 1.8136\n",
            "Epoch [1328/30000]: Train loss: 1.5354, Valid loss: 1.8065, L1 loss: 1.8163\n",
            "Epoch [1329/30000]: Train loss: 1.5539, Valid loss: 1.8124, L1 loss: 1.8255\n",
            "Epoch [1330/30000]: Train loss: 1.5423, Valid loss: 1.8328, L1 loss: 1.8429\n",
            "Epoch [1331/30000]: Train loss: 1.5484, Valid loss: 1.8255, L1 loss: 1.8389\n",
            "Epoch [1332/30000]: Train loss: 1.5383, Valid loss: 1.8086, L1 loss: 1.8260\n",
            "Epoch [1333/30000]: Train loss: 1.5313, Valid loss: 1.8099, L1 loss: 1.8209\n",
            "Epoch [1334/30000]: Train loss: 1.5431, Valid loss: 1.7989, L1 loss: 1.8170\n",
            "Epoch [1335/30000]: Train loss: 1.5536, Valid loss: 1.7719, L1 loss: 1.7988\n",
            "Epoch [1336/30000]: Train loss: 1.5506, Valid loss: 1.7482, L1 loss: 1.7746\n",
            "Epoch [1337/30000]: Train loss: 1.5402, Valid loss: 1.8038, L1 loss: 1.8204\n",
            "Epoch [1338/30000]: Train loss: 1.5430, Valid loss: 1.7863, L1 loss: 1.8102\n",
            "Epoch [1339/30000]: Train loss: 1.5515, Valid loss: 1.7835, L1 loss: 1.8079\n",
            "Epoch [1340/30000]: Train loss: 1.5485, Valid loss: 1.8026, L1 loss: 1.8236\n",
            "Epoch [1341/30000]: Train loss: 1.5500, Valid loss: 1.7816, L1 loss: 1.8057\n",
            "Epoch [1342/30000]: Train loss: 1.5502, Valid loss: 1.8463, L1 loss: 1.8490\n",
            "Epoch [1343/30000]: Train loss: 1.5453, Valid loss: 1.8052, L1 loss: 1.8245\n",
            "Epoch [1344/30000]: Train loss: 1.5418, Valid loss: 1.8123, L1 loss: 1.8243\n",
            "Epoch [1345/30000]: Train loss: 1.5354, Valid loss: 1.7896, L1 loss: 1.8092\n",
            "Epoch [1346/30000]: Train loss: 1.5454, Valid loss: 1.8072, L1 loss: 1.8237\n",
            "Epoch [1347/30000]: Train loss: 1.5544, Valid loss: 1.7816, L1 loss: 1.8068\n",
            "Epoch [1348/30000]: Train loss: 1.5301, Valid loss: 1.8427, L1 loss: 1.8534\n",
            "Epoch [1349/30000]: Train loss: 1.5445, Valid loss: 1.8128, L1 loss: 1.8303\n",
            "Epoch [1350/30000]: Train loss: 1.5429, Valid loss: 1.8387, L1 loss: 1.8541\n",
            "Epoch [1351/30000]: Train loss: 1.5385, Valid loss: 1.8092, L1 loss: 1.8236\n",
            "Epoch [1352/30000]: Train loss: 1.5341, Valid loss: 1.8314, L1 loss: 1.8443\n",
            "Epoch [1353/30000]: Train loss: 1.5425, Valid loss: 1.7947, L1 loss: 1.8128\n",
            "Epoch [1354/30000]: Train loss: 1.5447, Valid loss: 1.8071, L1 loss: 1.8173\n",
            "Epoch [1355/30000]: Train loss: 1.5512, Valid loss: 1.7828, L1 loss: 1.8035\n",
            "Epoch [1356/30000]: Train loss: 1.5487, Valid loss: 1.7881, L1 loss: 1.7998\n",
            "Epoch [1357/30000]: Train loss: 1.5499, Valid loss: 1.7886, L1 loss: 1.8073\n",
            "Epoch [1358/30000]: Train loss: 1.5414, Valid loss: 1.7660, L1 loss: 1.7883\n",
            "Epoch [1359/30000]: Train loss: 1.5388, Valid loss: 1.7993, L1 loss: 1.8080\n",
            "Epoch [1360/30000]: Train loss: 1.5548, Valid loss: 1.7804, L1 loss: 1.7957\n",
            "Epoch [1361/30000]: Train loss: 1.5380, Valid loss: 1.7644, L1 loss: 1.7877\n",
            "Epoch [1362/30000]: Train loss: 1.5572, Valid loss: 1.8129, L1 loss: 1.8244\n",
            "Epoch [1363/30000]: Train loss: 1.5414, Valid loss: 1.8284, L1 loss: 1.8321\n",
            "Epoch [1364/30000]: Train loss: 1.5391, Valid loss: 1.8107, L1 loss: 1.8195\n",
            "Epoch [1365/30000]: Train loss: 1.5528, Valid loss: 1.8101, L1 loss: 1.8194\n",
            "Epoch [1366/30000]: Train loss: 1.5500, Valid loss: 1.8033, L1 loss: 1.8180\n",
            "Epoch [1367/30000]: Train loss: 1.5443, Valid loss: 1.7863, L1 loss: 1.8019\n",
            "Epoch [1368/30000]: Train loss: 1.5416, Valid loss: 1.7785, L1 loss: 1.7938\n",
            "Epoch [1369/30000]: Train loss: 1.5567, Valid loss: 1.7773, L1 loss: 1.7917\n",
            "Epoch [1370/30000]: Train loss: 1.5494, Valid loss: 1.7887, L1 loss: 1.8070\n",
            "Epoch [1371/30000]: Train loss: 1.5297, Valid loss: 1.7786, L1 loss: 1.7977\n",
            "Epoch [1372/30000]: Train loss: 1.5560, Valid loss: 1.7768, L1 loss: 1.7963\n",
            "Epoch [1373/30000]: Train loss: 1.5535, Valid loss: 1.7495, L1 loss: 1.7798\n",
            "Epoch [1374/30000]: Train loss: 1.5373, Valid loss: 1.7714, L1 loss: 1.7935\n",
            "Epoch [1375/30000]: Train loss: 1.5457, Valid loss: 1.8117, L1 loss: 1.8196\n",
            "Epoch [1376/30000]: Train loss: 1.5499, Valid loss: 1.7968, L1 loss: 1.8103\n",
            "Epoch [1377/30000]: Train loss: 1.5464, Valid loss: 1.7685, L1 loss: 1.7879\n",
            "Epoch [1378/30000]: Train loss: 1.5381, Valid loss: 1.7759, L1 loss: 1.7992\n",
            "Epoch [1379/30000]: Train loss: 1.5430, Valid loss: 1.7696, L1 loss: 1.7854\n",
            "Epoch [1380/30000]: Train loss: 1.5614, Valid loss: 1.7464, L1 loss: 1.7694\n",
            "Epoch [1381/30000]: Train loss: 1.5504, Valid loss: 1.7627, L1 loss: 1.7860\n",
            "Epoch [1382/30000]: Train loss: 1.5479, Valid loss: 1.7753, L1 loss: 1.7891\n",
            "Epoch [1383/30000]: Train loss: 1.5469, Valid loss: 1.7741, L1 loss: 1.7927\n",
            "Epoch [1384/30000]: Train loss: 1.5395, Valid loss: 1.7684, L1 loss: 1.7870\n",
            "Epoch [1385/30000]: Train loss: 1.5474, Valid loss: 1.7860, L1 loss: 1.7940\n",
            "Epoch [1386/30000]: Train loss: 1.5441, Valid loss: 1.7928, L1 loss: 1.8028\n",
            "Epoch [1387/30000]: Train loss: 1.5391, Valid loss: 1.7636, L1 loss: 1.7861\n",
            "Epoch [1388/30000]: Train loss: 1.5532, Valid loss: 1.7610, L1 loss: 1.7825\n",
            "Epoch [1389/30000]: Train loss: 1.5351, Valid loss: 1.7860, L1 loss: 1.8001\n",
            "Epoch [1390/30000]: Train loss: 1.5451, Valid loss: 1.7820, L1 loss: 1.7960\n",
            "Epoch [1391/30000]: Train loss: 1.5566, Valid loss: 1.7846, L1 loss: 1.7988\n",
            "Epoch [1392/30000]: Train loss: 1.5427, Valid loss: 1.7715, L1 loss: 1.7882\n",
            "Epoch [1393/30000]: Train loss: 1.5429, Valid loss: 1.7604, L1 loss: 1.7842\n",
            "Epoch [1394/30000]: Train loss: 1.5425, Valid loss: 1.7785, L1 loss: 1.7897\n",
            "Epoch [1395/30000]: Train loss: 1.5454, Valid loss: 1.8046, L1 loss: 1.8081\n",
            "Epoch [1396/30000]: Train loss: 1.5306, Valid loss: 1.8031, L1 loss: 1.8186\n",
            "Epoch [1397/30000]: Train loss: 1.5498, Valid loss: 1.7876, L1 loss: 1.8072\n",
            "Epoch [1398/30000]: Train loss: 1.5426, Valid loss: 1.7803, L1 loss: 1.7950\n",
            "Epoch [1399/30000]: Train loss: 1.5527, Valid loss: 1.7891, L1 loss: 1.7999\n",
            "Epoch [1400/30000]: Train loss: 1.5364, Valid loss: 1.8381, L1 loss: 1.8417\n",
            "Epoch [1401/30000]: Train loss: 1.5421, Valid loss: 1.8083, L1 loss: 1.8144\n",
            "Epoch [1402/30000]: Train loss: 1.5430, Valid loss: 1.7711, L1 loss: 1.7850\n",
            "Epoch [1403/30000]: Train loss: 1.5412, Valid loss: 1.7898, L1 loss: 1.7935\n",
            "Epoch [1404/30000]: Train loss: 1.5249, Valid loss: 1.7830, L1 loss: 1.7928\n",
            "Epoch [1405/30000]: Train loss: 1.5315, Valid loss: 1.7934, L1 loss: 1.8001\n",
            "Epoch [1406/30000]: Train loss: 1.5434, Valid loss: 1.7931, L1 loss: 1.8045\n",
            "Epoch [1407/30000]: Train loss: 1.5466, Valid loss: 1.7998, L1 loss: 1.8088\n",
            "Epoch [1408/30000]: Train loss: 1.5421, Valid loss: 1.7841, L1 loss: 1.7998\n",
            "Epoch [1409/30000]: Train loss: 1.5387, Valid loss: 1.7956, L1 loss: 1.8050\n",
            "Epoch [1410/30000]: Train loss: 1.5478, Valid loss: 1.8351, L1 loss: 1.8359\n",
            "Epoch [1411/30000]: Train loss: 1.5523, Valid loss: 1.8515, L1 loss: 1.8519\n",
            "Epoch [1412/30000]: Train loss: 1.5427, Valid loss: 1.8307, L1 loss: 1.8326\n",
            "Epoch [1413/30000]: Train loss: 1.5489, Valid loss: 1.8004, L1 loss: 1.8113\n",
            "Epoch [1414/30000]: Train loss: 1.5451, Valid loss: 1.8064, L1 loss: 1.8178\n",
            "Epoch [1415/30000]: Train loss: 1.5459, Valid loss: 1.7932, L1 loss: 1.8028\n",
            "Epoch [1416/30000]: Train loss: 1.5504, Valid loss: 1.8363, L1 loss: 1.8381\n",
            "Epoch [1417/30000]: Train loss: 1.5352, Valid loss: 1.8120, L1 loss: 1.8165\n",
            "Epoch [1418/30000]: Train loss: 1.5536, Valid loss: 1.8168, L1 loss: 1.8231\n",
            "Epoch [1419/30000]: Train loss: 1.5425, Valid loss: 1.7934, L1 loss: 1.8068\n",
            "Epoch [1420/30000]: Train loss: 1.5496, Valid loss: 1.8216, L1 loss: 1.8373\n",
            "Epoch [1421/30000]: Train loss: 1.5560, Valid loss: 1.8283, L1 loss: 1.8342\n",
            "Epoch [1422/30000]: Train loss: 1.5362, Valid loss: 1.7808, L1 loss: 1.8042\n",
            "Epoch [1423/30000]: Train loss: 1.5425, Valid loss: 1.7961, L1 loss: 1.8137\n",
            "Epoch [1424/30000]: Train loss: 1.5470, Valid loss: 1.7660, L1 loss: 1.7845\n",
            "Epoch [1425/30000]: Train loss: 1.5395, Valid loss: 1.7647, L1 loss: 1.7888\n",
            "Epoch [1426/30000]: Train loss: 1.5530, Valid loss: 1.7971, L1 loss: 1.8124\n",
            "Epoch [1427/30000]: Train loss: 1.5405, Valid loss: 1.8175, L1 loss: 1.8269\n",
            "Epoch [1428/30000]: Train loss: 1.5414, Valid loss: 1.7702, L1 loss: 1.7896\n",
            "Epoch [1429/30000]: Train loss: 1.5408, Valid loss: 1.8032, L1 loss: 1.8155\n",
            "Epoch [1430/30000]: Train loss: 1.5490, Valid loss: 1.7967, L1 loss: 1.8095\n",
            "Epoch [1431/30000]: Train loss: 1.5457, Valid loss: 1.8088, L1 loss: 1.8200\n",
            "Epoch [1432/30000]: Train loss: 1.5504, Valid loss: 1.7852, L1 loss: 1.8034\n",
            "Epoch [1433/30000]: Train loss: 1.5448, Valid loss: 1.7900, L1 loss: 1.8037\n",
            "Epoch [1434/30000]: Train loss: 1.5513, Valid loss: 1.7907, L1 loss: 1.8058\n",
            "Epoch [1435/30000]: Train loss: 1.5553, Valid loss: 1.8057, L1 loss: 1.8132\n",
            "Epoch [1436/30000]: Train loss: 1.5456, Valid loss: 1.8323, L1 loss: 1.8365\n",
            "Epoch [1437/30000]: Train loss: 1.5342, Valid loss: 1.8119, L1 loss: 1.8187\n",
            "Epoch [1438/30000]: Train loss: 1.5474, Valid loss: 1.7960, L1 loss: 1.8081\n",
            "Epoch [1439/30000]: Train loss: 1.5282, Valid loss: 1.7925, L1 loss: 1.8029\n",
            "Epoch [1440/30000]: Train loss: 1.5568, Valid loss: 1.8094, L1 loss: 1.8292\n",
            "Epoch [1441/30000]: Train loss: 1.5438, Valid loss: 1.8238, L1 loss: 1.8350\n",
            "Epoch [1442/30000]: Train loss: 1.5377, Valid loss: 1.8137, L1 loss: 1.8254\n",
            "Epoch [1443/30000]: Train loss: 1.5590, Valid loss: 1.8185, L1 loss: 1.8282\n",
            "Epoch [1444/30000]: Train loss: 1.5374, Valid loss: 1.7782, L1 loss: 1.7923\n",
            "Epoch [1445/30000]: Train loss: 1.5513, Valid loss: 1.7800, L1 loss: 1.7985\n",
            "Epoch [1446/30000]: Train loss: 1.5579, Valid loss: 1.8132, L1 loss: 1.8208\n",
            "Epoch [1447/30000]: Train loss: 1.5426, Valid loss: 1.8044, L1 loss: 1.8200\n",
            "Epoch [1448/30000]: Train loss: 1.5542, Valid loss: 1.8120, L1 loss: 1.8270\n",
            "Epoch [1449/30000]: Train loss: 1.5559, Valid loss: 1.7847, L1 loss: 1.8040\n",
            "Epoch [1450/30000]: Train loss: 1.5374, Valid loss: 1.7994, L1 loss: 1.8116\n",
            "Epoch [1451/30000]: Train loss: 1.5465, Valid loss: 1.8495, L1 loss: 1.8495\n",
            "Epoch [1452/30000]: Train loss: 1.5444, Valid loss: 1.7972, L1 loss: 1.8066\n",
            "Epoch [1453/30000]: Train loss: 1.5393, Valid loss: 1.8229, L1 loss: 1.8294\n",
            "Epoch [1454/30000]: Train loss: 1.5470, Valid loss: 1.7944, L1 loss: 1.8077\n",
            "Epoch [1455/30000]: Train loss: 1.5388, Valid loss: 1.7932, L1 loss: 1.8061\n",
            "Epoch [1456/30000]: Train loss: 1.5444, Valid loss: 1.7658, L1 loss: 1.7855\n",
            "Epoch [1457/30000]: Train loss: 1.5502, Valid loss: 1.7779, L1 loss: 1.7926\n",
            "Epoch [1458/30000]: Train loss: 1.5448, Valid loss: 1.7847, L1 loss: 1.8042\n",
            "Epoch [1459/30000]: Train loss: 1.5488, Valid loss: 1.7924, L1 loss: 1.8065\n",
            "Epoch [1460/30000]: Train loss: 1.5650, Valid loss: 1.7757, L1 loss: 1.7939\n",
            "Epoch [1461/30000]: Train loss: 1.5584, Valid loss: 1.8011, L1 loss: 1.8122\n",
            "Epoch [1462/30000]: Train loss: 1.5250, Valid loss: 1.8024, L1 loss: 1.8168\n",
            "Epoch [1463/30000]: Train loss: 1.5412, Valid loss: 1.7899, L1 loss: 1.8081\n",
            "Epoch [1464/30000]: Train loss: 1.5390, Valid loss: 1.8014, L1 loss: 1.8086\n",
            "Epoch [1465/30000]: Train loss: 1.5406, Valid loss: 1.7873, L1 loss: 1.8048\n",
            "Epoch [1466/30000]: Train loss: 1.5343, Valid loss: 1.7904, L1 loss: 1.8001\n",
            "Epoch [1467/30000]: Train loss: 1.5408, Valid loss: 1.7853, L1 loss: 1.8037\n",
            "Epoch [1468/30000]: Train loss: 1.5399, Valid loss: 1.7730, L1 loss: 1.7908\n",
            "Epoch [1469/30000]: Train loss: 1.5402, Valid loss: 1.7876, L1 loss: 1.8007\n",
            "Epoch [1470/30000]: Train loss: 1.5357, Valid loss: 1.7997, L1 loss: 1.8160\n",
            "Epoch [1471/30000]: Train loss: 1.5531, Valid loss: 1.7625, L1 loss: 1.7830\n",
            "Epoch [1472/30000]: Train loss: 1.5490, Valid loss: 1.7905, L1 loss: 1.8031\n",
            "Epoch [1473/30000]: Train loss: 1.5537, Valid loss: 1.7747, L1 loss: 1.7918\n",
            "Epoch [1474/30000]: Train loss: 1.5439, Valid loss: 1.7588, L1 loss: 1.7823\n",
            "Epoch [1475/30000]: Train loss: 1.5323, Valid loss: 1.7652, L1 loss: 1.7837\n",
            "Epoch [1476/30000]: Train loss: 1.5526, Valid loss: 1.7963, L1 loss: 1.8104\n",
            "Epoch [1477/30000]: Train loss: 1.5494, Valid loss: 1.7827, L1 loss: 1.8002\n",
            "Epoch [1478/30000]: Train loss: 1.5492, Valid loss: 1.7937, L1 loss: 1.8050\n",
            "Epoch [1479/30000]: Train loss: 1.5490, Valid loss: 1.7765, L1 loss: 1.7947\n",
            "Epoch [1480/30000]: Train loss: 1.5633, Valid loss: 1.7597, L1 loss: 1.7792\n",
            "Epoch [1481/30000]: Train loss: 1.5401, Valid loss: 1.7536, L1 loss: 1.7754\n",
            "Epoch [1482/30000]: Train loss: 1.5289, Valid loss: 1.7981, L1 loss: 1.8158\n",
            "Epoch [1483/30000]: Train loss: 1.5328, Valid loss: 1.7779, L1 loss: 1.7978\n",
            "Epoch [1484/30000]: Train loss: 1.5499, Valid loss: 1.7647, L1 loss: 1.7803\n",
            "Epoch [1485/30000]: Train loss: 1.5449, Valid loss: 1.7612, L1 loss: 1.7828\n",
            "Epoch [1486/30000]: Train loss: 1.5457, Valid loss: 1.7462, L1 loss: 1.7647\n",
            "Epoch [1487/30000]: Train loss: 1.5497, Valid loss: 1.7811, L1 loss: 1.7945\n",
            "Epoch [1488/30000]: Train loss: 1.5488, Valid loss: 1.7631, L1 loss: 1.7802\n",
            "Epoch [1489/30000]: Train loss: 1.5402, Valid loss: 1.7413, L1 loss: 1.7633\n",
            "Epoch [1490/30000]: Train loss: 1.5507, Valid loss: 1.7552, L1 loss: 1.7797\n",
            "Epoch [1491/30000]: Train loss: 1.5547, Valid loss: 1.7591, L1 loss: 1.7821\n",
            "Epoch [1492/30000]: Train loss: 1.5407, Valid loss: 1.7359, L1 loss: 1.7603\n",
            "Epoch [1493/30000]: Train loss: 1.5586, Valid loss: 1.7367, L1 loss: 1.7606\n",
            "Epoch [1494/30000]: Train loss: 1.5374, Valid loss: 1.7571, L1 loss: 1.7747\n",
            "Epoch [1495/30000]: Train loss: 1.5443, Valid loss: 1.7733, L1 loss: 1.7875\n",
            "Epoch [1496/30000]: Train loss: 1.5529, Valid loss: 1.7703, L1 loss: 1.7846\n",
            "Epoch [1497/30000]: Train loss: 1.5521, Valid loss: 1.7733, L1 loss: 1.7883\n",
            "Epoch [1498/30000]: Train loss: 1.5452, Valid loss: 1.7642, L1 loss: 1.7790\n",
            "Epoch [1499/30000]: Train loss: 1.5321, Valid loss: 1.7793, L1 loss: 1.7949\n",
            "Epoch [1500/30000]: Train loss: 1.5458, Valid loss: 1.7457, L1 loss: 1.7699\n",
            "Epoch [1501/30000]: Train loss: 1.5447, Valid loss: 1.7634, L1 loss: 1.7791\n",
            "Epoch [1502/30000]: Train loss: 1.5406, Valid loss: 1.7468, L1 loss: 1.7661\n",
            "Epoch [1503/30000]: Train loss: 1.5440, Valid loss: 1.7580, L1 loss: 1.7777\n",
            "Epoch [1504/30000]: Train loss: 1.5452, Valid loss: 1.7413, L1 loss: 1.7649\n",
            "Epoch [1505/30000]: Train loss: 1.5486, Valid loss: 1.7564, L1 loss: 1.7754\n",
            "Epoch [1506/30000]: Train loss: 1.5423, Valid loss: 1.7391, L1 loss: 1.7613\n",
            "Epoch [1507/30000]: Train loss: 1.5320, Valid loss: 1.7748, L1 loss: 1.7924\n",
            "Epoch [1508/30000]: Train loss: 1.5426, Valid loss: 1.7620, L1 loss: 1.7866\n",
            "Epoch [1509/30000]: Train loss: 1.5457, Valid loss: 1.7635, L1 loss: 1.7794\n",
            "Epoch [1510/30000]: Train loss: 1.5385, Valid loss: 1.7643, L1 loss: 1.7842\n",
            "Epoch [1511/30000]: Train loss: 1.5503, Valid loss: 1.7399, L1 loss: 1.7622\n",
            "Epoch [1512/30000]: Train loss: 1.5554, Valid loss: 1.7336, L1 loss: 1.7612\n",
            "Epoch [1513/30000]: Train loss: 1.5450, Valid loss: 1.7222, L1 loss: 1.7517\n",
            "Epoch [1514/30000]: Train loss: 1.5491, Valid loss: 1.7562, L1 loss: 1.7765\n",
            "Epoch [1515/30000]: Train loss: 1.5386, Valid loss: 1.7678, L1 loss: 1.7891\n",
            "Epoch [1516/30000]: Train loss: 1.5438, Valid loss: 1.7662, L1 loss: 1.7923\n",
            "Epoch [1517/30000]: Train loss: 1.5447, Valid loss: 1.7495, L1 loss: 1.7724\n",
            "Epoch [1518/30000]: Train loss: 1.5594, Valid loss: 1.7646, L1 loss: 1.7806\n",
            "Epoch [1519/30000]: Train loss: 1.5364, Valid loss: 1.7589, L1 loss: 1.7784\n",
            "Epoch [1520/30000]: Train loss: 1.5509, Valid loss: 1.7574, L1 loss: 1.7727\n",
            "Epoch [1521/30000]: Train loss: 1.5462, Valid loss: 1.7800, L1 loss: 1.7964\n",
            "Epoch [1522/30000]: Train loss: 1.5418, Valid loss: 1.7579, L1 loss: 1.7768\n",
            "Epoch [1523/30000]: Train loss: 1.5304, Valid loss: 1.7712, L1 loss: 1.7846\n",
            "Epoch [1524/30000]: Train loss: 1.5492, Valid loss: 1.7843, L1 loss: 1.7970\n",
            "Epoch [1525/30000]: Train loss: 1.5465, Valid loss: 1.7571, L1 loss: 1.7749\n",
            "Epoch [1526/30000]: Train loss: 1.5383, Valid loss: 1.7703, L1 loss: 1.7856\n",
            "Epoch [1527/30000]: Train loss: 1.5608, Valid loss: 1.7988, L1 loss: 1.8079\n",
            "Epoch [1528/30000]: Train loss: 1.5569, Valid loss: 1.7516, L1 loss: 1.7722\n",
            "Epoch [1529/30000]: Train loss: 1.5402, Valid loss: 1.7821, L1 loss: 1.7981\n",
            "Epoch [1530/30000]: Train loss: 1.5417, Valid loss: 1.7948, L1 loss: 1.8065\n",
            "Epoch [1531/30000]: Train loss: 1.5397, Valid loss: 1.7761, L1 loss: 1.7945\n",
            "Epoch [1532/30000]: Train loss: 1.5481, Valid loss: 1.7699, L1 loss: 1.7850\n",
            "Epoch [1533/30000]: Train loss: 1.5505, Valid loss: 1.7684, L1 loss: 1.7857\n",
            "Epoch [1534/30000]: Train loss: 1.5348, Valid loss: 1.7631, L1 loss: 1.7833\n",
            "Epoch [1535/30000]: Train loss: 1.5427, Valid loss: 1.7927, L1 loss: 1.8015\n",
            "Epoch [1536/30000]: Train loss: 1.5450, Valid loss: 1.8070, L1 loss: 1.8156\n",
            "Epoch [1537/30000]: Train loss: 1.5428, Valid loss: 1.8083, L1 loss: 1.8207\n",
            "Epoch [1538/30000]: Train loss: 1.5456, Valid loss: 1.7604, L1 loss: 1.7819\n",
            "Epoch [1539/30000]: Train loss: 1.5576, Valid loss: 1.7549, L1 loss: 1.7724\n",
            "Epoch [1540/30000]: Train loss: 1.5304, Valid loss: 1.7464, L1 loss: 1.7719\n",
            "Epoch [1541/30000]: Train loss: 1.5433, Valid loss: 1.7787, L1 loss: 1.7858\n",
            "Epoch [1542/30000]: Train loss: 1.5328, Valid loss: 1.7759, L1 loss: 1.7893\n",
            "Epoch [1543/30000]: Train loss: 1.5456, Valid loss: 1.7640, L1 loss: 1.7793\n",
            "Epoch [1544/30000]: Train loss: 1.5454, Valid loss: 1.7939, L1 loss: 1.8065\n",
            "Epoch [1545/30000]: Train loss: 1.5357, Valid loss: 1.7913, L1 loss: 1.8042\n",
            "Epoch [1546/30000]: Train loss: 1.5431, Valid loss: 1.7568, L1 loss: 1.7769\n",
            "Epoch [1547/30000]: Train loss: 1.5357, Valid loss: 1.7480, L1 loss: 1.7723\n",
            "Epoch [1548/30000]: Train loss: 1.5487, Valid loss: 1.7687, L1 loss: 1.7926\n",
            "Epoch [1549/30000]: Train loss: 1.5495, Valid loss: 1.7652, L1 loss: 1.7804\n",
            "Epoch [1550/30000]: Train loss: 1.5389, Valid loss: 1.7703, L1 loss: 1.7885\n",
            "Epoch [1551/30000]: Train loss: 1.5374, Valid loss: 1.8302, L1 loss: 1.8405\n",
            "Epoch [1552/30000]: Train loss: 1.5486, Valid loss: 1.7802, L1 loss: 1.7966\n",
            "Epoch [1553/30000]: Train loss: 1.5380, Valid loss: 1.7731, L1 loss: 1.7891\n",
            "Epoch [1554/30000]: Train loss: 1.5383, Valid loss: 1.7920, L1 loss: 1.8096\n",
            "Epoch [1555/30000]: Train loss: 1.5351, Valid loss: 1.7423, L1 loss: 1.7623\n",
            "Epoch [1556/30000]: Train loss: 1.5516, Valid loss: 1.7887, L1 loss: 1.8040\n",
            "Epoch [1557/30000]: Train loss: 1.5372, Valid loss: 1.7764, L1 loss: 1.7929\n",
            "Epoch [1558/30000]: Train loss: 1.5429, Valid loss: 1.7562, L1 loss: 1.7822\n",
            "Epoch [1559/30000]: Train loss: 1.5469, Valid loss: 1.7542, L1 loss: 1.7733\n",
            "Epoch [1560/30000]: Train loss: 1.5290, Valid loss: 1.8199, L1 loss: 1.8219\n",
            "Epoch [1561/30000]: Train loss: 1.5440, Valid loss: 1.7672, L1 loss: 1.7825\n",
            "Epoch [1562/30000]: Train loss: 1.5456, Valid loss: 1.7722, L1 loss: 1.7831\n",
            "Epoch [1563/30000]: Train loss: 1.5575, Valid loss: 1.7746, L1 loss: 1.7937\n",
            "Epoch [1564/30000]: Train loss: 1.5473, Valid loss: 1.7816, L1 loss: 1.7909\n",
            "Epoch [1565/30000]: Train loss: 1.5441, Valid loss: 1.7758, L1 loss: 1.7881\n",
            "Epoch [1566/30000]: Train loss: 1.5428, Valid loss: 1.7566, L1 loss: 1.7742\n",
            "Epoch [1567/30000]: Train loss: 1.5393, Valid loss: 1.7683, L1 loss: 1.7868\n",
            "Epoch [1568/30000]: Train loss: 1.5389, Valid loss: 1.7820, L1 loss: 1.8004\n",
            "Epoch [1569/30000]: Train loss: 1.5362, Valid loss: 1.7832, L1 loss: 1.7934\n",
            "Epoch [1570/30000]: Train loss: 1.5424, Valid loss: 1.7690, L1 loss: 1.7863\n",
            "Epoch [1571/30000]: Train loss: 1.5521, Valid loss: 1.7773, L1 loss: 1.7897\n",
            "Epoch [1572/30000]: Train loss: 1.5457, Valid loss: 1.7974, L1 loss: 1.8065\n",
            "Epoch [1573/30000]: Train loss: 1.5503, Valid loss: 1.7557, L1 loss: 1.7730\n",
            "Epoch [1574/30000]: Train loss: 1.5436, Valid loss: 1.7806, L1 loss: 1.7962\n",
            "Epoch [1575/30000]: Train loss: 1.5522, Valid loss: 1.8299, L1 loss: 1.8303\n",
            "Epoch [1576/30000]: Train loss: 1.5362, Valid loss: 1.7899, L1 loss: 1.8073\n",
            "Epoch [1577/30000]: Train loss: 1.5471, Valid loss: 1.7997, L1 loss: 1.8109\n",
            "Epoch [1578/30000]: Train loss: 1.5469, Valid loss: 1.8105, L1 loss: 1.8159\n",
            "Epoch [1579/30000]: Train loss: 1.5444, Valid loss: 1.7794, L1 loss: 1.7955\n",
            "Epoch [1580/30000]: Train loss: 1.5328, Valid loss: 1.8116, L1 loss: 1.8227\n",
            "Epoch [1581/30000]: Train loss: 1.5473, Valid loss: 1.7868, L1 loss: 1.8012\n",
            "Epoch [1582/30000]: Train loss: 1.5485, Valid loss: 1.8002, L1 loss: 1.8091\n",
            "Epoch [1583/30000]: Train loss: 1.5562, Valid loss: 1.7801, L1 loss: 1.7961\n",
            "Epoch [1584/30000]: Train loss: 1.5449, Valid loss: 1.7977, L1 loss: 1.8093\n",
            "Epoch [1585/30000]: Train loss: 1.5540, Valid loss: 1.7827, L1 loss: 1.7984\n",
            "Epoch [1586/30000]: Train loss: 1.5525, Valid loss: 1.7746, L1 loss: 1.7917\n",
            "Epoch [1587/30000]: Train loss: 1.5244, Valid loss: 1.8110, L1 loss: 1.8203\n",
            "Epoch [1588/30000]: Train loss: 1.5452, Valid loss: 1.7843, L1 loss: 1.7990\n",
            "Epoch [1589/30000]: Train loss: 1.5428, Valid loss: 1.8020, L1 loss: 1.8138\n",
            "Epoch [1590/30000]: Train loss: 1.5411, Valid loss: 1.7983, L1 loss: 1.8097\n",
            "Epoch [1591/30000]: Train loss: 1.5452, Valid loss: 1.8065, L1 loss: 1.8152\n",
            "Epoch [1592/30000]: Train loss: 1.5394, Valid loss: 1.8397, L1 loss: 1.8472\n",
            "Epoch [1593/30000]: Train loss: 1.5159, Valid loss: 1.8290, L1 loss: 1.8336\n",
            "Epoch [1594/30000]: Train loss: 1.5448, Valid loss: 1.8091, L1 loss: 1.8214\n",
            "Epoch [1595/30000]: Train loss: 1.5333, Valid loss: 1.8058, L1 loss: 1.8240\n",
            "\n",
            "Model is not improving, so we halt the training session.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = My_Model(input_dim=train_data.shape[1]-1).to(device) # put your model and data on the same computation device.\n",
        "trainer(train_data, model, config, device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mNnGeuu72qdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "47b75940-43cd-4aa6-af12-ee2f75643cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 360.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7. 7. 5. ... 1. 3. 8.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cb0686c8-90ef-4a92-9fbc-8eada9b379c0\", \"sample_submission.csv\", 69482)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'Danceability'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i+17170, p])\n",
        "model.load_state_dict(torch.load(config['save_path']))\n",
        "preds = predict(test_loader, model, device)\n",
        "print(preds)\n",
        "save_pred(preds, 'sample_submission.csv')\n",
        "from google.colab import files\n",
        "files.download('sample_submission.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}